{"cells":[{"cell_type":"code","execution_count":2,"metadata":{"id":"BjQ84WIWZpos","executionInfo":{"status":"ok","timestamp":1666551216329,"user_tz":-330,"elapsed":2635,"user":{"displayName":"Gouthaman S","userId":"11145458456231366145"}}},"outputs":[],"source":["import pandas as pd\n","import numpy as np\n","\n","import matplotlib.pyplot as plt\n","%matplotlib inline\n","\n","from keras.models import Sequential, Model,save_model,load_model\n","from keras.layers import LSTM, Dense, Bidirectional,Input\n","\n","import os"]},{"cell_type":"markdown","metadata":{"id":"NL-D26wZZpo5"},"source":["# Data Preprocessing"]},{"cell_type":"code","source":[],"metadata":{"id":"aM7NZwRJdjU8","executionInfo":{"status":"ok","timestamp":1666548041440,"user_tz":-330,"elapsed":9,"user":{"displayName":"Gouthaman S","userId":"11145458456231366145"}}},"execution_count":1,"outputs":[]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"AvdNSzolZ7Mh","executionInfo":{"status":"ok","timestamp":1666551263615,"user_tz":-330,"elapsed":31143,"user":{"displayName":"Gouthaman S","userId":"11145458456231366145"}},"outputId":"07311bf0-6966-4f2c-b4b1-aa357cdd6db0"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","source":["os.chdir('/content/drive/MyDrive/MachineTransalation')"],"metadata":{"id":"7l8SpvhAbvdl","executionInfo":{"status":"ok","timestamp":1666551263618,"user_tz":-330,"elapsed":15,"user":{"displayName":"Gouthaman S","userId":"11145458456231366145"}}},"execution_count":5,"outputs":[]},{"cell_type":"code","execution_count":6,"metadata":{"id":"VG1vk3dbZppL","executionInfo":{"status":"ok","timestamp":1666551268822,"user_tz":-330,"elapsed":903,"user":{"displayName":"Gouthaman S","userId":"11145458456231366145"}}},"outputs":[],"source":["with open('DATASET.txt','r',encoding='utf-8') as f:\n","    lines = f.read().split('\\n')"]},{"cell_type":"code","source":["batch_size = 64  # Batch size for training.\n","epochs = 100  # Number of epochs to traina for.\n","latent_dim = 256  # Latent dimensionality of the encoding space.\n","num_samples = 10000  # Number of samples to train on.\n","# Path to the data txt file on disk.\n","data_path = \"DATASET.txt\""],"metadata":{"id":"T7couT3kcBaW","executionInfo":{"status":"ok","timestamp":1666551272841,"user_tz":-330,"elapsed":5,"user":{"displayName":"Gouthaman S","userId":"11145458456231366145"}}},"execution_count":7,"outputs":[]},{"cell_type":"code","source":["\n","range(min(num_samples, len(lines) - 1))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"-GDgqOx0cW7t","executionInfo":{"status":"ok","timestamp":1666551275081,"user_tz":-330,"elapsed":9,"user":{"displayName":"Gouthaman S","userId":"11145458456231366145"}},"outputId":"6d28cc77-2fe8-4ba4-dec4-5c1a3d9f4bc9"},"execution_count":8,"outputs":[{"output_type":"execute_result","data":{"text/plain":["range(0, 2867)"]},"metadata":{},"execution_count":8}]},{"cell_type":"code","source":["input_texts = []\n","target_texts = []\n","input_characters = set()\n","target_characters = set()"],"metadata":{"id":"fnsHGkBsMTbK","executionInfo":{"status":"ok","timestamp":1666551276629,"user_tz":-330,"elapsed":8,"user":{"displayName":"Gouthaman S","userId":"11145458456231366145"}}},"execution_count":9,"outputs":[]},{"cell_type":"code","execution_count":10,"metadata":{"id":"HIHaZfh-ZppO","executionInfo":{"status":"ok","timestamp":1666551333246,"user_tz":-330,"elapsed":5,"user":{"displayName":"Gouthaman S","userId":"11145458456231366145"}}},"outputs":[],"source":["for line in lines[: min(num_samples, len(lines) - 1)]:\n","    input_text, target_text = line.split(\"\\t\")\n","    # We use \"tab\" as the \"start sequence\" character\n","    # for the targets, and \"\\n\" as \"end sequence\" character.\n","    target_text = \"\\t\" + target_text + \"\\n\"\n","    input_texts.append(input_text)\n","    target_texts.append(target_text)\n","    for char in input_text:\n","        if char not in input_characters:\n","            input_characters.add(char)\n","    for char in target_text:\n","        if char not in target_characters:\n","            target_characters.add(char)\n","\n"]},{"cell_type":"code","execution_count":12,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"AwYCrcW7ZppQ","executionInfo":{"status":"ok","timestamp":1666551350168,"user_tz":-330,"elapsed":10,"user":{"displayName":"Gouthaman S","userId":"11145458456231366145"}},"outputId":"af170801-caf5-4b08-d2f0-a6fca3181391"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["(70, 92)"]},"metadata":{},"execution_count":12}],"source":["len(input_characters) , len(target_characters)"]},{"cell_type":"code","execution_count":13,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"bHja71esZppU","executionInfo":{"status":"ok","timestamp":1666551370002,"user_tz":-330,"elapsed":7,"user":{"displayName":"Gouthaman S","userId":"11145458456231366145"}},"outputId":"2ed22cdd-191a-4fb9-c6a4-372b257d3a68"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["['Wow!',\n"," 'Help!',\n"," 'Jump.',\n"," 'Jump.',\n"," 'Jump.',\n"," 'Hello!',\n"," 'Hello!',\n"," 'Cheers!',\n"," 'Cheers!',\n"," 'Got it?']"]},"metadata":{},"execution_count":13}],"source":["input_texts[:10]"]},{"cell_type":"code","source":["target_texts[:10]"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"tH1Ogq9VM9ob","executionInfo":{"status":"ok","timestamp":1666551381232,"user_tz":-330,"elapsed":7,"user":{"displayName":"Gouthaman S","userId":"11145458456231366145"}},"outputId":"76eacd74-d08a-43d3-8c08-2dc9e0c45583"},"execution_count":14,"outputs":[{"output_type":"execute_result","data":{"text/plain":["['\\tवाह!\\n',\n"," '\\tबचाओ!\\n',\n"," '\\tउछलो.\\n',\n"," '\\tकूदो.\\n',\n"," '\\tछलांग.\\n',\n"," '\\tनमस्ते।\\n',\n"," '\\tनमस्कार।\\n',\n"," '\\tवाह-वाह!\\n',\n"," '\\tचियर्स!\\n',\n"," '\\tसमझे कि नहीं?\\n']"]},"metadata":{},"execution_count":14}]},{"cell_type":"code","source":["len(input_character)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"9OOANL_EA1tm","executionInfo":{"status":"ok","timestamp":1666548248821,"user_tz":-330,"elapsed":576,"user":{"displayName":"Gouthaman S","userId":"11145458456231366145"}},"outputId":"ed451704-2dcf-4e4d-e4a6-28d222857972"},"execution_count":29,"outputs":[{"output_type":"execute_result","data":{"text/plain":["70"]},"metadata":{},"execution_count":29}]},{"cell_type":"code","source":["input_characters = sorted(list(input_characters))\n","target_characters = sorted(list(target_characters))\n","\n","num_encoder_tokens = len(input_characters)\n","num_decoder_tokens = len(target_characters)\n","\n","max_encoder_seq_length = max([len(txt) for txt in input_texts])\n","max_decoder_seq_length = max([len(txt) for txt in target_texts])"],"metadata":{"id":"AAXbjB8RNEtS","executionInfo":{"status":"ok","timestamp":1666551412190,"user_tz":-330,"elapsed":7,"user":{"displayName":"Gouthaman S","userId":"11145458456231366145"}}},"execution_count":15,"outputs":[]},{"cell_type":"code","source":["print(\"Number of samples:\", len(input_texts))\n","print(\"Number of unique input tokens:\", num_encoder_tokens)\n","print(\"Number of unique output tokens:\", num_decoder_tokens)\n","print(\"Max sequence length for inputs:\", max_encoder_seq_length)\n","print(\"Max sequence length for outputs:\", max_decoder_seq_length)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"IlR2qCTrNL-w","executionInfo":{"status":"ok","timestamp":1666551435848,"user_tz":-330,"elapsed":10,"user":{"displayName":"Gouthaman S","userId":"11145458456231366145"}},"outputId":"490a65ac-1e5a-495b-8950-cf148b854377"},"execution_count":16,"outputs":[{"output_type":"stream","name":"stdout","text":["Number of samples: 2867\n","Number of unique input tokens: 70\n","Number of unique output tokens: 92\n","Max sequence length for inputs: 124\n","Max sequence length for outputs: 123\n"]}]},{"cell_type":"code","source":["input_token_index = dict([(char, i) for i, char in enumerate(input_characters)])\n","target_token_index = dict([(char, i) for i, char in enumerate(target_characters)])"],"metadata":{"id":"H3FRvC8xNNzv","executionInfo":{"status":"ok","timestamp":1666551456525,"user_tz":-330,"elapsed":617,"user":{"displayName":"Gouthaman S","userId":"11145458456231366145"}}},"execution_count":17,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"UllAqnJ0Zppk"},"source":["# Decalaring the dimensions"]},{"cell_type":"code","source":["encoder_input_data = np.zeros(\n","    (len(input_texts), max_encoder_seq_length, num_encoder_tokens), dtype=\"float32\"\n",")"],"metadata":{"id":"Un_XVI1dNJty","executionInfo":{"status":"ok","timestamp":1666551469588,"user_tz":-330,"elapsed":8,"user":{"displayName":"Gouthaman S","userId":"11145458456231366145"}}},"execution_count":18,"outputs":[]},{"cell_type":"code","source":["encoder_input_data.shape\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"MAqwrr8jvkDH","executionInfo":{"status":"ok","timestamp":1666551491254,"user_tz":-330,"elapsed":11,"user":{"displayName":"Gouthaman S","userId":"11145458456231366145"}},"outputId":"af45a831-1470-4dd6-dc6b-ac474be364b9"},"execution_count":20,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(2867, 124, 70)"]},"metadata":{},"execution_count":20}]},{"cell_type":"code","source":["decoder_input_data = np.zeros(\n","    (len(input_texts), max_decoder_seq_length, num_decoder_tokens), dtype=\"float32\"\n",")"],"metadata":{"id":"iUj2Zz0ENdGK","executionInfo":{"status":"ok","timestamp":1666551513317,"user_tz":-330,"elapsed":7,"user":{"displayName":"Gouthaman S","userId":"11145458456231366145"}}},"execution_count":21,"outputs":[]},{"cell_type":"code","source":["decoder_input_data.shape"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"qzZ9uvByNf9l","executionInfo":{"status":"ok","timestamp":1666551525233,"user_tz":-330,"elapsed":18,"user":{"displayName":"Gouthaman S","userId":"11145458456231366145"}},"outputId":"464d8308-797c-45a0-c9b5-b2a366cd038e"},"execution_count":23,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(2867, 123, 92)"]},"metadata":{},"execution_count":23}]},{"cell_type":"code","source":["\n","decoder_target_data = np.zeros(\n","    (len(input_texts), max_decoder_seq_length, num_decoder_tokens), dtype=\"float32\"\n",")"],"metadata":{"id":"-xk71TkoNlfn","executionInfo":{"status":"ok","timestamp":1666551538826,"user_tz":-330,"elapsed":7,"user":{"displayName":"Gouthaman S","userId":"11145458456231366145"}}},"execution_count":24,"outputs":[]},{"cell_type":"code","source":["decoder_target_data.shape"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Y0T8wPFENnIC","executionInfo":{"status":"ok","timestamp":1666551557028,"user_tz":-330,"elapsed":11,"user":{"displayName":"Gouthaman S","userId":"11145458456231366145"}},"outputId":"0483ca06-da5f-476c-e067-c1c69de9785b"},"execution_count":25,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(2867, 123, 92)"]},"metadata":{},"execution_count":25}]},{"cell_type":"code","execution_count":26,"metadata":{"id":"xi_xoRS2Zppn","executionInfo":{"status":"ok","timestamp":1666551886010,"user_tz":-330,"elapsed":711,"user":{"displayName":"Gouthaman S","userId":"11145458456231366145"}}},"outputs":[],"source":["\n","for i, (input_text, target_text) in enumerate(zip(input_texts, target_texts)):\n","    for t, char in enumerate(input_text):\n","        encoder_input_data[i, t, input_token_index[char]] = 1.0\n","        encoder_input_data[i, t + 1 :, input_token_index[\" \"]] = 1.0\n","    for t, char in enumerate(target_text):\n","        # decoder_target_data is ahead of decoder_input_data by one timestep\n","        decoder_input_data[i, t, target_token_index[char]] = 1.0\n","        if t > 0:\n","            # decoder_target_data will be ahead by one timestep\n","            # and will not include the start character.\n","            decoder_target_data[i, t - 1, target_token_index[char]] = 1.0\n","            \n","    decoder_input_data[i, t + 1 :, target_token_index[\" \"]] = 1.0\n","    decoder_target_data[i, t:, target_token_index[\" \"]] = 1.0\n"]},{"cell_type":"markdown","metadata":{"id":"hm9DQ03pZppv"},"source":["# One Hot Representation"]},{"cell_type":"code","source":["'''for i in range(len(input_texts)):\n","    for indx, charc in enumerate(input_texts[i]):\n","        encoder_input_data[i,indx,input_character_char_indx[charc]] =1.0\n","        encoder_input_data[i,indx+1:,input_character_char_indx[' ']]=1.0\n","    \n","    for indx, charc in enumerate(target_texts[i]):\n","        decoder_input_data[i,indx,target_character_char_indx[char]]=1.0\n","    \n","    #Decoder will not have first input (it will come form int encoder )\n","    # that is decoder will be ahead of one time step\n","        if indx>0:    \n","            decoder_output_data[i,indx-1,target_character_char_indx[char]] =1.0\n","    \n","    decoder_input_data[i,indx+1:,target_character_char_indx[' ']]=1.0\n","    decoder_output_data[i,indx:,target_character_char_indx[' ']]=1.0'''"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":122},"id":"2S3iS5reH5mT","executionInfo":{"status":"ok","timestamp":1666550069005,"user_tz":-330,"elapsed":524,"user":{"displayName":"Gouthaman S","userId":"11145458456231366145"}},"outputId":"80aa3eca-6123-425b-a716-291f18d22474"},"execution_count":60,"outputs":[{"output_type":"execute_result","data":{"text/plain":["\"for i in range(len(input_texts)):\\n    for indx, charc in enumerate(input_texts[i]):\\n        encoder_input_data[i,indx,input_character_char_indx[charc]] =1.0\\n        encoder_input_data[i,indx+1:,input_character_char_indx[' ']]=1.0\\n    \\n    for indx, charc in enumerate(target_texts[i]):\\n        decoder_input_data[i,indx,target_character_char_indx[char]]=1.0\\n    \\n    #Decoder will not have first input (it will come form int encoder )\\n    # that is decoder will be ahead of one time step\\n        if indx>0:    \\n            decoder_output_data[i,indx-1,target_character_char_indx[char]] =1.0\\n    \\n    decoder_input_data[i,indx+1:,target_character_char_indx[' ']]=1.0\\n    decoder_output_data[i,indx:,target_character_char_indx[' ']]=1.0\""],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":60}]},{"cell_type":"markdown","metadata":{"id":"HJNEMg09Zpp5"},"source":["# Model Building"]},{"cell_type":"markdown","metadata":{"id":"IcY6pU-cZpp5"},"source":["### Encoder"]},{"cell_type":"code","execution_count":29,"metadata":{"id":"8EPDMEF4Zpp7","executionInfo":{"status":"ok","timestamp":1666551940614,"user_tz":-330,"elapsed":2741,"user":{"displayName":"Gouthaman S","userId":"11145458456231366145"}}},"outputs":[],"source":["# Define an input sequence and process it.\n","encoder_inputs = keras.Input(shape=(None, num_encoder_tokens))\n","encoder = keras.layers.LSTM(latent_dim, return_state=True)\n","encoder_outputs, state_h, state_c = encoder(encoder_inputs)\n","\n","# We discard `encoder_outputs` and only keep the states.\n","encoder_states = [state_h, state_c]\n","\n","# Set up the decoder, using `encoder_states` as initial state.\n","decoder_inputs = keras.Input(shape=(None, num_decoder_tokens))\n","\n","# We set up our decoder to return full output sequences, and to return internal states as well. We don't use the\n","# return states in the training model, but we will use them in inference.\n","decoder_lstm = keras.layers.LSTM(latent_dim, return_sequences=True, return_state=True)\n","decoder_outputs, _, _ = decoder_lstm(decoder_inputs, initial_state=encoder_states)\n","decoder_dense = keras.layers.Dense(num_decoder_tokens, activation=\"softmax\")\n","decoder_outputs = decoder_dense(decoder_outputs)\n","\n","# Define the model that will turn\n","# `encoder_input_data` & `decoder_input_data` into `decoder_target_data`\n","model = keras.Model([encoder_inputs, decoder_inputs], decoder_outputs)"]},{"cell_type":"code","source":["import keras"],"metadata":{"id":"tdNznWfwPFWb","executionInfo":{"status":"ok","timestamp":1666551934649,"user_tz":-330,"elapsed":8,"user":{"displayName":"Gouthaman S","userId":"11145458456231366145"}}},"execution_count":28,"outputs":[]},{"cell_type":"code","execution_count":30,"metadata":{"id":"FoX7arqOZpp_","executionInfo":{"status":"ok","timestamp":1666551971250,"user_tz":-330,"elapsed":7,"user":{"displayName":"Gouthaman S","userId":"11145458456231366145"}}},"outputs":[],"source":["from keras.callbacks import EarlyStopping,ModelCheckpoint\n","\n","es = EarlyStopping(monitor='val_loss',mode='min',verbose=1, patience=200)\n","mc = ModelCheckpoint('hinditoenglishmodel.h5',monitor='val_accuracy', mode='max', save_best_only=True,verbose=1)\n"]},{"cell_type":"code","execution_count":31,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"PRsBZKa7Zpp_","executionInfo":{"status":"ok","timestamp":1666552286750,"user_tz":-330,"elapsed":286996,"user":{"displayName":"Gouthaman S","userId":"11145458456231366145"}},"outputId":"e8e8cf25-536f-4665-c092-edbe90006350"},"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1/1000\n","36/36 [==============================] - ETA: 0s - loss: 1.3116 - accuracy: 0.7844\n","Epoch 1: val_accuracy improved from -inf to 0.68070, saving model to hinditoenglishmodel.h5\n","36/36 [==============================] - 12s 107ms/step - loss: 1.3116 - accuracy: 0.7844 - val_loss: 1.7285 - val_accuracy: 0.6807\n","Epoch 2/1000\n","36/36 [==============================] - ETA: 0s - loss: 0.8880 - accuracy: 0.8072\n","Epoch 2: val_accuracy improved from 0.68070 to 0.68784, saving model to hinditoenglishmodel.h5\n","36/36 [==============================] - 1s 33ms/step - loss: 0.8880 - accuracy: 0.8072 - val_loss: 2.1520 - val_accuracy: 0.6878\n","Epoch 3/1000\n","34/36 [===========================>..] - ETA: 0s - loss: 0.8458 - accuracy: 0.8086\n","Epoch 3: val_accuracy improved from 0.68784 to 0.69001, saving model to hinditoenglishmodel.h5\n","36/36 [==============================] - 1s 30ms/step - loss: 0.8444 - accuracy: 0.8087 - val_loss: 1.5113 - val_accuracy: 0.6900\n","Epoch 4/1000\n","36/36 [==============================] - ETA: 0s - loss: 0.7981 - accuracy: 0.8104\n","Epoch 4: val_accuracy did not improve from 0.69001\n","36/36 [==============================] - 1s 30ms/step - loss: 0.7981 - accuracy: 0.8104 - val_loss: 1.3388 - val_accuracy: 0.6894\n","Epoch 5/1000\n","35/36 [============================>.] - ETA: 0s - loss: 0.7502 - accuracy: 0.8137\n","Epoch 5: val_accuracy improved from 0.69001 to 0.69052, saving model to hinditoenglishmodel.h5\n","36/36 [==============================] - 1s 30ms/step - loss: 0.7507 - accuracy: 0.8138 - val_loss: 1.7677 - val_accuracy: 0.6905\n","Epoch 6/1000\n","34/36 [===========================>..] - ETA: 0s - loss: 0.7201 - accuracy: 0.8239\n","Epoch 6: val_accuracy improved from 0.69052 to 0.69294, saving model to hinditoenglishmodel.h5\n","36/36 [==============================] - 1s 31ms/step - loss: 0.7195 - accuracy: 0.8241 - val_loss: 1.5786 - val_accuracy: 0.6929\n","Epoch 7/1000\n","35/36 [============================>.] - ETA: 0s - loss: 0.6550 - accuracy: 0.8362\n","Epoch 7: val_accuracy improved from 0.69294 to 0.72336, saving model to hinditoenglishmodel.h5\n","36/36 [==============================] - 1s 31ms/step - loss: 0.6541 - accuracy: 0.8366 - val_loss: 1.0718 - val_accuracy: 0.7234\n","Epoch 8/1000\n","36/36 [==============================] - ETA: 0s - loss: 0.6217 - accuracy: 0.8456\n","Epoch 8: val_accuracy improved from 0.72336 to 0.73665, saving model to hinditoenglishmodel.h5\n","36/36 [==============================] - 1s 31ms/step - loss: 0.6217 - accuracy: 0.8456 - val_loss: 1.0228 - val_accuracy: 0.7367\n","Epoch 9/1000\n","35/36 [============================>.] - ETA: 0s - loss: 0.5795 - accuracy: 0.8510\n","Epoch 9: val_accuracy did not improve from 0.73665\n","36/36 [==============================] - 1s 29ms/step - loss: 0.5791 - accuracy: 0.8512 - val_loss: 1.3141 - val_accuracy: 0.7201\n","Epoch 10/1000\n","36/36 [==============================] - ETA: 0s - loss: 0.5517 - accuracy: 0.8557\n","Epoch 10: val_accuracy improved from 0.73665 to 0.74644, saving model to hinditoenglishmodel.h5\n","36/36 [==============================] - 1s 31ms/step - loss: 0.5517 - accuracy: 0.8557 - val_loss: 0.9610 - val_accuracy: 0.7464\n","Epoch 11/1000\n","36/36 [==============================] - ETA: 0s - loss: 0.5285 - accuracy: 0.8614\n","Epoch 11: val_accuracy improved from 0.74644 to 0.74761, saving model to hinditoenglishmodel.h5\n","36/36 [==============================] - 2s 42ms/step - loss: 0.5285 - accuracy: 0.8614 - val_loss: 0.9710 - val_accuracy: 0.7476\n","Epoch 12/1000\n","35/36 [============================>.] - ETA: 0s - loss: 0.5117 - accuracy: 0.8651\n","Epoch 12: val_accuracy improved from 0.74761 to 0.75729, saving model to hinditoenglishmodel.h5\n","36/36 [==============================] - 1s 31ms/step - loss: 0.5118 - accuracy: 0.8650 - val_loss: 0.9182 - val_accuracy: 0.7573\n","Epoch 13/1000\n","36/36 [==============================] - ETA: 0s - loss: 0.4967 - accuracy: 0.8679\n","Epoch 13: val_accuracy improved from 0.75729 to 0.76965, saving model to hinditoenglishmodel.h5\n","36/36 [==============================] - 1s 31ms/step - loss: 0.4967 - accuracy: 0.8679 - val_loss: 0.8771 - val_accuracy: 0.7697\n","Epoch 14/1000\n","35/36 [============================>.] - ETA: 0s - loss: 0.4848 - accuracy: 0.8712\n","Epoch 14: val_accuracy did not improve from 0.76965\n","36/36 [==============================] - 1s 29ms/step - loss: 0.4842 - accuracy: 0.8713 - val_loss: 0.8690 - val_accuracy: 0.7671\n","Epoch 15/1000\n","35/36 [============================>.] - ETA: 0s - loss: 0.4753 - accuracy: 0.8729\n","Epoch 15: val_accuracy improved from 0.76965 to 0.77198, saving model to hinditoenglishmodel.h5\n","36/36 [==============================] - 1s 31ms/step - loss: 0.4749 - accuracy: 0.8729 - val_loss: 0.8497 - val_accuracy: 0.7720\n","Epoch 16/1000\n","36/36 [==============================] - ETA: 0s - loss: 0.4641 - accuracy: 0.8746\n","Epoch 16: val_accuracy improved from 0.77198 to 0.77251, saving model to hinditoenglishmodel.h5\n","36/36 [==============================] - 1s 31ms/step - loss: 0.4641 - accuracy: 0.8746 - val_loss: 0.8455 - val_accuracy: 0.7725\n","Epoch 17/1000\n","34/36 [===========================>..] - ETA: 0s - loss: 0.4568 - accuracy: 0.8759\n","Epoch 17: val_accuracy improved from 0.77251 to 0.77584, saving model to hinditoenglishmodel.h5\n","36/36 [==============================] - 1s 31ms/step - loss: 0.4558 - accuracy: 0.8761 - val_loss: 0.8287 - val_accuracy: 0.7758\n","Epoch 18/1000\n","35/36 [============================>.] - ETA: 0s - loss: 0.4480 - accuracy: 0.8774\n","Epoch 18: val_accuracy improved from 0.77584 to 0.78016, saving model to hinditoenglishmodel.h5\n","36/36 [==============================] - 1s 31ms/step - loss: 0.4474 - accuracy: 0.8776 - val_loss: 0.8101 - val_accuracy: 0.7802\n","Epoch 19/1000\n","36/36 [==============================] - ETA: 0s - loss: 0.4407 - accuracy: 0.8788\n","Epoch 19: val_accuracy did not improve from 0.78016\n","36/36 [==============================] - 1s 30ms/step - loss: 0.4407 - accuracy: 0.8788 - val_loss: 0.8211 - val_accuracy: 0.7765\n","Epoch 20/1000\n","36/36 [==============================] - ETA: 0s - loss: 0.4333 - accuracy: 0.8800\n","Epoch 20: val_accuracy improved from 0.78016 to 0.78047, saving model to hinditoenglishmodel.h5\n","36/36 [==============================] - 1s 31ms/step - loss: 0.4333 - accuracy: 0.8800 - val_loss: 0.8060 - val_accuracy: 0.7805\n","Epoch 21/1000\n","35/36 [============================>.] - ETA: 0s - loss: 0.4261 - accuracy: 0.8816\n","Epoch 21: val_accuracy did not improve from 0.78047\n","36/36 [==============================] - 1s 29ms/step - loss: 0.4264 - accuracy: 0.8815 - val_loss: 0.8077 - val_accuracy: 0.7799\n","Epoch 22/1000\n","35/36 [============================>.] - ETA: 0s - loss: 0.4196 - accuracy: 0.8834\n","Epoch 22: val_accuracy improved from 0.78047 to 0.78122, saving model to hinditoenglishmodel.h5\n","36/36 [==============================] - 1s 32ms/step - loss: 0.4203 - accuracy: 0.8833 - val_loss: 0.8016 - val_accuracy: 0.7812\n","Epoch 23/1000\n","35/36 [============================>.] - ETA: 0s - loss: 0.4140 - accuracy: 0.8848\n","Epoch 23: val_accuracy improved from 0.78122 to 0.78484, saving model to hinditoenglishmodel.h5\n","36/36 [==============================] - 1s 32ms/step - loss: 0.4141 - accuracy: 0.8848 - val_loss: 0.7878 - val_accuracy: 0.7848\n","Epoch 24/1000\n","36/36 [==============================] - ETA: 0s - loss: 0.4080 - accuracy: 0.8863\n","Epoch 24: val_accuracy did not improve from 0.78484\n","36/36 [==============================] - 1s 30ms/step - loss: 0.4080 - accuracy: 0.8863 - val_loss: 0.8018 - val_accuracy: 0.7806\n","Epoch 25/1000\n","34/36 [===========================>..] - ETA: 0s - loss: 0.4023 - accuracy: 0.8872\n","Epoch 25: val_accuracy improved from 0.78484 to 0.78802, saving model to hinditoenglishmodel.h5\n","36/36 [==============================] - 1s 31ms/step - loss: 0.4018 - accuracy: 0.8875 - val_loss: 0.7731 - val_accuracy: 0.7880\n","Epoch 26/1000\n","34/36 [===========================>..] - ETA: 0s - loss: 0.3958 - accuracy: 0.8892\n","Epoch 26: val_accuracy improved from 0.78802 to 0.78896, saving model to hinditoenglishmodel.h5\n","36/36 [==============================] - 1s 32ms/step - loss: 0.3964 - accuracy: 0.8889 - val_loss: 0.7734 - val_accuracy: 0.7890\n","Epoch 27/1000\n","35/36 [============================>.] - ETA: 0s - loss: 0.3907 - accuracy: 0.8901\n","Epoch 27: val_accuracy improved from 0.78896 to 0.78913, saving model to hinditoenglishmodel.h5\n","36/36 [==============================] - 1s 32ms/step - loss: 0.3911 - accuracy: 0.8899 - val_loss: 0.7667 - val_accuracy: 0.7891\n","Epoch 28/1000\n","35/36 [============================>.] - ETA: 0s - loss: 0.3882 - accuracy: 0.8910\n","Epoch 28: val_accuracy improved from 0.78913 to 0.78926, saving model to hinditoenglishmodel.h5\n","36/36 [==============================] - 1s 32ms/step - loss: 0.3873 - accuracy: 0.8913 - val_loss: 0.7736 - val_accuracy: 0.7893\n","Epoch 29/1000\n","35/36 [============================>.] - ETA: 0s - loss: 0.3798 - accuracy: 0.8930\n","Epoch 29: val_accuracy improved from 0.78926 to 0.79110, saving model to hinditoenglishmodel.h5\n","36/36 [==============================] - 1s 30ms/step - loss: 0.3806 - accuracy: 0.8929 - val_loss: 0.7618 - val_accuracy: 0.7911\n","Epoch 30/1000\n","35/36 [============================>.] - ETA: 0s - loss: 0.3748 - accuracy: 0.8947\n","Epoch 30: val_accuracy improved from 0.79110 to 0.79192, saving model to hinditoenglishmodel.h5\n","36/36 [==============================] - 1s 31ms/step - loss: 0.3749 - accuracy: 0.8947 - val_loss: 0.7657 - val_accuracy: 0.7919\n","Epoch 31/1000\n","34/36 [===========================>..] - ETA: 0s - loss: 0.3706 - accuracy: 0.8955\n","Epoch 31: val_accuracy did not improve from 0.79192\n","36/36 [==============================] - 1s 29ms/step - loss: 0.3698 - accuracy: 0.8956 - val_loss: 0.7679 - val_accuracy: 0.7919\n","Epoch 32/1000\n","36/36 [==============================] - ETA: 0s - loss: 0.3646 - accuracy: 0.8969\n","Epoch 32: val_accuracy improved from 0.79192 to 0.79329, saving model to hinditoenglishmodel.h5\n","36/36 [==============================] - 1s 32ms/step - loss: 0.3646 - accuracy: 0.8969 - val_loss: 0.7576 - val_accuracy: 0.7933\n","Epoch 33/1000\n","36/36 [==============================] - ETA: 0s - loss: 0.3617 - accuracy: 0.8979\n","Epoch 33: val_accuracy improved from 0.79329 to 0.79331, saving model to hinditoenglishmodel.h5\n","36/36 [==============================] - 1s 31ms/step - loss: 0.3617 - accuracy: 0.8979 - val_loss: 0.7603 - val_accuracy: 0.7933\n","Epoch 34/1000\n","34/36 [===========================>..] - ETA: 0s - loss: 0.3540 - accuracy: 0.9001\n","Epoch 34: val_accuracy improved from 0.79331 to 0.79450, saving model to hinditoenglishmodel.h5\n","36/36 [==============================] - 1s 31ms/step - loss: 0.3550 - accuracy: 0.8999 - val_loss: 0.7592 - val_accuracy: 0.7945\n","Epoch 35/1000\n","35/36 [============================>.] - ETA: 0s - loss: 0.3509 - accuracy: 0.9008\n","Epoch 35: val_accuracy did not improve from 0.79450\n","36/36 [==============================] - 1s 30ms/step - loss: 0.3506 - accuracy: 0.9009 - val_loss: 0.7594 - val_accuracy: 0.7935\n","Epoch 36/1000\n","35/36 [============================>.] - ETA: 0s - loss: 0.3451 - accuracy: 0.9027\n","Epoch 36: val_accuracy did not improve from 0.79450\n","36/36 [==============================] - 1s 30ms/step - loss: 0.3453 - accuracy: 0.9026 - val_loss: 0.7587 - val_accuracy: 0.7935\n","Epoch 37/1000\n","35/36 [============================>.] - ETA: 0s - loss: 0.3407 - accuracy: 0.9039\n","Epoch 37: val_accuracy did not improve from 0.79450\n","36/36 [==============================] - 1s 31ms/step - loss: 0.3407 - accuracy: 0.9039 - val_loss: 0.7651 - val_accuracy: 0.7929\n","Epoch 38/1000\n","35/36 [============================>.] - ETA: 0s - loss: 0.3354 - accuracy: 0.9052\n","Epoch 38: val_accuracy did not improve from 0.79450\n","36/36 [==============================] - 1s 30ms/step - loss: 0.3354 - accuracy: 0.9052 - val_loss: 0.7627 - val_accuracy: 0.7940\n","Epoch 39/1000\n","34/36 [===========================>..] - ETA: 0s - loss: 0.3289 - accuracy: 0.9070\n","Epoch 39: val_accuracy improved from 0.79450 to 0.79611, saving model to hinditoenglishmodel.h5\n","36/36 [==============================] - 1s 31ms/step - loss: 0.3295 - accuracy: 0.9069 - val_loss: 0.7562 - val_accuracy: 0.7961\n","Epoch 40/1000\n","35/36 [============================>.] - ETA: 0s - loss: 0.3246 - accuracy: 0.9082\n","Epoch 40: val_accuracy did not improve from 0.79611\n","36/36 [==============================] - 1s 30ms/step - loss: 0.3250 - accuracy: 0.9081 - val_loss: 0.7740 - val_accuracy: 0.7923\n","Epoch 41/1000\n","35/36 [============================>.] - ETA: 0s - loss: 0.3201 - accuracy: 0.9097\n","Epoch 41: val_accuracy did not improve from 0.79611\n","36/36 [==============================] - 1s 29ms/step - loss: 0.3198 - accuracy: 0.9097 - val_loss: 0.7754 - val_accuracy: 0.7940\n","Epoch 42/1000\n","36/36 [==============================] - ETA: 0s - loss: 0.3142 - accuracy: 0.9112\n","Epoch 42: val_accuracy did not improve from 0.79611\n","36/36 [==============================] - 1s 30ms/step - loss: 0.3142 - accuracy: 0.9112 - val_loss: 0.7614 - val_accuracy: 0.7951\n","Epoch 43/1000\n","36/36 [==============================] - ETA: 0s - loss: 0.3093 - accuracy: 0.9125\n","Epoch 43: val_accuracy did not improve from 0.79611\n","36/36 [==============================] - 1s 29ms/step - loss: 0.3093 - accuracy: 0.9125 - val_loss: 0.7784 - val_accuracy: 0.7946\n","Epoch 44/1000\n","35/36 [============================>.] - ETA: 0s - loss: 0.3049 - accuracy: 0.9138\n","Epoch 44: val_accuracy did not improve from 0.79611\n","36/36 [==============================] - 1s 30ms/step - loss: 0.3044 - accuracy: 0.9140 - val_loss: 0.7725 - val_accuracy: 0.7946\n","Epoch 45/1000\n","35/36 [============================>.] - ETA: 0s - loss: 0.2996 - accuracy: 0.9152\n","Epoch 45: val_accuracy did not improve from 0.79611\n","36/36 [==============================] - 1s 29ms/step - loss: 0.2992 - accuracy: 0.9154 - val_loss: 0.7900 - val_accuracy: 0.7925\n","Epoch 46/1000\n","35/36 [============================>.] - ETA: 0s - loss: 0.2950 - accuracy: 0.9163\n","Epoch 46: val_accuracy did not improve from 0.79611\n","36/36 [==============================] - 1s 30ms/step - loss: 0.2944 - accuracy: 0.9165 - val_loss: 0.7960 - val_accuracy: 0.7932\n","Epoch 47/1000\n","35/36 [============================>.] - ETA: 0s - loss: 0.2885 - accuracy: 0.9184\n","Epoch 47: val_accuracy did not improve from 0.79611\n","36/36 [==============================] - 1s 31ms/step - loss: 0.2891 - accuracy: 0.9182 - val_loss: 0.8031 - val_accuracy: 0.7919\n","Epoch 48/1000\n","35/36 [============================>.] - ETA: 0s - loss: 0.2843 - accuracy: 0.9195\n","Epoch 48: val_accuracy did not improve from 0.79611\n","36/36 [==============================] - 1s 31ms/step - loss: 0.2845 - accuracy: 0.9194 - val_loss: 0.7957 - val_accuracy: 0.7938\n","Epoch 49/1000\n","35/36 [============================>.] - ETA: 0s - loss: 0.2802 - accuracy: 0.9208\n","Epoch 49: val_accuracy did not improve from 0.79611\n","36/36 [==============================] - 1s 30ms/step - loss: 0.2797 - accuracy: 0.9209 - val_loss: 0.8055 - val_accuracy: 0.7936\n","Epoch 50/1000\n","34/36 [===========================>..] - ETA: 0s - loss: 0.2745 - accuracy: 0.9227\n","Epoch 50: val_accuracy did not improve from 0.79611\n","36/36 [==============================] - 1s 30ms/step - loss: 0.2751 - accuracy: 0.9226 - val_loss: 0.8221 - val_accuracy: 0.7892\n","Epoch 51/1000\n","35/36 [============================>.] - ETA: 0s - loss: 0.2705 - accuracy: 0.9239\n","Epoch 51: val_accuracy did not improve from 0.79611\n","36/36 [==============================] - 1s 31ms/step - loss: 0.2704 - accuracy: 0.9239 - val_loss: 0.8223 - val_accuracy: 0.7920\n","Epoch 52/1000\n","36/36 [==============================] - ETA: 0s - loss: 0.2653 - accuracy: 0.9251\n","Epoch 52: val_accuracy did not improve from 0.79611\n","36/36 [==============================] - 1s 31ms/step - loss: 0.2653 - accuracy: 0.9251 - val_loss: 0.8310 - val_accuracy: 0.7894\n","Epoch 53/1000\n","35/36 [============================>.] - ETA: 0s - loss: 0.2612 - accuracy: 0.9263\n","Epoch 53: val_accuracy did not improve from 0.79611\n","36/36 [==============================] - 1s 30ms/step - loss: 0.2616 - accuracy: 0.9263 - val_loss: 0.8410 - val_accuracy: 0.7889\n","Epoch 54/1000\n","36/36 [==============================] - ETA: 0s - loss: 0.2565 - accuracy: 0.9278\n","Epoch 54: val_accuracy did not improve from 0.79611\n","36/36 [==============================] - 1s 30ms/step - loss: 0.2565 - accuracy: 0.9278 - val_loss: 0.8479 - val_accuracy: 0.7890\n","Epoch 55/1000\n","35/36 [============================>.] - ETA: 0s - loss: 0.2525 - accuracy: 0.9290\n","Epoch 55: val_accuracy did not improve from 0.79611\n","36/36 [==============================] - 1s 30ms/step - loss: 0.2530 - accuracy: 0.9289 - val_loss: 0.8526 - val_accuracy: 0.7874\n","Epoch 56/1000\n","36/36 [==============================] - ETA: 0s - loss: 0.2482 - accuracy: 0.9304\n","Epoch 56: val_accuracy did not improve from 0.79611\n","36/36 [==============================] - 1s 30ms/step - loss: 0.2482 - accuracy: 0.9304 - val_loss: 0.8524 - val_accuracy: 0.7890\n","Epoch 57/1000\n","35/36 [============================>.] - ETA: 0s - loss: 0.2442 - accuracy: 0.9313\n","Epoch 57: val_accuracy did not improve from 0.79611\n","36/36 [==============================] - 1s 31ms/step - loss: 0.2445 - accuracy: 0.9313 - val_loss: 0.8547 - val_accuracy: 0.7881\n","Epoch 58/1000\n","35/36 [============================>.] - ETA: 0s - loss: 0.2410 - accuracy: 0.9323\n","Epoch 58: val_accuracy did not improve from 0.79611\n","36/36 [==============================] - 1s 30ms/step - loss: 0.2406 - accuracy: 0.9324 - val_loss: 0.8629 - val_accuracy: 0.7883\n","Epoch 59/1000\n","35/36 [============================>.] - ETA: 0s - loss: 0.2370 - accuracy: 0.9336\n","Epoch 59: val_accuracy did not improve from 0.79611\n","36/36 [==============================] - 1s 30ms/step - loss: 0.2371 - accuracy: 0.9335 - val_loss: 0.8851 - val_accuracy: 0.7866\n","Epoch 60/1000\n","36/36 [==============================] - ETA: 0s - loss: 0.2330 - accuracy: 0.9351\n","Epoch 60: val_accuracy did not improve from 0.79611\n","36/36 [==============================] - 1s 30ms/step - loss: 0.2330 - accuracy: 0.9351 - val_loss: 0.8890 - val_accuracy: 0.7865\n","Epoch 61/1000\n","35/36 [============================>.] - ETA: 0s - loss: 0.2303 - accuracy: 0.9357\n","Epoch 61: val_accuracy did not improve from 0.79611\n","36/36 [==============================] - 1s 30ms/step - loss: 0.2312 - accuracy: 0.9355 - val_loss: 0.8932 - val_accuracy: 0.7866\n","Epoch 62/1000\n","35/36 [============================>.] - ETA: 0s - loss: 0.2263 - accuracy: 0.9369\n","Epoch 62: val_accuracy did not improve from 0.79611\n","36/36 [==============================] - 1s 31ms/step - loss: 0.2266 - accuracy: 0.9368 - val_loss: 0.9060 - val_accuracy: 0.7835\n","Epoch 63/1000\n","34/36 [===========================>..] - ETA: 0s - loss: 0.2217 - accuracy: 0.9382\n","Epoch 63: val_accuracy did not improve from 0.79611\n","36/36 [==============================] - 1s 30ms/step - loss: 0.2225 - accuracy: 0.9379 - val_loss: 0.9022 - val_accuracy: 0.7851\n","Epoch 64/1000\n","35/36 [============================>.] - ETA: 0s - loss: 0.2183 - accuracy: 0.9392\n","Epoch 64: val_accuracy did not improve from 0.79611\n","36/36 [==============================] - 1s 31ms/step - loss: 0.2184 - accuracy: 0.9392 - val_loss: 0.9080 - val_accuracy: 0.7852\n","Epoch 65/1000\n","35/36 [============================>.] - ETA: 0s - loss: 0.2150 - accuracy: 0.9400\n","Epoch 65: val_accuracy did not improve from 0.79611\n","36/36 [==============================] - 1s 30ms/step - loss: 0.2153 - accuracy: 0.9398 - val_loss: 0.9275 - val_accuracy: 0.7839\n","Epoch 66/1000\n","35/36 [============================>.] - ETA: 0s - loss: 0.2106 - accuracy: 0.9420\n","Epoch 66: val_accuracy did not improve from 0.79611\n","36/36 [==============================] - 1s 31ms/step - loss: 0.2109 - accuracy: 0.9419 - val_loss: 0.9395 - val_accuracy: 0.7818\n","Epoch 67/1000\n","35/36 [============================>.] - ETA: 0s - loss: 0.2093 - accuracy: 0.9421\n","Epoch 67: val_accuracy did not improve from 0.79611\n","36/36 [==============================] - 1s 31ms/step - loss: 0.2092 - accuracy: 0.9421 - val_loss: 0.9305 - val_accuracy: 0.7857\n","Epoch 68/1000\n","35/36 [============================>.] - ETA: 0s - loss: 0.2044 - accuracy: 0.9435\n","Epoch 68: val_accuracy did not improve from 0.79611\n","36/36 [==============================] - 1s 31ms/step - loss: 0.2046 - accuracy: 0.9435 - val_loss: 0.9373 - val_accuracy: 0.7850\n","Epoch 69/1000\n","35/36 [============================>.] - ETA: 0s - loss: 0.2006 - accuracy: 0.9445\n","Epoch 69: val_accuracy did not improve from 0.79611\n","36/36 [==============================] - 1s 30ms/step - loss: 0.2010 - accuracy: 0.9445 - val_loss: 0.9571 - val_accuracy: 0.7832\n","Epoch 70/1000\n","35/36 [============================>.] - ETA: 0s - loss: 0.1978 - accuracy: 0.9452\n","Epoch 70: val_accuracy did not improve from 0.79611\n","36/36 [==============================] - 1s 30ms/step - loss: 0.1979 - accuracy: 0.9452 - val_loss: 0.9576 - val_accuracy: 0.7834\n","Epoch 71/1000\n","36/36 [==============================] - ETA: 0s - loss: 0.1938 - accuracy: 0.9468\n","Epoch 71: val_accuracy did not improve from 0.79611\n","36/36 [==============================] - 1s 31ms/step - loss: 0.1938 - accuracy: 0.9468 - val_loss: 0.9963 - val_accuracy: 0.7793\n","Epoch 72/1000\n","35/36 [============================>.] - ETA: 0s - loss: 0.1902 - accuracy: 0.9479\n","Epoch 72: val_accuracy did not improve from 0.79611\n","36/36 [==============================] - 1s 31ms/step - loss: 0.1908 - accuracy: 0.9477 - val_loss: 0.9732 - val_accuracy: 0.7811\n","Epoch 73/1000\n","35/36 [============================>.] - ETA: 0s - loss: 0.1891 - accuracy: 0.9483\n","Epoch 73: val_accuracy did not improve from 0.79611\n","36/36 [==============================] - 1s 31ms/step - loss: 0.1889 - accuracy: 0.9483 - val_loss: 0.9754 - val_accuracy: 0.7821\n","Epoch 74/1000\n","35/36 [============================>.] - ETA: 0s - loss: 0.1850 - accuracy: 0.9491\n","Epoch 74: val_accuracy did not improve from 0.79611\n","36/36 [==============================] - 1s 31ms/step - loss: 0.1853 - accuracy: 0.9490 - val_loss: 0.9943 - val_accuracy: 0.7818\n","Epoch 75/1000\n","35/36 [============================>.] - ETA: 0s - loss: 0.1830 - accuracy: 0.9500\n","Epoch 75: val_accuracy did not improve from 0.79611\n","36/36 [==============================] - 1s 31ms/step - loss: 0.1827 - accuracy: 0.9500 - val_loss: 0.9854 - val_accuracy: 0.7835\n","Epoch 76/1000\n","35/36 [============================>.] - ETA: 0s - loss: 0.1806 - accuracy: 0.9504\n","Epoch 76: val_accuracy did not improve from 0.79611\n","36/36 [==============================] - 1s 31ms/step - loss: 0.1807 - accuracy: 0.9504 - val_loss: 1.0025 - val_accuracy: 0.7817\n","Epoch 77/1000\n","35/36 [============================>.] - ETA: 0s - loss: 0.1803 - accuracy: 0.9506\n","Epoch 77: val_accuracy did not improve from 0.79611\n","36/36 [==============================] - 1s 31ms/step - loss: 0.1798 - accuracy: 0.9507 - val_loss: 1.0169 - val_accuracy: 0.7817\n","Epoch 78/1000\n","35/36 [============================>.] - ETA: 0s - loss: 0.1757 - accuracy: 0.9520\n","Epoch 78: val_accuracy did not improve from 0.79611\n","36/36 [==============================] - 1s 31ms/step - loss: 0.1758 - accuracy: 0.9520 - val_loss: 1.0162 - val_accuracy: 0.7808\n","Epoch 79/1000\n","36/36 [==============================] - ETA: 0s - loss: 0.1741 - accuracy: 0.9523\n","Epoch 79: val_accuracy did not improve from 0.79611\n","36/36 [==============================] - 1s 31ms/step - loss: 0.1741 - accuracy: 0.9523 - val_loss: 1.0341 - val_accuracy: 0.7796\n","Epoch 80/1000\n","36/36 [==============================] - ETA: 0s - loss: 0.1715 - accuracy: 0.9532\n","Epoch 80: val_accuracy did not improve from 0.79611\n","36/36 [==============================] - 1s 30ms/step - loss: 0.1715 - accuracy: 0.9532 - val_loss: 1.0278 - val_accuracy: 0.7818\n","Epoch 81/1000\n","35/36 [============================>.] - ETA: 0s - loss: 0.1672 - accuracy: 0.9548\n","Epoch 81: val_accuracy did not improve from 0.79611\n","36/36 [==============================] - 1s 31ms/step - loss: 0.1671 - accuracy: 0.9548 - val_loss: 1.0471 - val_accuracy: 0.7808\n","Epoch 82/1000\n","35/36 [============================>.] - ETA: 0s - loss: 0.1653 - accuracy: 0.9554\n","Epoch 82: val_accuracy did not improve from 0.79611\n","36/36 [==============================] - 1s 31ms/step - loss: 0.1654 - accuracy: 0.9554 - val_loss: 1.0559 - val_accuracy: 0.7797\n","Epoch 83/1000\n","35/36 [============================>.] - ETA: 0s - loss: 0.1633 - accuracy: 0.9557\n","Epoch 83: val_accuracy did not improve from 0.79611\n","36/36 [==============================] - 1s 32ms/step - loss: 0.1632 - accuracy: 0.9556 - val_loss: 1.0602 - val_accuracy: 0.7807\n","Epoch 84/1000\n","35/36 [============================>.] - ETA: 0s - loss: 0.1604 - accuracy: 0.9568\n","Epoch 84: val_accuracy did not improve from 0.79611\n","36/36 [==============================] - 1s 31ms/step - loss: 0.1606 - accuracy: 0.9567 - val_loss: 1.0729 - val_accuracy: 0.7777\n","Epoch 85/1000\n","35/36 [============================>.] - ETA: 0s - loss: 0.1597 - accuracy: 0.9567\n","Epoch 85: val_accuracy did not improve from 0.79611\n","36/36 [==============================] - 1s 31ms/step - loss: 0.1596 - accuracy: 0.9567 - val_loss: 1.0749 - val_accuracy: 0.7817\n","Epoch 86/1000\n","36/36 [==============================] - ETA: 0s - loss: 0.1561 - accuracy: 0.9579\n","Epoch 86: val_accuracy did not improve from 0.79611\n","36/36 [==============================] - 1s 31ms/step - loss: 0.1561 - accuracy: 0.9579 - val_loss: 1.0706 - val_accuracy: 0.7798\n","Epoch 87/1000\n","36/36 [==============================] - ETA: 0s - loss: 0.1540 - accuracy: 0.9583\n","Epoch 87: val_accuracy did not improve from 0.79611\n","36/36 [==============================] - 1s 31ms/step - loss: 0.1540 - accuracy: 0.9583 - val_loss: 1.0981 - val_accuracy: 0.7791\n","Epoch 88/1000\n","35/36 [============================>.] - ETA: 0s - loss: 0.1518 - accuracy: 0.9590\n","Epoch 88: val_accuracy did not improve from 0.79611\n","36/36 [==============================] - 1s 31ms/step - loss: 0.1519 - accuracy: 0.9589 - val_loss: 1.0919 - val_accuracy: 0.7782\n","Epoch 89/1000\n","35/36 [============================>.] - ETA: 0s - loss: 0.1495 - accuracy: 0.9597\n","Epoch 89: val_accuracy did not improve from 0.79611\n","36/36 [==============================] - 1s 32ms/step - loss: 0.1495 - accuracy: 0.9597 - val_loss: 1.1113 - val_accuracy: 0.7788\n","Epoch 90/1000\n","35/36 [============================>.] - ETA: 0s - loss: 0.1474 - accuracy: 0.9603\n","Epoch 90: val_accuracy did not improve from 0.79611\n","36/36 [==============================] - 1s 31ms/step - loss: 0.1475 - accuracy: 0.9603 - val_loss: 1.1226 - val_accuracy: 0.7772\n","Epoch 91/1000\n","35/36 [============================>.] - ETA: 0s - loss: 0.1454 - accuracy: 0.9610\n","Epoch 91: val_accuracy did not improve from 0.79611\n","36/36 [==============================] - 1s 31ms/step - loss: 0.1454 - accuracy: 0.9610 - val_loss: 1.1303 - val_accuracy: 0.7774\n","Epoch 92/1000\n","35/36 [============================>.] - ETA: 0s - loss: 0.1437 - accuracy: 0.9611\n","Epoch 92: val_accuracy did not improve from 0.79611\n","36/36 [==============================] - 1s 31ms/step - loss: 0.1437 - accuracy: 0.9611 - val_loss: 1.1114 - val_accuracy: 0.7792\n","Epoch 93/1000\n","36/36 [==============================] - ETA: 0s - loss: 0.1420 - accuracy: 0.9616\n","Epoch 93: val_accuracy did not improve from 0.79611\n","36/36 [==============================] - 1s 32ms/step - loss: 0.1420 - accuracy: 0.9616 - val_loss: 1.1342 - val_accuracy: 0.7783\n","Epoch 94/1000\n","35/36 [============================>.] - ETA: 0s - loss: 0.1394 - accuracy: 0.9628\n","Epoch 94: val_accuracy did not improve from 0.79611\n","36/36 [==============================] - 1s 31ms/step - loss: 0.1395 - accuracy: 0.9627 - val_loss: 1.1421 - val_accuracy: 0.7781\n","Epoch 95/1000\n","35/36 [============================>.] - ETA: 0s - loss: 0.1380 - accuracy: 0.9631\n","Epoch 95: val_accuracy did not improve from 0.79611\n","36/36 [==============================] - 1s 31ms/step - loss: 0.1380 - accuracy: 0.9630 - val_loss: 1.1482 - val_accuracy: 0.7782\n","Epoch 96/1000\n","35/36 [============================>.] - ETA: 0s - loss: 0.1362 - accuracy: 0.9635\n","Epoch 96: val_accuracy did not improve from 0.79611\n","36/36 [==============================] - 1s 31ms/step - loss: 0.1362 - accuracy: 0.9635 - val_loss: 1.1604 - val_accuracy: 0.7781\n","Epoch 97/1000\n","35/36 [============================>.] - ETA: 0s - loss: 0.3031 - accuracy: 0.9344\n","Epoch 97: val_accuracy did not improve from 0.79611\n","36/36 [==============================] - 1s 32ms/step - loss: 0.3003 - accuracy: 0.9348 - val_loss: 1.1542 - val_accuracy: 0.7766\n","Epoch 98/1000\n","35/36 [============================>.] - ETA: 0s - loss: 0.1503 - accuracy: 0.9580\n","Epoch 98: val_accuracy did not improve from 0.79611\n","36/36 [==============================] - 1s 31ms/step - loss: 0.1499 - accuracy: 0.9581 - val_loss: 1.1497 - val_accuracy: 0.7785\n","Epoch 99/1000\n","35/36 [============================>.] - ETA: 0s - loss: 0.1305 - accuracy: 0.9653\n","Epoch 99: val_accuracy did not improve from 0.79611\n","36/36 [==============================] - 1s 32ms/step - loss: 0.1306 - accuracy: 0.9653 - val_loss: 1.1615 - val_accuracy: 0.7787\n","Epoch 100/1000\n","35/36 [============================>.] - ETA: 0s - loss: 0.1277 - accuracy: 0.9662\n","Epoch 100: val_accuracy did not improve from 0.79611\n","36/36 [==============================] - 1s 32ms/step - loss: 0.1278 - accuracy: 0.9662 - val_loss: 1.1859 - val_accuracy: 0.7784\n","Epoch 101/1000\n","35/36 [============================>.] - ETA: 0s - loss: 0.1308 - accuracy: 0.9649\n","Epoch 101: val_accuracy did not improve from 0.79611\n","36/36 [==============================] - 1s 31ms/step - loss: 0.1311 - accuracy: 0.9649 - val_loss: 1.1796 - val_accuracy: 0.7787\n","Epoch 102/1000\n","35/36 [============================>.] - ETA: 0s - loss: 0.1296 - accuracy: 0.9653\n","Epoch 102: val_accuracy did not improve from 0.79611\n","36/36 [==============================] - 1s 32ms/step - loss: 0.1298 - accuracy: 0.9652 - val_loss: 1.1903 - val_accuracy: 0.7770\n","Epoch 103/1000\n","35/36 [============================>.] - ETA: 0s - loss: 0.1281 - accuracy: 0.9654\n","Epoch 103: val_accuracy did not improve from 0.79611\n","36/36 [==============================] - 1s 31ms/step - loss: 0.1281 - accuracy: 0.9654 - val_loss: 1.1975 - val_accuracy: 0.7771\n","Epoch 104/1000\n","35/36 [============================>.] - ETA: 0s - loss: 0.1269 - accuracy: 0.9657\n","Epoch 104: val_accuracy did not improve from 0.79611\n","36/36 [==============================] - 1s 31ms/step - loss: 0.1270 - accuracy: 0.9657 - val_loss: 1.2020 - val_accuracy: 0.7764\n","Epoch 105/1000\n","35/36 [============================>.] - ETA: 0s - loss: 0.1263 - accuracy: 0.9660\n","Epoch 105: val_accuracy did not improve from 0.79611\n","36/36 [==============================] - 1s 32ms/step - loss: 0.1265 - accuracy: 0.9660 - val_loss: 1.2081 - val_accuracy: 0.7779\n","Epoch 106/1000\n","35/36 [============================>.] - ETA: 0s - loss: 0.1210 - accuracy: 0.9678\n","Epoch 106: val_accuracy did not improve from 0.79611\n","36/36 [==============================] - 1s 31ms/step - loss: 0.1209 - accuracy: 0.9677 - val_loss: 1.2286 - val_accuracy: 0.7770\n","Epoch 107/1000\n","35/36 [============================>.] - ETA: 0s - loss: 0.1214 - accuracy: 0.9674\n","Epoch 107: val_accuracy did not improve from 0.79611\n","36/36 [==============================] - 1s 32ms/step - loss: 0.1216 - accuracy: 0.9674 - val_loss: 1.2363 - val_accuracy: 0.7780\n","Epoch 108/1000\n","35/36 [============================>.] - ETA: 0s - loss: 0.1215 - accuracy: 0.9675\n","Epoch 108: val_accuracy did not improve from 0.79611\n","36/36 [==============================] - 1s 31ms/step - loss: 0.1214 - accuracy: 0.9675 - val_loss: 1.2349 - val_accuracy: 0.7768\n","Epoch 109/1000\n","35/36 [============================>.] - ETA: 0s - loss: 0.1206 - accuracy: 0.9675\n","Epoch 109: val_accuracy did not improve from 0.79611\n","36/36 [==============================] - 1s 31ms/step - loss: 0.1205 - accuracy: 0.9676 - val_loss: 1.2446 - val_accuracy: 0.7774\n","Epoch 110/1000\n","35/36 [============================>.] - ETA: 0s - loss: 0.1185 - accuracy: 0.9682\n","Epoch 110: val_accuracy did not improve from 0.79611\n","36/36 [==============================] - 1s 32ms/step - loss: 0.1186 - accuracy: 0.9682 - val_loss: 1.2459 - val_accuracy: 0.7770\n","Epoch 111/1000\n","35/36 [============================>.] - ETA: 0s - loss: 0.1173 - accuracy: 0.9685\n","Epoch 111: val_accuracy did not improve from 0.79611\n","36/36 [==============================] - 1s 32ms/step - loss: 0.1175 - accuracy: 0.9685 - val_loss: 1.2483 - val_accuracy: 0.7773\n","Epoch 112/1000\n","35/36 [============================>.] - ETA: 0s - loss: 0.1168 - accuracy: 0.9685\n","Epoch 112: val_accuracy did not improve from 0.79611\n","36/36 [==============================] - 1s 32ms/step - loss: 0.1168 - accuracy: 0.9685 - val_loss: 1.2612 - val_accuracy: 0.7759\n","Epoch 113/1000\n","35/36 [============================>.] - ETA: 0s - loss: 0.1152 - accuracy: 0.9690\n","Epoch 113: val_accuracy did not improve from 0.79611\n","36/36 [==============================] - 1s 31ms/step - loss: 0.1152 - accuracy: 0.9689 - val_loss: 1.2490 - val_accuracy: 0.7772\n","Epoch 114/1000\n","35/36 [============================>.] - ETA: 0s - loss: 0.1147 - accuracy: 0.9692\n","Epoch 114: val_accuracy did not improve from 0.79611\n","36/36 [==============================] - 1s 32ms/step - loss: 0.1148 - accuracy: 0.9692 - val_loss: 1.2687 - val_accuracy: 0.7767\n","Epoch 115/1000\n","35/36 [============================>.] - ETA: 0s - loss: 0.1118 - accuracy: 0.9698\n","Epoch 115: val_accuracy did not improve from 0.79611\n","36/36 [==============================] - 1s 31ms/step - loss: 0.1119 - accuracy: 0.9697 - val_loss: 1.2772 - val_accuracy: 0.7769\n","Epoch 116/1000\n","35/36 [============================>.] - ETA: 0s - loss: 0.1121 - accuracy: 0.9697\n","Epoch 116: val_accuracy did not improve from 0.79611\n","36/36 [==============================] - 1s 32ms/step - loss: 0.1123 - accuracy: 0.9696 - val_loss: 1.2773 - val_accuracy: 0.7766\n","Epoch 117/1000\n","35/36 [============================>.] - ETA: 0s - loss: 0.1108 - accuracy: 0.9700\n","Epoch 117: val_accuracy did not improve from 0.79611\n","36/36 [==============================] - 1s 31ms/step - loss: 0.1109 - accuracy: 0.9700 - val_loss: 1.2709 - val_accuracy: 0.7775\n","Epoch 118/1000\n","35/36 [============================>.] - ETA: 0s - loss: 0.1102 - accuracy: 0.9702\n","Epoch 118: val_accuracy did not improve from 0.79611\n","36/36 [==============================] - 1s 32ms/step - loss: 0.1102 - accuracy: 0.9702 - val_loss: 1.2768 - val_accuracy: 0.7761\n","Epoch 119/1000\n","35/36 [============================>.] - ETA: 0s - loss: 0.1102 - accuracy: 0.9701\n","Epoch 119: val_accuracy did not improve from 0.79611\n","36/36 [==============================] - 1s 32ms/step - loss: 0.1102 - accuracy: 0.9702 - val_loss: 1.3082 - val_accuracy: 0.7747\n","Epoch 120/1000\n","35/36 [============================>.] - ETA: 0s - loss: 0.1086 - accuracy: 0.9705\n","Epoch 120: val_accuracy did not improve from 0.79611\n","36/36 [==============================] - 1s 32ms/step - loss: 0.1085 - accuracy: 0.9706 - val_loss: 1.2942 - val_accuracy: 0.7763\n","Epoch 121/1000\n","35/36 [============================>.] - ETA: 0s - loss: 0.1070 - accuracy: 0.9708\n","Epoch 121: val_accuracy did not improve from 0.79611\n","36/36 [==============================] - 1s 32ms/step - loss: 0.1071 - accuracy: 0.9708 - val_loss: 1.3077 - val_accuracy: 0.7779\n","Epoch 122/1000\n","35/36 [============================>.] - ETA: 0s - loss: 0.1060 - accuracy: 0.9712\n","Epoch 122: val_accuracy did not improve from 0.79611\n","36/36 [==============================] - 1s 32ms/step - loss: 0.1061 - accuracy: 0.9712 - val_loss: 1.3085 - val_accuracy: 0.7771\n","Epoch 123/1000\n","35/36 [============================>.] - ETA: 0s - loss: 0.1049 - accuracy: 0.9714\n","Epoch 123: val_accuracy did not improve from 0.79611\n","36/36 [==============================] - 1s 32ms/step - loss: 0.1050 - accuracy: 0.9713 - val_loss: 1.3125 - val_accuracy: 0.7755\n","Epoch 124/1000\n","35/36 [============================>.] - ETA: 0s - loss: 0.1042 - accuracy: 0.9714\n","Epoch 124: val_accuracy did not improve from 0.79611\n","36/36 [==============================] - 1s 32ms/step - loss: 0.1042 - accuracy: 0.9714 - val_loss: 1.3361 - val_accuracy: 0.7753\n","Epoch 125/1000\n","35/36 [============================>.] - ETA: 0s - loss: 0.1032 - accuracy: 0.9718\n","Epoch 125: val_accuracy did not improve from 0.79611\n","36/36 [==============================] - 1s 31ms/step - loss: 0.1031 - accuracy: 0.9718 - val_loss: 1.3479 - val_accuracy: 0.7749\n","Epoch 126/1000\n","35/36 [============================>.] - ETA: 0s - loss: 0.1030 - accuracy: 0.9719\n","Epoch 126: val_accuracy did not improve from 0.79611\n","36/36 [==============================] - 1s 31ms/step - loss: 0.1030 - accuracy: 0.9719 - val_loss: 1.3486 - val_accuracy: 0.7760\n","Epoch 127/1000\n","35/36 [============================>.] - ETA: 0s - loss: 0.1019 - accuracy: 0.9720\n","Epoch 127: val_accuracy did not improve from 0.79611\n","36/36 [==============================] - 1s 32ms/step - loss: 0.1020 - accuracy: 0.9720 - val_loss: 1.3429 - val_accuracy: 0.7751\n","Epoch 128/1000\n","35/36 [============================>.] - ETA: 0s - loss: 0.1010 - accuracy: 0.9723\n","Epoch 128: val_accuracy did not improve from 0.79611\n","36/36 [==============================] - 1s 32ms/step - loss: 0.1011 - accuracy: 0.9723 - val_loss: 1.3468 - val_accuracy: 0.7770\n","Epoch 129/1000\n","35/36 [============================>.] - ETA: 0s - loss: 0.1000 - accuracy: 0.9725\n","Epoch 129: val_accuracy did not improve from 0.79611\n","36/36 [==============================] - 1s 32ms/step - loss: 0.1001 - accuracy: 0.9724 - val_loss: 1.3557 - val_accuracy: 0.7766\n","Epoch 130/1000\n","35/36 [============================>.] - ETA: 0s - loss: 0.0994 - accuracy: 0.9728\n","Epoch 130: val_accuracy did not improve from 0.79611\n","36/36 [==============================] - 1s 32ms/step - loss: 0.0994 - accuracy: 0.9728 - val_loss: 1.3609 - val_accuracy: 0.7757\n","Epoch 131/1000\n","35/36 [============================>.] - ETA: 0s - loss: 0.0983 - accuracy: 0.9732\n","Epoch 131: val_accuracy did not improve from 0.79611\n","36/36 [==============================] - 1s 38ms/step - loss: 0.0983 - accuracy: 0.9732 - val_loss: 1.3651 - val_accuracy: 0.7760\n","Epoch 132/1000\n","35/36 [============================>.] - ETA: 0s - loss: 0.0985 - accuracy: 0.9729\n","Epoch 132: val_accuracy did not improve from 0.79611\n","36/36 [==============================] - 1s 40ms/step - loss: 0.0988 - accuracy: 0.9729 - val_loss: 1.3599 - val_accuracy: 0.7765\n","Epoch 133/1000\n","35/36 [============================>.] - ETA: 0s - loss: 0.0969 - accuracy: 0.9734\n","Epoch 133: val_accuracy did not improve from 0.79611\n","36/36 [==============================] - 2s 42ms/step - loss: 0.0970 - accuracy: 0.9734 - val_loss: 1.3681 - val_accuracy: 0.7758\n","Epoch 134/1000\n","35/36 [============================>.] - ETA: 0s - loss: 0.0967 - accuracy: 0.9732\n","Epoch 134: val_accuracy did not improve from 0.79611\n","36/36 [==============================] - 1s 37ms/step - loss: 0.0969 - accuracy: 0.9732 - val_loss: 1.3823 - val_accuracy: 0.7760\n","Epoch 135/1000\n","35/36 [============================>.] - ETA: 0s - loss: 0.0959 - accuracy: 0.9735\n","Epoch 135: val_accuracy did not improve from 0.79611\n","36/36 [==============================] - 1s 32ms/step - loss: 0.0959 - accuracy: 0.9735 - val_loss: 1.3918 - val_accuracy: 0.7744\n","Epoch 136/1000\n","35/36 [============================>.] - ETA: 0s - loss: 0.0960 - accuracy: 0.9734\n","Epoch 136: val_accuracy did not improve from 0.79611\n","36/36 [==============================] - 1s 33ms/step - loss: 0.0965 - accuracy: 0.9733 - val_loss: 1.3787 - val_accuracy: 0.7763\n","Epoch 137/1000\n","35/36 [============================>.] - ETA: 0s - loss: 0.1007 - accuracy: 0.9721\n","Epoch 137: val_accuracy did not improve from 0.79611\n","36/36 [==============================] - 1s 33ms/step - loss: 0.1007 - accuracy: 0.9721 - val_loss: 1.3737 - val_accuracy: 0.7760\n","Epoch 138/1000\n","35/36 [============================>.] - ETA: 0s - loss: 0.0943 - accuracy: 0.9739\n","Epoch 138: val_accuracy did not improve from 0.79611\n","36/36 [==============================] - 1s 32ms/step - loss: 0.0945 - accuracy: 0.9738 - val_loss: 1.4002 - val_accuracy: 0.7763\n","Epoch 139/1000\n","35/36 [============================>.] - ETA: 0s - loss: 0.0935 - accuracy: 0.9740\n","Epoch 139: val_accuracy did not improve from 0.79611\n","36/36 [==============================] - 1s 32ms/step - loss: 0.0936 - accuracy: 0.9740 - val_loss: 1.3853 - val_accuracy: 0.7768\n","Epoch 140/1000\n","35/36 [============================>.] - ETA: 0s - loss: 0.0961 - accuracy: 0.9734\n","Epoch 140: val_accuracy did not improve from 0.79611\n","36/36 [==============================] - 1s 32ms/step - loss: 0.0961 - accuracy: 0.9734 - val_loss: 1.4065 - val_accuracy: 0.7758\n","Epoch 141/1000\n","35/36 [============================>.] - ETA: 0s - loss: 0.0949 - accuracy: 0.9735\n","Epoch 141: val_accuracy did not improve from 0.79611\n","36/36 [==============================] - 1s 31ms/step - loss: 0.0948 - accuracy: 0.9736 - val_loss: 1.3934 - val_accuracy: 0.7772\n","Epoch 142/1000\n","35/36 [============================>.] - ETA: 0s - loss: 0.0944 - accuracy: 0.9737\n","Epoch 142: val_accuracy did not improve from 0.79611\n","36/36 [==============================] - 1s 32ms/step - loss: 0.0945 - accuracy: 0.9737 - val_loss: 1.4121 - val_accuracy: 0.7755\n","Epoch 143/1000\n","35/36 [============================>.] - ETA: 0s - loss: 0.0944 - accuracy: 0.9733\n","Epoch 143: val_accuracy did not improve from 0.79611\n","36/36 [==============================] - 1s 32ms/step - loss: 0.0944 - accuracy: 0.9733 - val_loss: 1.4065 - val_accuracy: 0.7751\n","Epoch 144/1000\n","35/36 [============================>.] - ETA: 0s - loss: 0.0917 - accuracy: 0.9745\n","Epoch 144: val_accuracy did not improve from 0.79611\n","36/36 [==============================] - 1s 32ms/step - loss: 0.0917 - accuracy: 0.9744 - val_loss: 1.4162 - val_accuracy: 0.7759\n","Epoch 145/1000\n","35/36 [============================>.] - ETA: 0s - loss: 0.0911 - accuracy: 0.9743\n","Epoch 145: val_accuracy did not improve from 0.79611\n","36/36 [==============================] - 1s 32ms/step - loss: 0.0912 - accuracy: 0.9742 - val_loss: 1.4146 - val_accuracy: 0.7758\n","Epoch 146/1000\n","35/36 [============================>.] - ETA: 0s - loss: 0.0916 - accuracy: 0.9742\n","Epoch 146: val_accuracy did not improve from 0.79611\n","36/36 [==============================] - 1s 32ms/step - loss: 0.0915 - accuracy: 0.9742 - val_loss: 1.4348 - val_accuracy: 0.7759\n","Epoch 147/1000\n","35/36 [============================>.] - ETA: 0s - loss: 0.1012 - accuracy: 0.9708\n","Epoch 147: val_accuracy did not improve from 0.79611\n","36/36 [==============================] - 1s 33ms/step - loss: 0.1011 - accuracy: 0.9708 - val_loss: 1.4157 - val_accuracy: 0.7772\n","Epoch 148/1000\n","35/36 [============================>.] - ETA: 0s - loss: 0.1007 - accuracy: 0.9713\n","Epoch 148: val_accuracy did not improve from 0.79611\n","36/36 [==============================] - 1s 32ms/step - loss: 0.1006 - accuracy: 0.9713 - val_loss: 1.4083 - val_accuracy: 0.7777\n","Epoch 149/1000\n","35/36 [============================>.] - ETA: 0s - loss: 0.0961 - accuracy: 0.9728\n","Epoch 149: val_accuracy did not improve from 0.79611\n","36/36 [==============================] - 1s 32ms/step - loss: 0.0960 - accuracy: 0.9728 - val_loss: 1.4115 - val_accuracy: 0.7762\n","Epoch 150/1000\n","35/36 [============================>.] - ETA: 0s - loss: 0.0943 - accuracy: 0.9733\n","Epoch 150: val_accuracy did not improve from 0.79611\n","36/36 [==============================] - 1s 32ms/step - loss: 0.0943 - accuracy: 0.9734 - val_loss: 1.4163 - val_accuracy: 0.7777\n","Epoch 151/1000\n","35/36 [============================>.] - ETA: 0s - loss: 0.0942 - accuracy: 0.9732\n","Epoch 151: val_accuracy did not improve from 0.79611\n","36/36 [==============================] - 1s 32ms/step - loss: 0.0944 - accuracy: 0.9731 - val_loss: 1.4287 - val_accuracy: 0.7767\n","Epoch 152/1000\n","35/36 [============================>.] - ETA: 0s - loss: 0.0935 - accuracy: 0.9735\n","Epoch 152: val_accuracy did not improve from 0.79611\n","36/36 [==============================] - 1s 33ms/step - loss: 0.0935 - accuracy: 0.9735 - val_loss: 1.4310 - val_accuracy: 0.7766\n","Epoch 153/1000\n","35/36 [============================>.] - ETA: 0s - loss: 0.0915 - accuracy: 0.9742\n","Epoch 153: val_accuracy did not improve from 0.79611\n","36/36 [==============================] - 1s 33ms/step - loss: 0.0917 - accuracy: 0.9742 - val_loss: 1.4287 - val_accuracy: 0.7767\n","Epoch 154/1000\n","35/36 [============================>.] - ETA: 0s - loss: 0.0906 - accuracy: 0.9743\n","Epoch 154: val_accuracy did not improve from 0.79611\n","36/36 [==============================] - 1s 33ms/step - loss: 0.0906 - accuracy: 0.9742 - val_loss: 1.4581 - val_accuracy: 0.7748\n","Epoch 155/1000\n","35/36 [============================>.] - ETA: 0s - loss: 0.0903 - accuracy: 0.9744\n","Epoch 155: val_accuracy did not improve from 0.79611\n","36/36 [==============================] - 1s 33ms/step - loss: 0.0905 - accuracy: 0.9744 - val_loss: 1.4404 - val_accuracy: 0.7760\n","Epoch 156/1000\n","35/36 [============================>.] - ETA: 0s - loss: 0.0901 - accuracy: 0.9743\n","Epoch 156: val_accuracy did not improve from 0.79611\n","36/36 [==============================] - 1s 32ms/step - loss: 0.0901 - accuracy: 0.9743 - val_loss: 1.4353 - val_accuracy: 0.7772\n","Epoch 157/1000\n","35/36 [============================>.] - ETA: 0s - loss: 0.0897 - accuracy: 0.9743\n","Epoch 157: val_accuracy did not improve from 0.79611\n","36/36 [==============================] - 1s 32ms/step - loss: 0.0896 - accuracy: 0.9743 - val_loss: 1.4590 - val_accuracy: 0.7764\n","Epoch 158/1000\n","35/36 [============================>.] - ETA: 0s - loss: 0.0889 - accuracy: 0.9744\n","Epoch 158: val_accuracy did not improve from 0.79611\n","36/36 [==============================] - 1s 32ms/step - loss: 0.0889 - accuracy: 0.9745 - val_loss: 1.4546 - val_accuracy: 0.7759\n","Epoch 159/1000\n","35/36 [============================>.] - ETA: 0s - loss: 0.0880 - accuracy: 0.9748\n","Epoch 159: val_accuracy did not improve from 0.79611\n","36/36 [==============================] - 1s 32ms/step - loss: 0.0882 - accuracy: 0.9748 - val_loss: 1.4573 - val_accuracy: 0.7763\n","Epoch 160/1000\n","35/36 [============================>.] - ETA: 0s - loss: 0.0877 - accuracy: 0.9749\n","Epoch 160: val_accuracy did not improve from 0.79611\n","36/36 [==============================] - 1s 32ms/step - loss: 0.0877 - accuracy: 0.9749 - val_loss: 1.4823 - val_accuracy: 0.7762\n","Epoch 161/1000\n","35/36 [============================>.] - ETA: 0s - loss: 0.0871 - accuracy: 0.9748\n","Epoch 161: val_accuracy did not improve from 0.79611\n","36/36 [==============================] - 1s 32ms/step - loss: 0.0872 - accuracy: 0.9749 - val_loss: 1.4741 - val_accuracy: 0.7757\n","Epoch 162/1000\n","35/36 [============================>.] - ETA: 0s - loss: 0.0862 - accuracy: 0.9752\n","Epoch 162: val_accuracy did not improve from 0.79611\n","36/36 [==============================] - 1s 32ms/step - loss: 0.0863 - accuracy: 0.9751 - val_loss: 1.4865 - val_accuracy: 0.7746\n","Epoch 163/1000\n","35/36 [============================>.] - ETA: 0s - loss: 0.0860 - accuracy: 0.9753\n","Epoch 163: val_accuracy did not improve from 0.79611\n","36/36 [==============================] - 1s 33ms/step - loss: 0.0860 - accuracy: 0.9753 - val_loss: 1.4844 - val_accuracy: 0.7761\n","Epoch 164/1000\n","35/36 [============================>.] - ETA: 0s - loss: 0.0852 - accuracy: 0.9755\n","Epoch 164: val_accuracy did not improve from 0.79611\n","36/36 [==============================] - 1s 33ms/step - loss: 0.0851 - accuracy: 0.9755 - val_loss: 1.4806 - val_accuracy: 0.7755\n","Epoch 165/1000\n","35/36 [============================>.] - ETA: 0s - loss: 0.0853 - accuracy: 0.9753\n","Epoch 165: val_accuracy did not improve from 0.79611\n","36/36 [==============================] - 1s 32ms/step - loss: 0.0853 - accuracy: 0.9754 - val_loss: 1.4829 - val_accuracy: 0.7749\n","Epoch 166/1000\n","35/36 [============================>.] - ETA: 0s - loss: 0.0843 - accuracy: 0.9755\n","Epoch 166: val_accuracy did not improve from 0.79611\n","36/36 [==============================] - 1s 32ms/step - loss: 0.0843 - accuracy: 0.9755 - val_loss: 1.4856 - val_accuracy: 0.7749\n","Epoch 167/1000\n","35/36 [============================>.] - ETA: 0s - loss: 0.0838 - accuracy: 0.9757\n","Epoch 167: val_accuracy did not improve from 0.79611\n","36/36 [==============================] - 1s 32ms/step - loss: 0.0839 - accuracy: 0.9757 - val_loss: 1.4861 - val_accuracy: 0.7769\n","Epoch 168/1000\n","35/36 [============================>.] - ETA: 0s - loss: 0.0834 - accuracy: 0.9757\n","Epoch 168: val_accuracy did not improve from 0.79611\n","36/36 [==============================] - 1s 33ms/step - loss: 0.0836 - accuracy: 0.9756 - val_loss: 1.4885 - val_accuracy: 0.7756\n","Epoch 169/1000\n","35/36 [============================>.] - ETA: 0s - loss: 0.0837 - accuracy: 0.9756\n","Epoch 169: val_accuracy did not improve from 0.79611\n","36/36 [==============================] - 1s 33ms/step - loss: 0.0837 - accuracy: 0.9756 - val_loss: 1.5036 - val_accuracy: 0.7771\n","Epoch 170/1000\n","35/36 [============================>.] - ETA: 0s - loss: 0.0886 - accuracy: 0.9739\n","Epoch 170: val_accuracy did not improve from 0.79611\n","36/36 [==============================] - 1s 33ms/step - loss: 0.0887 - accuracy: 0.9739 - val_loss: 1.4932 - val_accuracy: 0.7770\n","Epoch 171/1000\n","35/36 [============================>.] - ETA: 0s - loss: 0.0867 - accuracy: 0.9746\n","Epoch 171: val_accuracy did not improve from 0.79611\n","36/36 [==============================] - 1s 32ms/step - loss: 0.0867 - accuracy: 0.9746 - val_loss: 1.4956 - val_accuracy: 0.7758\n","Epoch 172/1000\n","35/36 [============================>.] - ETA: 0s - loss: 0.0915 - accuracy: 0.9730\n","Epoch 172: val_accuracy did not improve from 0.79611\n","36/36 [==============================] - 1s 32ms/step - loss: 0.0918 - accuracy: 0.9729 - val_loss: 1.4883 - val_accuracy: 0.7765\n","Epoch 173/1000\n","35/36 [============================>.] - ETA: 0s - loss: 0.0911 - accuracy: 0.9736\n","Epoch 173: val_accuracy did not improve from 0.79611\n","36/36 [==============================] - 1s 33ms/step - loss: 0.0911 - accuracy: 0.9735 - val_loss: 1.4988 - val_accuracy: 0.7757\n","Epoch 174/1000\n","35/36 [============================>.] - ETA: 0s - loss: 0.0882 - accuracy: 0.9743\n","Epoch 174: val_accuracy did not improve from 0.79611\n","36/36 [==============================] - 1s 33ms/step - loss: 0.0882 - accuracy: 0.9743 - val_loss: 1.5050 - val_accuracy: 0.7765\n","Epoch 175/1000\n","35/36 [============================>.] - ETA: 0s - loss: 0.0869 - accuracy: 0.9749\n","Epoch 175: val_accuracy did not improve from 0.79611\n","36/36 [==============================] - 1s 33ms/step - loss: 0.0870 - accuracy: 0.9749 - val_loss: 1.4950 - val_accuracy: 0.7774\n","Epoch 176/1000\n","35/36 [============================>.] - ETA: 0s - loss: 0.0861 - accuracy: 0.9751\n","Epoch 176: val_accuracy did not improve from 0.79611\n","36/36 [==============================] - 1s 32ms/step - loss: 0.0862 - accuracy: 0.9750 - val_loss: 1.5060 - val_accuracy: 0.7765\n","Epoch 177/1000\n","35/36 [============================>.] - ETA: 0s - loss: 0.0853 - accuracy: 0.9753\n","Epoch 177: val_accuracy did not improve from 0.79611\n","36/36 [==============================] - 1s 32ms/step - loss: 0.0854 - accuracy: 0.9752 - val_loss: 1.4982 - val_accuracy: 0.7769\n","Epoch 178/1000\n","35/36 [============================>.] - ETA: 0s - loss: 0.0852 - accuracy: 0.9751\n","Epoch 178: val_accuracy did not improve from 0.79611\n","36/36 [==============================] - 1s 32ms/step - loss: 0.0852 - accuracy: 0.9751 - val_loss: 1.5094 - val_accuracy: 0.7757\n","Epoch 179/1000\n","35/36 [============================>.] - ETA: 0s - loss: 0.0843 - accuracy: 0.9753\n","Epoch 179: val_accuracy did not improve from 0.79611\n","36/36 [==============================] - 1s 33ms/step - loss: 0.0843 - accuracy: 0.9753 - val_loss: 1.5236 - val_accuracy: 0.7756\n","Epoch 180/1000\n","35/36 [============================>.] - ETA: 0s - loss: 0.0833 - accuracy: 0.9755\n","Epoch 180: val_accuracy did not improve from 0.79611\n","36/36 [==============================] - 1s 33ms/step - loss: 0.0834 - accuracy: 0.9755 - val_loss: 1.5208 - val_accuracy: 0.7758\n","Epoch 181/1000\n","35/36 [============================>.] - ETA: 0s - loss: 0.0837 - accuracy: 0.9755\n","Epoch 181: val_accuracy did not improve from 0.79611\n","36/36 [==============================] - 1s 32ms/step - loss: 0.0837 - accuracy: 0.9755 - val_loss: 1.5166 - val_accuracy: 0.7770\n","Epoch 182/1000\n","35/36 [============================>.] - ETA: 0s - loss: 0.0832 - accuracy: 0.9755\n","Epoch 182: val_accuracy did not improve from 0.79611\n","36/36 [==============================] - 1s 33ms/step - loss: 0.0831 - accuracy: 0.9755 - val_loss: 1.5184 - val_accuracy: 0.7782\n","Epoch 183/1000\n","35/36 [============================>.] - ETA: 0s - loss: 0.0828 - accuracy: 0.9756\n","Epoch 183: val_accuracy did not improve from 0.79611\n","36/36 [==============================] - 1s 33ms/step - loss: 0.0828 - accuracy: 0.9756 - val_loss: 1.5320 - val_accuracy: 0.7765\n","Epoch 184/1000\n","35/36 [============================>.] - ETA: 0s - loss: 0.0822 - accuracy: 0.9757\n","Epoch 184: val_accuracy did not improve from 0.79611\n","36/36 [==============================] - 1s 32ms/step - loss: 0.0823 - accuracy: 0.9757 - val_loss: 1.5396 - val_accuracy: 0.7765\n","Epoch 185/1000\n","35/36 [============================>.] - ETA: 0s - loss: 0.0815 - accuracy: 0.9762\n","Epoch 185: val_accuracy did not improve from 0.79611\n","36/36 [==============================] - 1s 33ms/step - loss: 0.0816 - accuracy: 0.9761 - val_loss: 1.5285 - val_accuracy: 0.7773\n","Epoch 186/1000\n","35/36 [============================>.] - ETA: 0s - loss: 0.0816 - accuracy: 0.9758\n","Epoch 186: val_accuracy did not improve from 0.79611\n","36/36 [==============================] - 1s 32ms/step - loss: 0.0815 - accuracy: 0.9759 - val_loss: 1.5251 - val_accuracy: 0.7764\n","Epoch 187/1000\n","35/36 [============================>.] - ETA: 0s - loss: 0.0812 - accuracy: 0.9761\n","Epoch 187: val_accuracy did not improve from 0.79611\n","36/36 [==============================] - 1s 32ms/step - loss: 0.0813 - accuracy: 0.9760 - val_loss: 1.5353 - val_accuracy: 0.7748\n","Epoch 188/1000\n","35/36 [============================>.] - ETA: 0s - loss: 0.0812 - accuracy: 0.9760\n","Epoch 188: val_accuracy did not improve from 0.79611\n","36/36 [==============================] - 1s 32ms/step - loss: 0.0813 - accuracy: 0.9760 - val_loss: 1.5270 - val_accuracy: 0.7769\n","Epoch 189/1000\n","35/36 [============================>.] - ETA: 0s - loss: 0.0805 - accuracy: 0.9762\n","Epoch 189: val_accuracy did not improve from 0.79611\n","36/36 [==============================] - 1s 33ms/step - loss: 0.0806 - accuracy: 0.9762 - val_loss: 1.5299 - val_accuracy: 0.7784\n","Epoch 190/1000\n","35/36 [============================>.] - ETA: 0s - loss: 0.0803 - accuracy: 0.9763\n","Epoch 190: val_accuracy did not improve from 0.79611\n","36/36 [==============================] - 1s 33ms/step - loss: 0.0804 - accuracy: 0.9763 - val_loss: 1.5513 - val_accuracy: 0.7771\n","Epoch 191/1000\n","35/36 [============================>.] - ETA: 0s - loss: 0.0802 - accuracy: 0.9763\n","Epoch 191: val_accuracy did not improve from 0.79611\n","36/36 [==============================] - 1s 33ms/step - loss: 0.0802 - accuracy: 0.9763 - val_loss: 1.5526 - val_accuracy: 0.7771\n","Epoch 192/1000\n","35/36 [============================>.] - ETA: 0s - loss: 0.0806 - accuracy: 0.9761\n","Epoch 192: val_accuracy did not improve from 0.79611\n","36/36 [==============================] - 1s 33ms/step - loss: 0.0806 - accuracy: 0.9761 - val_loss: 1.5453 - val_accuracy: 0.7764\n","Epoch 193/1000\n","35/36 [============================>.] - ETA: 0s - loss: 0.0803 - accuracy: 0.9760\n","Epoch 193: val_accuracy did not improve from 0.79611\n","36/36 [==============================] - 1s 33ms/step - loss: 0.0804 - accuracy: 0.9760 - val_loss: 1.5523 - val_accuracy: 0.7785\n","Epoch 194/1000\n","35/36 [============================>.] - ETA: 0s - loss: 0.0801 - accuracy: 0.9762\n","Epoch 194: val_accuracy did not improve from 0.79611\n","36/36 [==============================] - 1s 33ms/step - loss: 0.0800 - accuracy: 0.9762 - val_loss: 1.5636 - val_accuracy: 0.7766\n","Epoch 195/1000\n","35/36 [============================>.] - ETA: 0s - loss: 0.0786 - accuracy: 0.9764\n","Epoch 195: val_accuracy did not improve from 0.79611\n","36/36 [==============================] - 1s 33ms/step - loss: 0.0786 - accuracy: 0.9763 - val_loss: 1.5710 - val_accuracy: 0.7758\n","Epoch 196/1000\n","35/36 [============================>.] - ETA: 0s - loss: 0.0791 - accuracy: 0.9762\n","Epoch 196: val_accuracy did not improve from 0.79611\n","36/36 [==============================] - 1s 33ms/step - loss: 0.0792 - accuracy: 0.9762 - val_loss: 1.5568 - val_accuracy: 0.7769\n","Epoch 197/1000\n","35/36 [============================>.] - ETA: 0s - loss: 0.0789 - accuracy: 0.9763\n","Epoch 197: val_accuracy did not improve from 0.79611\n","36/36 [==============================] - 1s 33ms/step - loss: 0.0789 - accuracy: 0.9763 - val_loss: 1.5560 - val_accuracy: 0.7772\n","Epoch 198/1000\n","35/36 [============================>.] - ETA: 0s - loss: 0.0785 - accuracy: 0.9764\n","Epoch 198: val_accuracy did not improve from 0.79611\n","36/36 [==============================] - 1s 32ms/step - loss: 0.0786 - accuracy: 0.9764 - val_loss: 1.5649 - val_accuracy: 0.7765\n","Epoch 199/1000\n","35/36 [============================>.] - ETA: 0s - loss: 0.0783 - accuracy: 0.9765\n","Epoch 199: val_accuracy did not improve from 0.79611\n","36/36 [==============================] - 1s 33ms/step - loss: 0.0783 - accuracy: 0.9765 - val_loss: 1.5738 - val_accuracy: 0.7772\n","Epoch 200/1000\n","35/36 [============================>.] - ETA: 0s - loss: 0.0780 - accuracy: 0.9767\n","Epoch 200: val_accuracy did not improve from 0.79611\n","36/36 [==============================] - 1s 33ms/step - loss: 0.0780 - accuracy: 0.9767 - val_loss: 1.5513 - val_accuracy: 0.7784\n","Epoch 201/1000\n","35/36 [============================>.] - ETA: 0s - loss: 0.0777 - accuracy: 0.9766\n","Epoch 201: val_accuracy did not improve from 0.79611\n","36/36 [==============================] - 1s 33ms/step - loss: 0.0777 - accuracy: 0.9766 - val_loss: 1.5831 - val_accuracy: 0.7776\n","Epoch 202/1000\n","35/36 [============================>.] - ETA: 0s - loss: 0.0775 - accuracy: 0.9767\n","Epoch 202: val_accuracy did not improve from 0.79611\n","36/36 [==============================] - 1s 33ms/step - loss: 0.0775 - accuracy: 0.9766 - val_loss: 1.5758 - val_accuracy: 0.7767\n","Epoch 203/1000\n","35/36 [============================>.] - ETA: 0s - loss: 0.0772 - accuracy: 0.9766\n","Epoch 203: val_accuracy did not improve from 0.79611\n","36/36 [==============================] - 1s 33ms/step - loss: 0.0774 - accuracy: 0.9766 - val_loss: 1.5818 - val_accuracy: 0.7754\n","Epoch 204/1000\n","35/36 [============================>.] - ETA: 0s - loss: 0.0770 - accuracy: 0.9766\n","Epoch 204: val_accuracy did not improve from 0.79611\n","36/36 [==============================] - 1s 32ms/step - loss: 0.0770 - accuracy: 0.9766 - val_loss: 1.5813 - val_accuracy: 0.7761\n","Epoch 205/1000\n","35/36 [============================>.] - ETA: 0s - loss: 0.0769 - accuracy: 0.9767\n","Epoch 205: val_accuracy did not improve from 0.79611\n","36/36 [==============================] - 1s 33ms/step - loss: 0.0769 - accuracy: 0.9766 - val_loss: 1.5861 - val_accuracy: 0.7773\n","Epoch 206/1000\n","35/36 [============================>.] - ETA: 0s - loss: 0.0764 - accuracy: 0.9767\n","Epoch 206: val_accuracy did not improve from 0.79611\n","36/36 [==============================] - 1s 33ms/step - loss: 0.0764 - accuracy: 0.9767 - val_loss: 1.5822 - val_accuracy: 0.7783\n","Epoch 207/1000\n","35/36 [============================>.] - ETA: 0s - loss: 0.0764 - accuracy: 0.9770\n","Epoch 207: val_accuracy did not improve from 0.79611\n","36/36 [==============================] - 1s 33ms/step - loss: 0.0763 - accuracy: 0.9770 - val_loss: 1.5875 - val_accuracy: 0.7761\n","Epoch 208/1000\n","35/36 [============================>.] - ETA: 0s - loss: 0.0764 - accuracy: 0.9767\n","Epoch 208: val_accuracy did not improve from 0.79611\n","36/36 [==============================] - 1s 33ms/step - loss: 0.0764 - accuracy: 0.9767 - val_loss: 1.5950 - val_accuracy: 0.7760\n","Epoch 209/1000\n","35/36 [============================>.] - ETA: 0s - loss: 0.0761 - accuracy: 0.9766\n","Epoch 209: val_accuracy did not improve from 0.79611\n","36/36 [==============================] - 1s 34ms/step - loss: 0.0762 - accuracy: 0.9766 - val_loss: 1.5933 - val_accuracy: 0.7777\n","Epoch 210/1000\n","35/36 [============================>.] - ETA: 0s - loss: 0.0778 - accuracy: 0.9763\n","Epoch 210: val_accuracy did not improve from 0.79611\n","36/36 [==============================] - 1s 33ms/step - loss: 0.0779 - accuracy: 0.9763 - val_loss: 1.6050 - val_accuracy: 0.7766\n","Epoch 211/1000\n","35/36 [============================>.] - ETA: 0s - loss: 0.0783 - accuracy: 0.9760\n","Epoch 211: val_accuracy did not improve from 0.79611\n","36/36 [==============================] - 1s 33ms/step - loss: 0.0783 - accuracy: 0.9761 - val_loss: 1.5972 - val_accuracy: 0.7770\n","Epoch 212/1000\n","35/36 [============================>.] - ETA: 0s - loss: 0.0768 - accuracy: 0.9767\n","Epoch 212: val_accuracy did not improve from 0.79611\n","36/36 [==============================] - 1s 33ms/step - loss: 0.0769 - accuracy: 0.9766 - val_loss: 1.5893 - val_accuracy: 0.7780\n","Epoch 213/1000\n","35/36 [============================>.] - ETA: 0s - loss: 0.0770 - accuracy: 0.9765\n","Epoch 213: val_accuracy did not improve from 0.79611\n","36/36 [==============================] - 1s 33ms/step - loss: 0.0770 - accuracy: 0.9765 - val_loss: 1.6089 - val_accuracy: 0.7774\n","Epoch 214/1000\n","35/36 [============================>.] - ETA: 0s - loss: 0.0761 - accuracy: 0.9765\n","Epoch 214: val_accuracy did not improve from 0.79611\n","36/36 [==============================] - 1s 34ms/step - loss: 0.0762 - accuracy: 0.9765 - val_loss: 1.6047 - val_accuracy: 0.7775\n","Epoch 215/1000\n","35/36 [============================>.] - ETA: 0s - loss: 0.0761 - accuracy: 0.9768\n","Epoch 215: val_accuracy did not improve from 0.79611\n","36/36 [==============================] - 1s 33ms/step - loss: 0.0762 - accuracy: 0.9768 - val_loss: 1.6040 - val_accuracy: 0.7772\n","Epoch 216/1000\n","35/36 [============================>.] - ETA: 0s - loss: 0.0760 - accuracy: 0.9767\n","Epoch 216: val_accuracy did not improve from 0.79611\n","36/36 [==============================] - 1s 33ms/step - loss: 0.0760 - accuracy: 0.9767 - val_loss: 1.5990 - val_accuracy: 0.7779\n","Epoch 217/1000\n","35/36 [============================>.] - ETA: 0s - loss: 0.0758 - accuracy: 0.9766\n","Epoch 217: val_accuracy did not improve from 0.79611\n","36/36 [==============================] - 1s 33ms/step - loss: 0.0758 - accuracy: 0.9766 - val_loss: 1.6136 - val_accuracy: 0.7781\n","Epoch 218/1000\n","35/36 [============================>.] - ETA: 0s - loss: 0.0758 - accuracy: 0.9765\n","Epoch 218: val_accuracy did not improve from 0.79611\n","36/36 [==============================] - 1s 33ms/step - loss: 0.0758 - accuracy: 0.9765 - val_loss: 1.6214 - val_accuracy: 0.7779\n","Epoch 219/1000\n","35/36 [============================>.] - ETA: 0s - loss: 0.0759 - accuracy: 0.9767\n","Epoch 219: val_accuracy did not improve from 0.79611\n","36/36 [==============================] - 1s 33ms/step - loss: 0.0759 - accuracy: 0.9767 - val_loss: 1.6041 - val_accuracy: 0.7775\n","Epoch 220/1000\n","35/36 [============================>.] - ETA: 0s - loss: 0.0751 - accuracy: 0.9767\n","Epoch 220: val_accuracy did not improve from 0.79611\n","36/36 [==============================] - 1s 34ms/step - loss: 0.0752 - accuracy: 0.9767 - val_loss: 1.6335 - val_accuracy: 0.7774\n","Epoch 221/1000\n","35/36 [============================>.] - ETA: 0s - loss: 0.0750 - accuracy: 0.9769\n","Epoch 221: val_accuracy did not improve from 0.79611\n","36/36 [==============================] - 1s 33ms/step - loss: 0.0750 - accuracy: 0.9769 - val_loss: 1.6224 - val_accuracy: 0.7778\n","Epoch 222/1000\n","35/36 [============================>.] - ETA: 0s - loss: 0.0751 - accuracy: 0.9768\n","Epoch 222: val_accuracy did not improve from 0.79611\n","36/36 [==============================] - 1s 33ms/step - loss: 0.0752 - accuracy: 0.9768 - val_loss: 1.6304 - val_accuracy: 0.7776\n","Epoch 223/1000\n","35/36 [============================>.] - ETA: 0s - loss: 0.0751 - accuracy: 0.9767\n","Epoch 223: val_accuracy did not improve from 0.79611\n","36/36 [==============================] - 1s 33ms/step - loss: 0.0752 - accuracy: 0.9767 - val_loss: 1.6314 - val_accuracy: 0.7771\n","Epoch 224/1000\n","35/36 [============================>.] - ETA: 0s - loss: 0.0753 - accuracy: 0.9767\n","Epoch 224: val_accuracy did not improve from 0.79611\n","36/36 [==============================] - 1s 33ms/step - loss: 0.0753 - accuracy: 0.9767 - val_loss: 1.6266 - val_accuracy: 0.7780\n","Epoch 225/1000\n","35/36 [============================>.] - ETA: 0s - loss: 0.0909 - accuracy: 0.9719\n","Epoch 225: val_accuracy did not improve from 0.79611\n","36/36 [==============================] - 1s 33ms/step - loss: 0.0908 - accuracy: 0.9719 - val_loss: 1.6295 - val_accuracy: 0.7761\n","Epoch 226/1000\n","35/36 [============================>.] - ETA: 0s - loss: 0.0763 - accuracy: 0.9767\n","Epoch 226: val_accuracy did not improve from 0.79611\n","36/36 [==============================] - 1s 33ms/step - loss: 0.0763 - accuracy: 0.9767 - val_loss: 1.6295 - val_accuracy: 0.7770\n","Epoch 227/1000\n","35/36 [============================>.] - ETA: 0s - loss: 0.0713 - accuracy: 0.9780\n","Epoch 227: val_accuracy did not improve from 0.79611\n","36/36 [==============================] - 1s 34ms/step - loss: 0.0714 - accuracy: 0.9779 - val_loss: 1.6306 - val_accuracy: 0.7781\n","Epoch 228/1000\n","35/36 [============================>.] - ETA: 0s - loss: 0.0723 - accuracy: 0.9774\n","Epoch 228: val_accuracy did not improve from 0.79611\n","36/36 [==============================] - 1s 33ms/step - loss: 0.0724 - accuracy: 0.9774 - val_loss: 1.6312 - val_accuracy: 0.7770\n","Epoch 229/1000\n","35/36 [============================>.] - ETA: 0s - loss: 0.0741 - accuracy: 0.9770\n","Epoch 229: val_accuracy did not improve from 0.79611\n","36/36 [==============================] - 1s 33ms/step - loss: 0.0742 - accuracy: 0.9769 - val_loss: 1.6366 - val_accuracy: 0.7786\n","Epoch 230/1000\n","35/36 [============================>.] - ETA: 0s - loss: 0.0744 - accuracy: 0.9771\n","Epoch 230: val_accuracy did not improve from 0.79611\n","36/36 [==============================] - 1s 33ms/step - loss: 0.0744 - accuracy: 0.9771 - val_loss: 1.6411 - val_accuracy: 0.7775\n","Epoch 231/1000\n","35/36 [============================>.] - ETA: 0s - loss: 0.0743 - accuracy: 0.9768\n","Epoch 231: val_accuracy did not improve from 0.79611\n","36/36 [==============================] - 1s 33ms/step - loss: 0.0743 - accuracy: 0.9769 - val_loss: 1.6453 - val_accuracy: 0.7779\n","Epoch 232/1000\n","35/36 [============================>.] - ETA: 0s - loss: 0.0739 - accuracy: 0.9769\n","Epoch 232: val_accuracy did not improve from 0.79611\n","36/36 [==============================] - 1s 33ms/step - loss: 0.0739 - accuracy: 0.9769 - val_loss: 1.6491 - val_accuracy: 0.7782\n","Epoch 233/1000\n","35/36 [============================>.] - ETA: 0s - loss: 0.0738 - accuracy: 0.9771\n","Epoch 233: val_accuracy did not improve from 0.79611\n","36/36 [==============================] - 1s 33ms/step - loss: 0.0739 - accuracy: 0.9770 - val_loss: 1.6497 - val_accuracy: 0.7779\n","Epoch 234/1000\n","35/36 [============================>.] - ETA: 0s - loss: 0.0739 - accuracy: 0.9769\n","Epoch 234: val_accuracy did not improve from 0.79611\n","36/36 [==============================] - 1s 33ms/step - loss: 0.0739 - accuracy: 0.9769 - val_loss: 1.6581 - val_accuracy: 0.7783\n","Epoch 235/1000\n","35/36 [============================>.] - ETA: 0s - loss: 0.0738 - accuracy: 0.9769\n","Epoch 235: val_accuracy did not improve from 0.79611\n","36/36 [==============================] - 1s 33ms/step - loss: 0.0739 - accuracy: 0.9769 - val_loss: 1.6482 - val_accuracy: 0.7769\n","Epoch 236/1000\n","35/36 [============================>.] - ETA: 0s - loss: 0.0738 - accuracy: 0.9769\n","Epoch 236: val_accuracy did not improve from 0.79611\n","36/36 [==============================] - 1s 33ms/step - loss: 0.0739 - accuracy: 0.9769 - val_loss: 1.6442 - val_accuracy: 0.7775\n","Epoch 237/1000\n","35/36 [============================>.] - ETA: 0s - loss: 0.0733 - accuracy: 0.9769\n","Epoch 237: val_accuracy did not improve from 0.79611\n","36/36 [==============================] - 1s 33ms/step - loss: 0.0733 - accuracy: 0.9769 - val_loss: 1.6427 - val_accuracy: 0.7780\n","Epoch 238/1000\n","35/36 [============================>.] - ETA: 0s - loss: 0.0731 - accuracy: 0.9770\n","Epoch 238: val_accuracy did not improve from 0.79611\n","36/36 [==============================] - 1s 34ms/step - loss: 0.0731 - accuracy: 0.9770 - val_loss: 1.6531 - val_accuracy: 0.7789\n","Epoch 239/1000\n","35/36 [============================>.] - ETA: 0s - loss: 0.0731 - accuracy: 0.9771\n","Epoch 239: val_accuracy did not improve from 0.79611\n","36/36 [==============================] - 1s 34ms/step - loss: 0.0731 - accuracy: 0.9771 - val_loss: 1.6597 - val_accuracy: 0.7775\n","Epoch 239: early stopping\n"]}],"source":["## Train the model\n","model.compile(optimizer=\"rmsprop\", loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])\n","history =model.fit(\n","    [encoder_input_data, decoder_input_data],\n","    decoder_target_data,\n","    batch_size=batch_size,\n","    epochs=1000,\n","    validation_split=0.2,\n","    callbacks=[es,mc])\n","# Save model"]},{"cell_type":"code","source":["import keras"],"metadata":{"id":"wVxQRewhk_xP","executionInfo":{"status":"ok","timestamp":1666552369570,"user_tz":-330,"elapsed":739,"user":{"displayName":"Gouthaman S","userId":"11145458456231366145"}}},"execution_count":32,"outputs":[]},{"cell_type":"code","source":["model =keras.models.load_model(\"hinditoenglishmodel.h5\")"],"metadata":{"id":"qhCIrMxEdTBs","executionInfo":{"status":"ok","timestamp":1666552371582,"user_tz":-330,"elapsed":1268,"user":{"displayName":"Gouthaman S","userId":"11145458456231366145"}}},"execution_count":33,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"GpliEylOZpqA"},"source":["## Inference Setup"]},{"cell_type":"code","execution_count":34,"metadata":{"id":"TcFcNRmBZpqM","executionInfo":{"status":"ok","timestamp":1666552398004,"user_tz":-330,"elapsed":7,"user":{"displayName":"Gouthaman S","userId":"11145458456231366145"}}},"outputs":[],"source":["encoder_inputs = model.input[0]  # input_1\n","encoder_outputs, state_h_enc, state_c_enc = model.layers[2].output  # lstm_1\n","encoder_states = [state_h_enc, state_c_enc]\n","encoder_model = keras.Model(encoder_inputs, encoder_states)\n","\n","decoder_inputs = model.input[1]  # input_2\n","decoder_state_input_h = keras.Input(shape=(latent_dim,), name=\"inpu t_3\")\n","decoder_state_input_c = keras.Input(shape=(latent_dim,), name=\"input_4\")\n","decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n","decoder_lstm = model.layers[3]\n","decoder_outputs, state_h_dec, state_c_dec = decoder_lstm(\n","    decoder_inputs, initial_state=decoder_states_inputs\n",")\n","decoder_states = [state_h_dec, state_c_dec]\n","decoder_dense = model.layers[4]\n","decoder_outputs = decoder_dense(decoder_outputs)\n","decoder_model = keras.Model(\n","    [decoder_inputs] + decoder_states_inputs, [decoder_outputs] + decoder_states\n",")"]},{"cell_type":"code","execution_count":35,"metadata":{"id":"abrE08ObZpqO","executionInfo":{"status":"ok","timestamp":1666552412031,"user_tz":-330,"elapsed":7,"user":{"displayName":"Gouthaman S","userId":"11145458456231366145"}}},"outputs":[],"source":["# Reverse-lookup token index to decode sequences back to\n","# something readable.\n","reverse_input_char_index = dict((i, char) for char, i in input_token_index.items())\n","reverse_target_char_index = dict((i, char) for char, i in target_token_index.items())"]},{"cell_type":"code","execution_count":36,"metadata":{"id":"UXj97UE7ZpqQ","executionInfo":{"status":"ok","timestamp":1666552432665,"user_tz":-330,"elapsed":554,"user":{"displayName":"Gouthaman S","userId":"11145458456231366145"}}},"outputs":[],"source":["def decode_sequence(input_seq):\n","    # Encode the input as state vectors.\n","    states_value = encoder_model.predict(input_seq)\n","\n","    # Generate empty target sequence of length 1.\n","    target_seq = np.zeros((1, 1, num_decoder_tokens))\n","    # Populate the first character of target sequence with the start character.\n","    target_seq[0, 0, target_token_index[\"\\t\"]] = 1.0\n","\n","    # Sampling loop for a batch of sequences\n","    # (to simplify, here we assume a batch of size 1).\n","    stop_condition = False\n","    decoded_sentence = \"\"\n","    while not stop_condition:\n","        output_tokens, h, c = decoder_model.predict([target_seq] + states_value)\n","\n","        # Sample a token\n","        sampled_token_index = np.argmax(output_tokens[0, -1, :])\n","        sampled_char = reverse_target_char_index[sampled_token_index]\n","        decoded_sentence += sampled_char\n","\n","        # Exit condition: either hit max length\n","        # or find stop character.\n","        if sampled_char == \"\\n\" or len(decoded_sentence) > max_decoder_seq_length:\n","            stop_condition = True\n","\n","        # Update the target sequence (of length 1).\n","        target_seq = np.zeros((1, 1, num_decoder_tokens))\n","        target_seq[0, 0, sampled_token_index] = 1.0\n","\n","        # Update states\n","        states_value = [h, c]\n","    return decoded_sentence"]},{"cell_type":"code","source":["seq_index=23\n","inp_seq = encoder_input_data[seq_index:seq_index+1]\n","translated_sent = decode_sequence(inp_seq)\n","print('-')\n","print('Input sentence:', input_texts[seq_index])\n","print('Decoded sentence:', translated_sent)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"HWodK4mytyzI","executionInfo":{"status":"ok","timestamp":1666552443588,"user_tz":-330,"elapsed":3381,"user":{"displayName":"Gouthaman S","userId":"11145458456231366145"}},"outputId":"9ad55985-d3f9-44c7-d63c-9b043d4b9feb"},"execution_count":37,"outputs":[{"output_type":"stream","name":"stdout","text":["1/1 [==============================] - 1s 805ms/step\n","1/1 [==============================] - 1s 983ms/step\n","1/1 [==============================] - 0s 17ms/step\n","1/1 [==============================] - 0s 16ms/step\n","1/1 [==============================] - 0s 16ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 16ms/step\n","1/1 [==============================] - 0s 16ms/step\n","1/1 [==============================] - 0s 16ms/step\n","1/1 [==============================] - 0s 17ms/step\n","1/1 [==============================] - 0s 15ms/step\n","1/1 [==============================] - 0s 20ms/step\n","1/1 [==============================] - 0s 18ms/step\n","1/1 [==============================] - 0s 19ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 16ms/step\n","1/1 [==============================] - 0s 15ms/step\n","1/1 [==============================] - 0s 17ms/step\n","1/1 [==============================] - 0s 15ms/step\n","1/1 [==============================] - 0s 18ms/step\n","1/1 [==============================] - 0s 15ms/step\n","1/1 [==============================] - 0s 18ms/step\n","1/1 [==============================] - 0s 19ms/step\n","1/1 [==============================] - 0s 19ms/step\n","1/1 [==============================] - 0s 16ms/step\n","1/1 [==============================] - 0s 19ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 16ms/step\n","1/1 [==============================] - 0s 18ms/step\n","1/1 [==============================] - 0s 15ms/step\n","1/1 [==============================] - 0s 18ms/step\n","-\n","Input sentence: I forgot.\n","Decoded sentence: मुझे उसके पास में का क्या है?\n","\n"]}]},{"cell_type":"code","execution_count":72,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"lUoJVsvKZpqU","executionInfo":{"status":"ok","timestamp":1666543150665,"user_tz":-330,"elapsed":7303,"user":{"displayName":"Gouthaman S","userId":"11145458456231366145"}},"outputId":"1df0688e-ea8b-498d-c140-1f199ce519ba"},"outputs":[{"output_type":"stream","name":"stdout","text":["1/1 [==============================] - 0s 21ms/step\n","1/1 [==============================] - 0s 19ms/step\n","1/1 [==============================] - 0s 18ms/step\n","1/1 [==============================] - 0s 18ms/step\n","1/1 [==============================] - 0s 18ms/step\n","1/1 [==============================] - 0s 20ms/step\n","1/1 [==============================] - 0s 19ms/step\n","1/1 [==============================] - 0s 16ms/step\n","1/1 [==============================] - 0s 20ms/step\n","1/1 [==============================] - 0s 15ms/step\n","1/1 [==============================] - 0s 18ms/step\n","1/1 [==============================] - 0s 20ms/step\n","1/1 [==============================] - 0s 18ms/step\n","1/1 [==============================] - 0s 16ms/step\n","1/1 [==============================] - 0s 15ms/step\n","1/1 [==============================] - 0s 19ms/step\n","1/1 [==============================] - 0s 15ms/step\n","1/1 [==============================] - 0s 16ms/step\n","1/1 [==============================] - 0s 18ms/step\n","1/1 [==============================] - 0s 18ms/step\n","1/1 [==============================] - 0s 16ms/step\n","1/1 [==============================] - 0s 15ms/step\n","1/1 [==============================] - 0s 18ms/step\n","1/1 [==============================] - 0s 17ms/step\n","1/1 [==============================] - 0s 18ms/step\n","1/1 [==============================] - 0s 18ms/step\n","1/1 [==============================] - 0s 18ms/step\n","1/1 [==============================] - 0s 21ms/step\n","1/1 [==============================] - 0s 18ms/step\n","1/1 [==============================] - 0s 17ms/step\n","1/1 [==============================] - 0s 15ms/step\n","1/1 [==============================] - 0s 17ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 17ms/step\n","1/1 [==============================] - 0s 17ms/step\n","1/1 [==============================] - 0s 17ms/step\n","1/1 [==============================] - 0s 17ms/step\n","1/1 [==============================] - 0s 18ms/step\n","1/1 [==============================] - 0s 19ms/step\n","1/1 [==============================] - 0s 18ms/step\n","1/1 [==============================] - 0s 16ms/step\n","1/1 [==============================] - 0s 19ms/step\n","1/1 [==============================] - 0s 18ms/step\n","1/1 [==============================] - 0s 18ms/step\n","1/1 [==============================] - 0s 17ms/step\n","1/1 [==============================] - 0s 15ms/step\n","1/1 [==============================] - 0s 15ms/step\n","1/1 [==============================] - 0s 18ms/step\n","1/1 [==============================] - 0s 16ms/step\n","1/1 [==============================] - 0s 15ms/step\n","1/1 [==============================] - 0s 15ms/step\n","1/1 [==============================] - 0s 19ms/step\n","1/1 [==============================] - 0s 20ms/step\n","1/1 [==============================] - 0s 16ms/step\n","1/1 [==============================] - 0s 18ms/step\n","1/1 [==============================] - 0s 17ms/step\n","1/1 [==============================] - 0s 15ms/step\n","1/1 [==============================] - 0s 15ms/step\n","1/1 [==============================] - 0s 15ms/step\n","1/1 [==============================] - 0s 17ms/step\n","1/1 [==============================] - 0s 15ms/step\n","1/1 [==============================] - 0s 16ms/step\n","1/1 [==============================] - 0s 15ms/step\n","1/1 [==============================] - 0s 15ms/step\n","1/1 [==============================] - 0s 18ms/step\n","1/1 [==============================] - 0s 16ms/step\n","1/1 [==============================] - 0s 17ms/step\n","1/1 [==============================] - 0s 18ms/step\n","1/1 [==============================] - 0s 20ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 21ms/step\n","1/1 [==============================] - 0s 20ms/step\n","1/1 [==============================] - 0s 16ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 17ms/step\n","1/1 [==============================] - 0s 19ms/step\n","1/1 [==============================] - 0s 16ms/step\n","1/1 [==============================] - 0s 15ms/step\n","1/1 [==============================] - 0s 15ms/step\n","1/1 [==============================] - 0s 17ms/step\n","1/1 [==============================] - 0s 17ms/step\n","1/1 [==============================] - 0s 19ms/step\n","1/1 [==============================] - 0s 20ms/step\n","1/1 [==============================] - 0s 19ms/step\n","1/1 [==============================] - 0s 19ms/step\n","1/1 [==============================] - 0s 19ms/step\n","1/1 [==============================] - 0s 17ms/step\n","1/1 [==============================] - 0s 16ms/step\n","1/1 [==============================] - 0s 18ms/step\n","1/1 [==============================] - 0s 17ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 19ms/step\n","1/1 [==============================] - 0s 18ms/step\n","1/1 [==============================] - 0s 16ms/step\n","1/1 [==============================] - 0s 16ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 17ms/step\n","1/1 [==============================] - 0s 16ms/step\n","1/1 [==============================] - 0s 15ms/step\n","1/1 [==============================] - 0s 18ms/step\n","1/1 [==============================] - 0s 21ms/step\n","1/1 [==============================] - 0s 22ms/step\n","1/1 [==============================] - 0s 19ms/step\n","1/1 [==============================] - 0s 18ms/step\n","1/1 [==============================] - 0s 21ms/step\n","1/1 [==============================] - 0s 18ms/step\n","1/1 [==============================] - 0s 16ms/step\n","1/1 [==============================] - 0s 16ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 18ms/step\n","1/1 [==============================] - 0s 16ms/step\n","1/1 [==============================] - 0s 18ms/step\n","1/1 [==============================] - 0s 19ms/step\n","1/1 [==============================] - 0s 17ms/step\n","1/1 [==============================] - 0s 16ms/step\n","1/1 [==============================] - 0s 16ms/step\n","1/1 [==============================] - 0s 15ms/step\n","1/1 [==============================] - 0s 20ms/step\n","1/1 [==============================] - 0s 17ms/step\n","1/1 [==============================] - 0s 17ms/step\n","1/1 [==============================] - 0s 21ms/step\n","1/1 [==============================] - 0s 18ms/step\n","1/1 [==============================] - 0s 19ms/step\n","1/1 [==============================] - 0s 20ms/step\n","1/1 [==============================] - 0s 18ms/step\n","-\n","Input sentence: I forgot.\n","Decoded sentence: ‍‍‍‍‍‍‍‍‍‍‍‍‍                                                                                                               \n"]}],"source":["seq_index=23\n","inp_seq = encoder_input_data[seq_index:seq_index+1]\n","translated_sent = decode_seq1(inp_seq)\n","print('-')\n","print('Input sentence:', input_texts[seq_index])\n","print('Decoded sentence:', translated_sent)"]},{"cell_type":"code","execution_count":39,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"OUhkMxrHZpqW","executionInfo":{"status":"ok","timestamp":1666552476237,"user_tz":-330,"elapsed":9125,"user":{"displayName":"Gouthaman S","userId":"11145458456231366145"}},"outputId":"731fe9be-ea87-452c-aa53-7bb6f54fee40"},"outputs":[{"output_type":"stream","name":"stdout","text":["1/1 [==============================] - 0s 21ms/step\n","1/1 [==============================] - 0s 20ms/step\n","1/1 [==============================] - 0s 18ms/step\n","1/1 [==============================] - 0s 19ms/step\n","1/1 [==============================] - 0s 17ms/step\n","1/1 [==============================] - 0s 18ms/step\n","1/1 [==============================] - 0s 15ms/step\n","1/1 [==============================] - 0s 18ms/step\n","1/1 [==============================] - 0s 18ms/step\n","1/1 [==============================] - 0s 18ms/step\n","1/1 [==============================] - 0s 16ms/step\n","1/1 [==============================] - 0s 15ms/step\n","1/1 [==============================] - 0s 15ms/step\n","1/1 [==============================] - 0s 17ms/step\n","1/1 [==============================] - 0s 17ms/step\n","1/1 [==============================] - 0s 17ms/step\n","1/1 [==============================] - 0s 18ms/step\n","1/1 [==============================] - 0s 15ms/step\n","1/1 [==============================] - 0s 19ms/step\n","1/1 [==============================] - 0s 18ms/step\n","1/1 [==============================] - 0s 19ms/step\n","1/1 [==============================] - 0s 18ms/step\n","1/1 [==============================] - 0s 16ms/step\n","1/1 [==============================] - 0s 16ms/step\n","1/1 [==============================] - 0s 19ms/step\n","1/1 [==============================] - 0s 16ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 20ms/step\n","1/1 [==============================] - 0s 32ms/step\n","1/1 [==============================] - 0s 21ms/step\n","1/1 [==============================] - 0s 19ms/step\n","-\n","Input sentence: Wow!\n","Decoded sentence: मुझे उसके पास में का क्या है?\n","\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 17ms/step\n","1/1 [==============================] - 0s 18ms/step\n","1/1 [==============================] - 0s 18ms/step\n","1/1 [==============================] - 0s 18ms/step\n","1/1 [==============================] - 0s 21ms/step\n","1/1 [==============================] - 0s 35ms/step\n","1/1 [==============================] - 0s 33ms/step\n","1/1 [==============================] - 0s 51ms/step\n","1/1 [==============================] - 0s 48ms/step\n","1/1 [==============================] - 0s 40ms/step\n","1/1 [==============================] - 0s 16ms/step\n","1/1 [==============================] - 0s 21ms/step\n","1/1 [==============================] - 0s 16ms/step\n","1/1 [==============================] - 0s 19ms/step\n","1/1 [==============================] - 0s 19ms/step\n","1/1 [==============================] - 0s 17ms/step\n","1/1 [==============================] - 0s 16ms/step\n","1/1 [==============================] - 0s 19ms/step\n","1/1 [==============================] - 0s 18ms/step\n","1/1 [==============================] - 0s 17ms/step\n","1/1 [==============================] - 0s 19ms/step\n","1/1 [==============================] - 0s 21ms/step\n","1/1 [==============================] - 0s 17ms/step\n","1/1 [==============================] - 0s 18ms/step\n","1/1 [==============================] - 0s 20ms/step\n","1/1 [==============================] - 0s 17ms/step\n","1/1 [==============================] - 0s 19ms/step\n","1/1 [==============================] - 0s 19ms/step\n","1/1 [==============================] - 0s 18ms/step\n","1/1 [==============================] - 0s 16ms/step\n","-\n","Input sentence: Help!\n","Decoded sentence: मुझे उसके पास में का क्या है?\n","\n","1/1 [==============================] - 0s 20ms/step\n","1/1 [==============================] - 0s 17ms/step\n","1/1 [==============================] - 0s 16ms/step\n","1/1 [==============================] - 0s 17ms/step\n","1/1 [==============================] - 0s 18ms/step\n","1/1 [==============================] - 0s 17ms/step\n","1/1 [==============================] - 0s 19ms/step\n","1/1 [==============================] - 0s 18ms/step\n","1/1 [==============================] - 0s 17ms/step\n","1/1 [==============================] - 0s 18ms/step\n","1/1 [==============================] - 0s 18ms/step\n","1/1 [==============================] - 0s 20ms/step\n","1/1 [==============================] - 0s 19ms/step\n","1/1 [==============================] - 0s 19ms/step\n","1/1 [==============================] - 0s 18ms/step\n","1/1 [==============================] - 0s 16ms/step\n","1/1 [==============================] - 0s 18ms/step\n","1/1 [==============================] - 0s 17ms/step\n","1/1 [==============================] - 0s 19ms/step\n","1/1 [==============================] - 0s 19ms/step\n","1/1 [==============================] - 0s 19ms/step\n","1/1 [==============================] - 0s 21ms/step\n","1/1 [==============================] - 0s 19ms/step\n","1/1 [==============================] - 0s 20ms/step\n","1/1 [==============================] - 0s 21ms/step\n","1/1 [==============================] - 0s 18ms/step\n","1/1 [==============================] - 0s 21ms/step\n","1/1 [==============================] - 0s 18ms/step\n","1/1 [==============================] - 0s 17ms/step\n","1/1 [==============================] - 0s 18ms/step\n","1/1 [==============================] - 0s 16ms/step\n","-\n","Input sentence: Jump.\n","Decoded sentence: मुझे उसके पास में का क्या है?\n","\n","1/1 [==============================] - 0s 21ms/step\n","1/1 [==============================] - 0s 19ms/step\n","1/1 [==============================] - 0s 18ms/step\n","1/1 [==============================] - 0s 16ms/step\n","1/1 [==============================] - 0s 19ms/step\n","1/1 [==============================] - 0s 17ms/step\n","1/1 [==============================] - 0s 20ms/step\n","1/1 [==============================] - 0s 19ms/step\n","1/1 [==============================] - 0s 16ms/step\n","1/1 [==============================] - 0s 19ms/step\n","1/1 [==============================] - 0s 21ms/step\n","1/1 [==============================] - 0s 17ms/step\n","1/1 [==============================] - 0s 16ms/step\n","1/1 [==============================] - 0s 20ms/step\n","1/1 [==============================] - 0s 16ms/step\n","1/1 [==============================] - 0s 15ms/step\n","1/1 [==============================] - 0s 19ms/step\n","1/1 [==============================] - 0s 19ms/step\n","1/1 [==============================] - 0s 16ms/step\n","1/1 [==============================] - 0s 15ms/step\n","1/1 [==============================] - 0s 16ms/step\n","1/1 [==============================] - 0s 17ms/step\n","1/1 [==============================] - 0s 18ms/step\n","1/1 [==============================] - 0s 17ms/step\n","1/1 [==============================] - 0s 19ms/step\n","1/1 [==============================] - 0s 20ms/step\n","1/1 [==============================] - 0s 20ms/step\n","1/1 [==============================] - 0s 19ms/step\n","1/1 [==============================] - 0s 15ms/step\n","1/1 [==============================] - 0s 20ms/step\n","1/1 [==============================] - 0s 16ms/step\n","-\n","Input sentence: Jump.\n","Decoded sentence: मुझे उसके पास में का क्या है?\n","\n","1/1 [==============================] - 0s 18ms/step\n","1/1 [==============================] - 0s 20ms/step\n","1/1 [==============================] - 0s 19ms/step\n","1/1 [==============================] - 0s 16ms/step\n","1/1 [==============================] - 0s 19ms/step\n","1/1 [==============================] - 0s 17ms/step\n","1/1 [==============================] - 0s 16ms/step\n","1/1 [==============================] - 0s 16ms/step\n","1/1 [==============================] - 0s 15ms/step\n","1/1 [==============================] - 0s 16ms/step\n","1/1 [==============================] - 0s 18ms/step\n","1/1 [==============================] - 0s 18ms/step\n","1/1 [==============================] - 0s 19ms/step\n","1/1 [==============================] - 0s 16ms/step\n","1/1 [==============================] - 0s 17ms/step\n","1/1 [==============================] - 0s 18ms/step\n","1/1 [==============================] - 0s 22ms/step\n","1/1 [==============================] - 0s 18ms/step\n","1/1 [==============================] - 0s 19ms/step\n","1/1 [==============================] - 0s 16ms/step\n","1/1 [==============================] - 0s 19ms/step\n","1/1 [==============================] - 0s 17ms/step\n","1/1 [==============================] - 0s 15ms/step\n","1/1 [==============================] - 0s 16ms/step\n","1/1 [==============================] - 0s 16ms/step\n","1/1 [==============================] - 0s 18ms/step\n","1/1 [==============================] - 0s 16ms/step\n","1/1 [==============================] - 0s 16ms/step\n","1/1 [==============================] - 0s 16ms/step\n","1/1 [==============================] - 0s 15ms/step\n","1/1 [==============================] - 0s 16ms/step\n","-\n","Input sentence: Jump.\n","Decoded sentence: मुझे उसके पास में का क्या है?\n","\n"]}],"source":["for seq_index in range(5):\n","    inp_seq = encoder_input_data[seq_index:seq_index+1]\n","    translated_sent = decode_sequence(inp_seq)\n","    print('-')\n","    print('Input sentence:', input_texts[seq_index])\n","    print('Decoded sentence:', translated_sent)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5y9JGDJiZpqZ","executionInfo":{"status":"aborted","timestamp":1666541139226,"user_tz":-330,"elapsed":16,"user":{"displayName":"Gouthaman S","userId":"11145458456231366145"}}},"outputs":[],"source":["\"\"\"#def decode_sequence(input_seq):\n","    # Encode the input as state vectors.\n","    states_value = encoder_model.predict(input_seq)\n","\n","    # Generate empty target sequence of length 1.\n","    target_seq = np.zeros((1, 1, num_decoder_tokens))\n","    # Populate the first character of target sequence with the start character.\n","    target_seq[0, 0, target_token_index['\\t']] = 1.\n","\n","    # Sampling loop for a batch of sequences\n","    # (to simplify, here we assume a batch of size 1).\n","    stop_condition = False\n","    decoded_sentence = ''\n","    while not stop_condition:\n","        output_tokens, h, c = decoder_model.predict(\n","            [target_seq] + states_value)\n","\n","        # Sample a token\n","        sampled_token_index = np.argmax(output_tokens[0, -1, :])\n","        sampled_char = reverse_target_char_index[sampled_token_index]\n","        decoded_sentence += sampled_char\n","\n","        # Exit condition: either hit max length\n","        # or find stop character.\n","        if (sampled_char == '\\n' or\n","           len(decoded_sentence) > max_decoder_seq_length):\n","            stop_condition = True\n","\n","        # Update the target sequence (of length 1).\n","        target_seq = np.zeros((1, 1, num_decoder_tokens))\n","        target_seq[0, 0, sampled_token_index] = 1.\n","\n","        # Update states\n","        states_value = [h, c]\n","\n","    return decoded_sentence\"\"\"\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"KSS1Zd6yZpqa","executionInfo":{"status":"aborted","timestamp":1666541139226,"user_tz":-330,"elapsed":16,"user":{"displayName":"Gouthaman S","userId":"11145458456231366145"}}},"outputs":[],"source":["encoder_input_data"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"otuaYQ3xZpqc","executionInfo":{"status":"aborted","timestamp":1666541139227,"user_tz":-330,"elapsed":17,"user":{"displayName":"Gouthaman S","userId":"11145458456231366145"}}},"outputs":[],"source":[]}],"metadata":{"kernelspec":{"display_name":"Python 3.9.12 ('base')","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.12"},"orig_nbformat":4,"vscode":{"interpreter":{"hash":"ad2bdc8ecc057115af97d19610ffacc2b4e99fae6737bb82f5d7fb13d2f2c186"}},"colab":{"provenance":[],"collapsed_sections":[]},"accelerator":"GPU"},"nbformat":4,"nbformat_minor":0}