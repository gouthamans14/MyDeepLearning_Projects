{"cells":[{"cell_type":"markdown","metadata":{"id":"BCqhBwPEL9Kq"},"source":["## Sequence to Sequence modelling"]},{"cell_type":"markdown","metadata":{"id":"3EmE5N9GL9K2"},"source":["simple implementation of sequence to sequence modelling in keras. The task is to translate short English sentences into French sentences, character-by-character using a sequence-to-sequence model.\n","\n","### summary of our implementation:\n","\n","- We start with input sequences from a domain (e.g. English sentences)and corresponding target sequences from another domain\n","    (e.g. French sentences).\n","- An encoder LSTM turns input sequences to 2 state vectors (we keep the last LSTM state and discard the outputs).\n","- A decoder LSTM is trained to turn the target sequences into the same sequence but offset by one timestep in the future,\n","    a training process called \"teacher forcing\" in this context. It uses as initial state the state vectors from the encoder.\n","    Effectively, the decoder learns to generate `targets[t+1...]` given `targets[...t]`, conditioned on the input sequence.\n","- In inference mode, when we want to decode unknown input sequences, we:\n","    - Encode the input sequence into state vectors\n","    - Start with a target sequence of size 1 (just the start-of-sequence character)\n","    - Feed the state vectors and 1-char target sequence to the decoder to produce predictions for the next character\n","    - Sample the next character using these predictions (we simply use argmax).\n","    - Append the sampled character to the target sequence\n","    - Repeat until we generate the end-of-sequence character or we hit the character limit.\n","    \n","### Download the data\n","\n","http://www.manythings.org/anki/fra-eng.zip\n","\n","### Steps:\n","\n","1) Turn the sentences into 3 Numpy arrays, encoder_input_data, decoder_input_data, decoder_target_data:\n","\n","- encoder_input_data is a 3D array of shape (num_pairs, max_english_sentence_length, num_english_characters) containing a one-hot vectorization of the English sentences\n","- decoder_input_data is a 3D array of shape (num_pairs, max_french_sentence_length, num_french_characters) containing a one-hot vectorization of the French sentences\n","- decoder_target_data is the same as decoder_input_data but offset by one timestep. decoder_target_data[:, t, :] will be the same as decoder_input_data[:, t + 1, :]\n","\n","2) Train a basic LSTM-based Seq2Seq model to predict decoder_target_data given encoder_input_data and decoder_input_data.\n","\n","3) Decode some sentences to check that the model is working (i.e. turn samples from encoder_input_data into corresponding samples from decoder_target_data).\n","\n"]},{"cell_type":"code","execution_count":1,"metadata":{"id":"DEKUDcCDL9K7","executionInfo":{"status":"ok","timestamp":1666550523561,"user_tz":-330,"elapsed":3203,"user":{"displayName":"Gouthaman S","userId":"11145458456231366145"}}},"outputs":[],"source":["# import modules\n","import numpy as np\n","import tensorflow as tf\n","from tensorflow import keras\n","\n","from keras.models import Model\n","from keras.layers import Input, LSTM, Dense\n","\n","import os"]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"4s1r3KcWMHQS","executionInfo":{"status":"ok","timestamp":1666550547039,"user_tz":-330,"elapsed":23492,"user":{"displayName":"Gouthaman S","userId":"11145458456231366145"}},"outputId":"1b294083-1df5-4906-df76-d07b81232d02"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","source":["import tensorflow as tf\n","tf.test.gpu_device_name()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":36},"id":"EaTSHRomOj6n","executionInfo":{"status":"ok","timestamp":1666550547870,"user_tz":-330,"elapsed":57,"user":{"displayName":"Gouthaman S","userId":"11145458456231366145"}},"outputId":"f64c1715-f061-427e-97a9-580506c55643"},"execution_count":3,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'/device:GPU:0'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":3}]},{"cell_type":"code","source":["os.getcwd()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":36},"id":"EwPWFwJ-MzQO","executionInfo":{"status":"ok","timestamp":1666550547872,"user_tz":-330,"elapsed":51,"user":{"displayName":"Gouthaman S","userId":"11145458456231366145"}},"outputId":"67aa01fe-e9a7-45e8-e5ad-44f51f07f37e"},"execution_count":4,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'/content'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":4}]},{"cell_type":"code","source":["os.chdir('/content/drive/MyDrive/MachineTransalation')"],"metadata":{"id":"FLNX6CGvM2Fu","executionInfo":{"status":"ok","timestamp":1666550547874,"user_tz":-330,"elapsed":48,"user":{"displayName":"Gouthaman S","userId":"11145458456231366145"}}},"execution_count":5,"outputs":[]},{"cell_type":"code","execution_count":6,"metadata":{"id":"dIVtihjCL9LA","executionInfo":{"status":"ok","timestamp":1666550547875,"user_tz":-330,"elapsed":48,"user":{"displayName":"Gouthaman S","userId":"11145458456231366145"}}},"outputs":[],"source":["batch_size = 64  # Batch size for training.\n","epochs = 100  # Number of epochs to traina for.\n","latent_dim = 256  # Latent dimensionality of the encoding space.\n","num_samples = 10000  # Number of samples to train on.\n","# Path to the data txt file on disk.\n","data_path = \"DATASET.txt\""]},{"cell_type":"code","execution_count":7,"metadata":{"id":"Fj30IiiZL9LD","executionInfo":{"status":"ok","timestamp":1666550547877,"user_tz":-330,"elapsed":48,"user":{"displayName":"Gouthaman S","userId":"11145458456231366145"}}},"outputs":[],"source":["# Reading the data (import data)\n","with open(data_path, \"r\", encoding=\"utf-8\") as f:\n","    lines = f.read().split(\"\\n\")"]},{"cell_type":"code","execution_count":8,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"JHUm6KywL9LG","executionInfo":{"status":"ok","timestamp":1666550547878,"user_tz":-330,"elapsed":47,"user":{"displayName":"Gouthaman S","userId":"11145458456231366145"}},"outputId":"bb7e3616-7cdb-4918-f299-2656c4f70c1a"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["['Wow!\\tवाह!',\n"," 'Help!\\tबचाओ!',\n"," 'Jump.\\tउछलो.',\n"," 'Jump.\\tकूदो.',\n"," 'Jump.\\tछलांग.',\n"," 'Hello!\\tनमस्ते।',\n"," 'Hello!\\tनमस्कार।',\n"," 'Cheers!\\tवाह-वाह!',\n"," 'Cheers!\\tचियर्स!',\n"," 'Got it?\\tसमझे कि नहीं?']"]},"metadata":{},"execution_count":8}],"source":["lines[:10]"]},{"cell_type":"code","execution_count":9,"metadata":{"id":"5HPKDZc4L9LL","executionInfo":{"status":"ok","timestamp":1666550547880,"user_tz":-330,"elapsed":40,"user":{"displayName":"Gouthaman S","userId":"11145458456231366145"}}},"outputs":[],"source":["# Vectorize the data.\n","input_texts = []\n","target_texts = []\n","input_characters = set()\n","target_characters = set()"]},{"cell_type":"code","execution_count":10,"metadata":{"id":"i7g_bzoPL9LQ","executionInfo":{"status":"ok","timestamp":1666550547882,"user_tz":-330,"elapsed":40,"user":{"displayName":"Gouthaman S","userId":"11145458456231366145"}}},"outputs":[],"source":["#input_text, target_text, _  = lines[0].split('\\t')"]},{"cell_type":"code","execution_count":12,"metadata":{"id":"hT7bjqsxL9LT","colab":{"base_uri":"https://localhost:8080/","height":165},"executionInfo":{"status":"error","timestamp":1666550556048,"user_tz":-330,"elapsed":12,"user":{"displayName":"Gouthaman S","userId":"11145458456231366145"}},"outputId":"59e8ffd5-9c15-4f47-b7f0-959546b91cdd"},"outputs":[{"output_type":"error","ename":"NameError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-12-7f81b4111fea>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0minput_text\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;31mNameError\u001b[0m: name 'input_text' is not defined"]}],"source":["input_text[:20]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xNgkohU2L9LX","executionInfo":{"status":"aborted","timestamp":1666550548289,"user_tz":-330,"elapsed":14,"user":{"displayName":"Gouthaman S","userId":"11145458456231366145"}}},"outputs":[],"source":["#target_text"]},{"cell_type":"code","execution_count":13,"metadata":{"id":"6bmigd-1L9LZ","outputId":"8d334b41-f357-44a2-e254-ffcd00b0bd89","colab":{"base_uri":"https://localhost:8080/","height":165},"executionInfo":{"status":"error","timestamp":1666550560844,"user_tz":-330,"elapsed":11,"user":{"displayName":"Gouthaman S","userId":"11145458456231366145"}}},"outputs":[{"output_type":"error","ename":"IndexError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)","\u001b[0;32m<ipython-input-13-1a60953177fe>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mlines\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m185583\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;31mIndexError\u001b[0m: list index out of range"]}],"source":["lines[185583]"]},{"cell_type":"code","execution_count":13,"metadata":{"id":"7AW_KR-JL9Lf","executionInfo":{"status":"ok","timestamp":1666550560845,"user_tz":-330,"elapsed":7,"user":{"displayName":"Gouthaman S","userId":"11145458456231366145"}}},"outputs":[],"source":[]},{"cell_type":"code","execution_count":14,"metadata":{"id":"yS5VFO9RL9Lh","outputId":"672317d0-5f1a-4ccc-cfe6-c72590448650","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1666550561505,"user_tz":-330,"elapsed":10,"user":{"displayName":"Gouthaman S","userId":"11145458456231366145"}}},"outputs":[{"output_type":"execute_result","data":{"text/plain":["2867"]},"metadata":{},"execution_count":14}],"source":["len(lines)-1"]},{"cell_type":"code","execution_count":15,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"MV_ST1PdL9Lj","executionInfo":{"status":"ok","timestamp":1666550561506,"user_tz":-330,"elapsed":9,"user":{"displayName":"Gouthaman S","userId":"11145458456231366145"}},"outputId":"4193f98a-9f26-4443-d668-c1fcb06d05b3"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["2867"]},"metadata":{},"execution_count":15}],"source":["min(num_samples, len(lines) - 1)"]},{"cell_type":"code","execution_count":16,"metadata":{"id":"eeS-WH-eL9Ll","executionInfo":{"status":"ok","timestamp":1666550561507,"user_tz":-330,"elapsed":5,"user":{"displayName":"Gouthaman S","userId":"11145458456231366145"}}},"outputs":[],"source":["#\"\\t\" + target_text + \"\\n\""]},{"cell_type":"code","execution_count":17,"metadata":{"id":"qeXkZNgEL9Ln","executionInfo":{"status":"ok","timestamp":1666550562713,"user_tz":-330,"elapsed":11,"user":{"displayName":"Gouthaman S","userId":"11145458456231366145"}}},"outputs":[],"source":["#Creating input_characters & Output Characters arrays\n","for line in lines[: min(num_samples, len(lines) - 1)]:\n","    input_text, target_text = line.split(\"\\t\")\n","    # We use \"tab\" as the \"start sequence\" character\n","    # for the targets, and \"\\n\" as \"end sequence\" character.\n","    target_text = \"\\t\" + target_text + \"\\n\"\n","    input_texts.append(input_text)\n","    target_texts.append(target_text)\n","    for char in input_text:\n","        if char not in input_characters:\n","            input_characters.add(char)\n","    for char in target_text:\n","        if char not in target_characters:\n","            target_characters.add(char)"]},{"cell_type":"code","execution_count":18,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ip8ivHcnL9Lp","executionInfo":{"status":"ok","timestamp":1666550562715,"user_tz":-330,"elapsed":11,"user":{"displayName":"Gouthaman S","userId":"11145458456231366145"}},"outputId":"dd10be0f-acd8-431b-cc12-94026932b66b"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["['Wow!',\n"," 'Help!',\n"," 'Jump.',\n"," 'Jump.',\n"," 'Jump.',\n"," 'Hello!',\n"," 'Hello!',\n"," 'Cheers!',\n"," 'Cheers!',\n"," 'Got it?']"]},"metadata":{},"execution_count":18}],"source":["input_texts[:10]"]},{"cell_type":"code","execution_count":19,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"EV5JvZSGL9Lr","executionInfo":{"status":"ok","timestamp":1666550563282,"user_tz":-330,"elapsed":8,"user":{"displayName":"Gouthaman S","userId":"11145458456231366145"}},"outputId":"9a45292c-c755-43d2-dbee-789869e22bee"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["['\\tवाह!\\n',\n"," '\\tबचाओ!\\n',\n"," '\\tउछलो.\\n',\n"," '\\tकूदो.\\n',\n"," '\\tछलांग.\\n',\n"," '\\tनमस्ते।\\n',\n"," '\\tनमस्कार।\\n',\n"," '\\tवाह-वाह!\\n',\n"," '\\tचियर्स!\\n',\n"," '\\tसमझे कि नहीं?\\n']"]},"metadata":{},"execution_count":19}],"source":["target_texts[:10]"]},{"cell_type":"code","execution_count":20,"metadata":{"id":"7gGIUz17L9Ls","executionInfo":{"status":"ok","timestamp":1666550563283,"user_tz":-330,"elapsed":6,"user":{"displayName":"Gouthaman S","userId":"11145458456231366145"}}},"outputs":[],"source":["#"]},{"cell_type":"code","execution_count":21,"metadata":{"id":"1bg_0WbSL9Lu","executionInfo":{"status":"ok","timestamp":1666550564956,"user_tz":-330,"elapsed":7,"user":{"displayName":"Gouthaman S","userId":"11145458456231366145"}}},"outputs":[],"source":["#target_characters"]},{"cell_type":"code","execution_count":22,"metadata":{"id":"By1QioCYL9Lw","executionInfo":{"status":"ok","timestamp":1666550565654,"user_tz":-330,"elapsed":13,"user":{"displayName":"Gouthaman S","userId":"11145458456231366145"}}},"outputs":[],"source":["input_characters = sorted(list(input_characters))\n","target_characters = sorted(list(target_characters))\n","\n","num_encoder_tokens = len(input_characters)\n","num_decoder_tokens = len(target_characters)\n","\n","max_encoder_seq_length = max([len(txt) for txt in input_texts])\n","max_decoder_seq_length = max([len(txt) for txt in target_texts])"]},{"cell_type":"code","execution_count":23,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"jMKB7u3oL9Lx","executionInfo":{"status":"ok","timestamp":1666550565656,"user_tz":-330,"elapsed":12,"user":{"displayName":"Gouthaman S","userId":"11145458456231366145"}},"outputId":"96d00ec5-40ba-4a0b-e1f8-60cb75b01b75"},"outputs":[{"output_type":"stream","name":"stdout","text":["Number of samples: 2867\n","Number of unique input tokens: 70\n","Number of unique output tokens: 92\n","Max sequence length for inputs: 124\n","Max sequence length for outputs: 123\n"]}],"source":["print(\"Number of samples:\", len(input_texts))\n","print(\"Number of unique input tokens:\", num_encoder_tokens)\n","print(\"Number of unique output tokens:\", num_decoder_tokens)\n","print(\"Max sequence length for inputs:\", max_encoder_seq_length)\n","print(\"Max sequence length for outputs:\", max_decoder_seq_length)"]},{"cell_type":"code","execution_count":24,"metadata":{"id":"9L4jKgU8L9Ly","executionInfo":{"status":"ok","timestamp":1666550566036,"user_tz":-330,"elapsed":10,"user":{"displayName":"Gouthaman S","userId":"11145458456231366145"}}},"outputs":[],"source":["#input_characters"]},{"cell_type":"code","execution_count":25,"metadata":{"id":"HJ1hVmBsL9Lz","executionInfo":{"status":"ok","timestamp":1666550566610,"user_tz":-330,"elapsed":3,"user":{"displayName":"Gouthaman S","userId":"11145458456231366145"}}},"outputs":[],"source":["input_token_index = dict([(char, i) for i, char in enumerate(input_characters)])\n","target_token_index = dict([(char, i) for i, char in enumerate(target_characters)])"]},{"cell_type":"code","execution_count":26,"metadata":{"id":"nCgdU8PnL9L0","outputId":"432458ff-489b-4472-b343-7990a4f7a394","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1666550567586,"user_tz":-330,"elapsed":9,"user":{"displayName":"Gouthaman S","userId":"11145458456231366145"}}},"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([[[0., 0.],\n","        [0., 0.],\n","        [0., 0.]],\n","\n","       [[0., 0.],\n","        [0., 0.],\n","        [0., 0.]]])"]},"metadata":{},"execution_count":26}],"source":["np.zeros((2,3,2))"]},{"cell_type":"code","execution_count":27,"metadata":{"id":"7lYxtiJWL9L1","executionInfo":{"status":"ok","timestamp":1666550569690,"user_tz":-330,"elapsed":6,"user":{"displayName":"Gouthaman S","userId":"11145458456231366145"}}},"outputs":[],"source":["encoder_input_data = np.zeros(\n","    (len(input_texts), max_encoder_seq_length, num_encoder_tokens), dtype=\"float32\"\n",")"]},{"cell_type":"code","execution_count":28,"metadata":{"id":"HRQ7qCWxL9L2","executionInfo":{"status":"ok","timestamp":1666550570165,"user_tz":-330,"elapsed":8,"user":{"displayName":"Gouthaman S","userId":"11145458456231366145"}}},"outputs":[],"source":["decoder_input_data = np.zeros(\n","    (len(input_texts), max_decoder_seq_length, num_decoder_tokens), dtype=\"float32\"\n",")"]},{"cell_type":"code","execution_count":29,"metadata":{"id":"-Q0OrurbL9L4","executionInfo":{"status":"ok","timestamp":1666550570907,"user_tz":-330,"elapsed":749,"user":{"displayName":"Gouthaman S","userId":"11145458456231366145"}}},"outputs":[],"source":["decoder_target_data = np.zeros(\n","    (len(input_texts), max_decoder_seq_length, num_decoder_tokens), dtype=\"float32\"\n",")"]},{"cell_type":"code","execution_count":30,"metadata":{"id":"L5PoO2lIL9L6","executionInfo":{"status":"ok","timestamp":1666550570908,"user_tz":-330,"elapsed":7,"user":{"displayName":"Gouthaman S","userId":"11145458456231366145"}}},"outputs":[],"source":["#input_texts"]},{"cell_type":"code","execution_count":31,"metadata":{"id":"LR2K-8xyL9L6","executionInfo":{"status":"ok","timestamp":1666550571520,"user_tz":-330,"elapsed":6,"user":{"displayName":"Gouthaman S","userId":"11145458456231366145"}}},"outputs":[],"source":["for i, (input_text, target_text) in enumerate(zip(input_texts, target_texts)):\n","    for t, char in enumerate(input_text):\n","        encoder_input_data[i, t, input_token_index[char]] = 1.0\n","        encoder_input_data[i, t + 1 :, input_token_index[\" \"]] = 1.0\n","    for t, char in enumerate(target_text):\n","        # decoder_target_data is ahead of decoder_input_data by one timestep\n","        decoder_input_data[i, t, target_token_index[char]] = 1.0\n","        if t > 0:\n","            # decoder_target_data will be ahead by one timestep\n","            # and will not include the start character.\n","            decoder_target_data[i, t - 1, target_token_index[char]] = 1.0\n","            \n","    decoder_input_data[i, t + 1 :, target_token_index[\" \"]] = 1.0\n","    decoder_target_data[i, t:, target_token_index[\" \"]] = 1.0"]},{"cell_type":"markdown","metadata":{"id":"F5vzKrr2L9L8"},"source":["## Build the model"]},{"cell_type":"code","source":[],"metadata":{"id":"0ghjxF63N-iK","executionInfo":{"status":"ok","timestamp":1666550573695,"user_tz":-330,"elapsed":4,"user":{"displayName":"Gouthaman S","userId":"11145458456231366145"}}},"execution_count":31,"outputs":[]},{"cell_type":"code","execution_count":32,"metadata":{"id":"7J1kTPCyL9L9","executionInfo":{"status":"ok","timestamp":1666550575657,"user_tz":-330,"elapsed":527,"user":{"displayName":"Gouthaman S","userId":"11145458456231366145"}}},"outputs":[],"source":["# Define an input sequence and process it.\n","encoder_inputs = keras.Input(shape=(None, num_encoder_tokens))\n","encoder = keras.layers.LSTM(latent_dim, return_state=True)\n","encoder_outputs, state_h, state_c = encoder(encoder_inputs)\n","\n","# We discard `encoder_outputs` and only keep the states.\n","encoder_states = [state_h, state_c]\n","\n","# Set up the decoder, using `encoder_states` as initial state.\n","decoder_inputs = keras.Input(shape=(None, num_decoder_tokens))\n","\n","# We set up our decoder to return full output sequences, and to return internal states as well. We don't use the\n","# return states in the training model, but we will use them in inference.\n","decoder_lstm = keras.layers.LSTM(latent_dim, return_sequences=True, return_state=True)\n","decoder_outputs, _, _ = decoder_lstm(decoder_inputs, initial_state=encoder_states)\n","decoder_dense = keras.layers.Dense(num_decoder_tokens, activation=\"softmax\")\n","decoder_outputs = decoder_dense(decoder_outputs)\n","\n","# Define the model that will turn\n","# `encoder_input_data` & `decoder_input_data` into `decoder_target_data`\n","model = keras.Model([encoder_inputs, decoder_inputs], decoder_outputs)"]},{"cell_type":"code","source":["from keras.callbacks import EarlyStopping,ModelCheckpoint\n","\n","es = EarlyStopping(monitor='val_loss',mode='min',verbose=1, patience=200)\n","mc = ModelCheckpoint('hinditoenglishmodel.h5',monitor='val_accuracy', mode='max', save_best_only=True,verbose=1)"],"metadata":{"id":"Dl84laCbKHm6","executionInfo":{"status":"ok","timestamp":1666550629837,"user_tz":-330,"elapsed":424,"user":{"displayName":"Gouthaman S","userId":"11145458456231366145"}}},"execution_count":34,"outputs":[]},{"cell_type":"code","execution_count":35,"metadata":{"id":"lQ_UvNqIL9L9","outputId":"a505fe70-aefe-4a32-eb16-9c09d28c4166","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1666550909977,"user_tz":-330,"elapsed":261456,"user":{"displayName":"Gouthaman S","userId":"11145458456231366145"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1/1000\n","35/36 [============================>.] - ETA: 0s - loss: 0.4956 - accuracy: 0.8685\n","Epoch 1: val_accuracy improved from -inf to 0.76815, saving model to hinditoenglishmodel.h5\n","36/36 [==============================] - 5s 70ms/step - loss: 0.4956 - accuracy: 0.8685 - val_loss: 0.8835 - val_accuracy: 0.7682\n","Epoch 2/1000\n","34/36 [===========================>..] - ETA: 0s - loss: 0.4804 - accuracy: 0.8717\n","Epoch 2: val_accuracy improved from 0.76815 to 0.77156, saving model to hinditoenglishmodel.h5\n","36/36 [==============================] - 1s 30ms/step - loss: 0.4802 - accuracy: 0.8717 - val_loss: 0.8538 - val_accuracy: 0.7716\n","Epoch 3/1000\n","34/36 [===========================>..] - ETA: 0s - loss: 0.4716 - accuracy: 0.8733\n","Epoch 3: val_accuracy improved from 0.77156 to 0.77297, saving model to hinditoenglishmodel.h5\n","36/36 [==============================] - 1s 30ms/step - loss: 0.4702 - accuracy: 0.8736 - val_loss: 0.8478 - val_accuracy: 0.7730\n","Epoch 4/1000\n","35/36 [============================>.] - ETA: 0s - loss: 0.4634 - accuracy: 0.8750\n","Epoch 4: val_accuracy improved from 0.77297 to 0.77628, saving model to hinditoenglishmodel.h5\n","36/36 [==============================] - 1s 31ms/step - loss: 0.4635 - accuracy: 0.8750 - val_loss: 0.8322 - val_accuracy: 0.7763\n","Epoch 5/1000\n","34/36 [===========================>..] - ETA: 0s - loss: 0.4560 - accuracy: 0.8757\n","Epoch 5: val_accuracy did not improve from 0.77628\n","36/36 [==============================] - 1s 29ms/step - loss: 0.4545 - accuracy: 0.8761 - val_loss: 0.8350 - val_accuracy: 0.7744\n","Epoch 6/1000\n","35/36 [============================>.] - ETA: 0s - loss: 0.4470 - accuracy: 0.8777\n","Epoch 6: val_accuracy improved from 0.77628 to 0.77934, saving model to hinditoenglishmodel.h5\n","36/36 [==============================] - 1s 30ms/step - loss: 0.4475 - accuracy: 0.8775 - val_loss: 0.8186 - val_accuracy: 0.7793\n","Epoch 7/1000\n","35/36 [============================>.] - ETA: 0s - loss: 0.4408 - accuracy: 0.8786\n","Epoch 7: val_accuracy improved from 0.77934 to 0.77961, saving model to hinditoenglishmodel.h5\n","36/36 [==============================] - 1s 30ms/step - loss: 0.4404 - accuracy: 0.8788 - val_loss: 0.8113 - val_accuracy: 0.7796\n","Epoch 8/1000\n","36/36 [==============================] - ETA: 0s - loss: 0.4338 - accuracy: 0.8803\n","Epoch 8: val_accuracy did not improve from 0.77961\n","36/36 [==============================] - 1s 29ms/step - loss: 0.4338 - accuracy: 0.8803 - val_loss: 0.8127 - val_accuracy: 0.7790\n","Epoch 9/1000\n","35/36 [============================>.] - ETA: 0s - loss: 0.4267 - accuracy: 0.8817\n","Epoch 9: val_accuracy improved from 0.77961 to 0.78227, saving model to hinditoenglishmodel.h5\n","36/36 [==============================] - 1s 30ms/step - loss: 0.4276 - accuracy: 0.8815 - val_loss: 0.7996 - val_accuracy: 0.7823\n","Epoch 10/1000\n","35/36 [============================>.] - ETA: 0s - loss: 0.4218 - accuracy: 0.8824\n","Epoch 10: val_accuracy did not improve from 0.78227\n","36/36 [==============================] - 1s 29ms/step - loss: 0.4220 - accuracy: 0.8824 - val_loss: 0.7991 - val_accuracy: 0.7807\n","Epoch 11/1000\n","36/36 [==============================] - ETA: 0s - loss: 0.4162 - accuracy: 0.8839\n","Epoch 11: val_accuracy improved from 0.78227 to 0.78277, saving model to hinditoenglishmodel.h5\n","36/36 [==============================] - 1s 30ms/step - loss: 0.4162 - accuracy: 0.8839 - val_loss: 0.7943 - val_accuracy: 0.7828\n","Epoch 12/1000\n","36/36 [==============================] - ETA: 0s - loss: 0.4107 - accuracy: 0.8852\n","Epoch 12: val_accuracy improved from 0.78277 to 0.78384, saving model to hinditoenglishmodel.h5\n","36/36 [==============================] - 1s 31ms/step - loss: 0.4107 - accuracy: 0.8852 - val_loss: 0.7908 - val_accuracy: 0.7838\n","Epoch 13/1000\n","34/36 [===========================>..] - ETA: 0s - loss: 0.4067 - accuracy: 0.8860\n","Epoch 13: val_accuracy improved from 0.78384 to 0.78656, saving model to hinditoenglishmodel.h5\n","36/36 [==============================] - 1s 30ms/step - loss: 0.4050 - accuracy: 0.8864 - val_loss: 0.7781 - val_accuracy: 0.7866\n","Epoch 14/1000\n","36/36 [==============================] - ETA: 0s - loss: 0.3995 - accuracy: 0.8878\n","Epoch 14: val_accuracy improved from 0.78656 to 0.78662, saving model to hinditoenglishmodel.h5\n","36/36 [==============================] - 1s 31ms/step - loss: 0.3995 - accuracy: 0.8878 - val_loss: 0.7767 - val_accuracy: 0.7866\n","Epoch 15/1000\n","36/36 [==============================] - ETA: 0s - loss: 0.3946 - accuracy: 0.8893\n","Epoch 15: val_accuracy improved from 0.78662 to 0.78799, saving model to hinditoenglishmodel.h5\n","36/36 [==============================] - 1s 31ms/step - loss: 0.3946 - accuracy: 0.8893 - val_loss: 0.7799 - val_accuracy: 0.7880\n","Epoch 16/1000\n","34/36 [===========================>..] - ETA: 0s - loss: 0.3894 - accuracy: 0.8903\n","Epoch 16: val_accuracy improved from 0.78799 to 0.78815, saving model to hinditoenglishmodel.h5\n","36/36 [==============================] - 1s 30ms/step - loss: 0.3897 - accuracy: 0.8903 - val_loss: 0.7685 - val_accuracy: 0.7882\n","Epoch 17/1000\n","35/36 [============================>.] - ETA: 0s - loss: 0.3842 - accuracy: 0.8921\n","Epoch 17: val_accuracy improved from 0.78815 to 0.78889, saving model to hinditoenglishmodel.h5\n","36/36 [==============================] - 1s 30ms/step - loss: 0.3847 - accuracy: 0.8919 - val_loss: 0.7662 - val_accuracy: 0.7889\n","Epoch 18/1000\n","35/36 [============================>.] - ETA: 0s - loss: 0.3798 - accuracy: 0.8932\n","Epoch 18: val_accuracy did not improve from 0.78889\n","36/36 [==============================] - 1s 30ms/step - loss: 0.3793 - accuracy: 0.8933 - val_loss: 0.7701 - val_accuracy: 0.7888\n","Epoch 19/1000\n","34/36 [===========================>..] - ETA: 0s - loss: 0.3752 - accuracy: 0.8943\n","Epoch 19: val_accuracy improved from 0.78889 to 0.79192, saving model to hinditoenglishmodel.h5\n","36/36 [==============================] - 1s 30ms/step - loss: 0.3745 - accuracy: 0.8947 - val_loss: 0.7598 - val_accuracy: 0.7919\n","Epoch 20/1000\n","35/36 [============================>.] - ETA: 0s - loss: 0.3683 - accuracy: 0.8964\n","Epoch 20: val_accuracy did not improve from 0.79192\n","36/36 [==============================] - 1s 30ms/step - loss: 0.3694 - accuracy: 0.8961 - val_loss: 0.7649 - val_accuracy: 0.7894\n","Epoch 21/1000\n","35/36 [============================>.] - ETA: 0s - loss: 0.3649 - accuracy: 0.8971\n","Epoch 21: val_accuracy improved from 0.79192 to 0.79332, saving model to hinditoenglishmodel.h5\n","36/36 [==============================] - 1s 31ms/step - loss: 0.3650 - accuracy: 0.8972 - val_loss: 0.7583 - val_accuracy: 0.7933\n","Epoch 22/1000\n","36/36 [==============================] - ETA: 0s - loss: 0.3600 - accuracy: 0.8982\n","Epoch 22: val_accuracy did not improve from 0.79332\n","36/36 [==============================] - 1s 29ms/step - loss: 0.3600 - accuracy: 0.8982 - val_loss: 0.7693 - val_accuracy: 0.7907\n","Epoch 23/1000\n","36/36 [==============================] - ETA: 0s - loss: 0.3555 - accuracy: 0.8998\n","Epoch 23: val_accuracy improved from 0.79332 to 0.79396, saving model to hinditoenglishmodel.h5\n","36/36 [==============================] - 1s 31ms/step - loss: 0.3555 - accuracy: 0.8998 - val_loss: 0.7574 - val_accuracy: 0.7940\n","Epoch 24/1000\n","35/36 [============================>.] - ETA: 0s - loss: 0.3516 - accuracy: 0.9008\n","Epoch 24: val_accuracy improved from 0.79396 to 0.79407, saving model to hinditoenglishmodel.h5\n","36/36 [==============================] - 1s 31ms/step - loss: 0.3511 - accuracy: 0.9010 - val_loss: 0.7571 - val_accuracy: 0.7941\n","Epoch 25/1000\n","35/36 [============================>.] - ETA: 0s - loss: 0.3473 - accuracy: 0.9025\n","Epoch 25: val_accuracy did not improve from 0.79407\n","36/36 [==============================] - 1s 30ms/step - loss: 0.3461 - accuracy: 0.9027 - val_loss: 0.7636 - val_accuracy: 0.7928\n","Epoch 26/1000\n","34/36 [===========================>..] - ETA: 0s - loss: 0.3408 - accuracy: 0.9039\n","Epoch 26: val_accuracy improved from 0.79407 to 0.79488, saving model to hinditoenglishmodel.h5\n","36/36 [==============================] - 1s 30ms/step - loss: 0.3416 - accuracy: 0.9037 - val_loss: 0.7569 - val_accuracy: 0.7949\n","Epoch 27/1000\n","34/36 [===========================>..] - ETA: 0s - loss: 0.3366 - accuracy: 0.9049\n","Epoch 27: val_accuracy did not improve from 0.79488\n","36/36 [==============================] - 1s 30ms/step - loss: 0.3370 - accuracy: 0.9048 - val_loss: 0.7609 - val_accuracy: 0.7940\n","Epoch 28/1000\n","34/36 [===========================>..] - ETA: 0s - loss: 0.3322 - accuracy: 0.9062\n","Epoch 28: val_accuracy did not improve from 0.79488\n","36/36 [==============================] - 1s 29ms/step - loss: 0.3321 - accuracy: 0.9062 - val_loss: 0.7589 - val_accuracy: 0.7945\n","Epoch 29/1000\n","34/36 [===========================>..] - ETA: 0s - loss: 0.3277 - accuracy: 0.9074\n","Epoch 29: val_accuracy did not improve from 0.79488\n","36/36 [==============================] - 1s 29ms/step - loss: 0.3278 - accuracy: 0.9075 - val_loss: 0.7648 - val_accuracy: 0.7946\n","Epoch 30/1000\n","34/36 [===========================>..] - ETA: 0s - loss: 0.3224 - accuracy: 0.9090\n","Epoch 30: val_accuracy improved from 0.79488 to 0.79569, saving model to hinditoenglishmodel.h5\n","36/36 [==============================] - 1s 30ms/step - loss: 0.3232 - accuracy: 0.9087 - val_loss: 0.7626 - val_accuracy: 0.7957\n","Epoch 31/1000\n","35/36 [============================>.] - ETA: 0s - loss: 0.3179 - accuracy: 0.9105\n","Epoch 31: val_accuracy did not improve from 0.79569\n","36/36 [==============================] - 1s 30ms/step - loss: 0.3187 - accuracy: 0.9103 - val_loss: 0.7730 - val_accuracy: 0.7928\n","Epoch 32/1000\n","35/36 [============================>.] - ETA: 0s - loss: 0.3137 - accuracy: 0.9115\n","Epoch 32: val_accuracy did not improve from 0.79569\n","36/36 [==============================] - 1s 29ms/step - loss: 0.3139 - accuracy: 0.9114 - val_loss: 0.7717 - val_accuracy: 0.7943\n","Epoch 33/1000\n","35/36 [============================>.] - ETA: 0s - loss: 0.3091 - accuracy: 0.9126\n","Epoch 33: val_accuracy did not improve from 0.79569\n","36/36 [==============================] - 1s 30ms/step - loss: 0.3093 - accuracy: 0.9125 - val_loss: 0.7736 - val_accuracy: 0.7939\n","Epoch 34/1000\n","35/36 [============================>.] - ETA: 0s - loss: 0.3052 - accuracy: 0.9138\n","Epoch 34: val_accuracy did not improve from 0.79569\n","36/36 [==============================] - 1s 29ms/step - loss: 0.3048 - accuracy: 0.9139 - val_loss: 0.7763 - val_accuracy: 0.7921\n","Epoch 35/1000\n","35/36 [============================>.] - ETA: 0s - loss: 0.3003 - accuracy: 0.9149\n","Epoch 35: val_accuracy did not improve from 0.79569\n","36/36 [==============================] - 1s 29ms/step - loss: 0.3002 - accuracy: 0.9150 - val_loss: 0.7783 - val_accuracy: 0.7954\n","Epoch 36/1000\n","35/36 [============================>.] - ETA: 0s - loss: 0.2943 - accuracy: 0.9170\n","Epoch 36: val_accuracy did not improve from 0.79569\n","36/36 [==============================] - 1s 29ms/step - loss: 0.2948 - accuracy: 0.9169 - val_loss: 0.7897 - val_accuracy: 0.7938\n","Epoch 37/1000\n","36/36 [==============================] - ETA: 0s - loss: 0.2902 - accuracy: 0.9183\n","Epoch 37: val_accuracy did not improve from 0.79569\n","36/36 [==============================] - 1s 30ms/step - loss: 0.2902 - accuracy: 0.9183 - val_loss: 0.7922 - val_accuracy: 0.7923\n","Epoch 38/1000\n","35/36 [============================>.] - ETA: 0s - loss: 0.2852 - accuracy: 0.9194\n","Epoch 38: val_accuracy did not improve from 0.79569\n","36/36 [==============================] - 1s 29ms/step - loss: 0.2851 - accuracy: 0.9194 - val_loss: 0.8085 - val_accuracy: 0.7918\n","Epoch 39/1000\n","35/36 [============================>.] - ETA: 0s - loss: 0.2807 - accuracy: 0.9207\n","Epoch 39: val_accuracy did not improve from 0.79569\n","36/36 [==============================] - 1s 30ms/step - loss: 0.2812 - accuracy: 0.9206 - val_loss: 0.7972 - val_accuracy: 0.7919\n","Epoch 40/1000\n","34/36 [===========================>..] - ETA: 0s - loss: 0.2772 - accuracy: 0.9219\n","Epoch 40: val_accuracy did not improve from 0.79569\n","36/36 [==============================] - 1s 29ms/step - loss: 0.2762 - accuracy: 0.9221 - val_loss: 0.8115 - val_accuracy: 0.7908\n","Epoch 41/1000\n","36/36 [==============================] - ETA: 0s - loss: 0.2720 - accuracy: 0.9236\n","Epoch 41: val_accuracy did not improve from 0.79569\n","36/36 [==============================] - 1s 30ms/step - loss: 0.2720 - accuracy: 0.9236 - val_loss: 0.8053 - val_accuracy: 0.7915\n","Epoch 42/1000\n","35/36 [============================>.] - ETA: 0s - loss: 0.2689 - accuracy: 0.9243\n","Epoch 42: val_accuracy did not improve from 0.79569\n","36/36 [==============================] - 1s 30ms/step - loss: 0.2689 - accuracy: 0.9243 - val_loss: 0.8147 - val_accuracy: 0.7896\n","Epoch 43/1000\n","34/36 [===========================>..] - ETA: 0s - loss: 0.2644 - accuracy: 0.9253\n","Epoch 43: val_accuracy did not improve from 0.79569\n","36/36 [==============================] - 1s 30ms/step - loss: 0.2641 - accuracy: 0.9254 - val_loss: 0.8213 - val_accuracy: 0.7904\n","Epoch 44/1000\n","36/36 [==============================] - ETA: 0s - loss: 0.2596 - accuracy: 0.9270\n","Epoch 44: val_accuracy did not improve from 0.79569\n","36/36 [==============================] - 1s 30ms/step - loss: 0.2596 - accuracy: 0.9270 - val_loss: 0.8201 - val_accuracy: 0.7915\n","Epoch 45/1000\n","36/36 [==============================] - ETA: 0s - loss: 0.2555 - accuracy: 0.9285\n","Epoch 45: val_accuracy did not improve from 0.79569\n","36/36 [==============================] - 1s 30ms/step - loss: 0.2555 - accuracy: 0.9285 - val_loss: 0.8340 - val_accuracy: 0.7885\n","Epoch 46/1000\n","36/36 [==============================] - ETA: 0s - loss: 0.2516 - accuracy: 0.9294\n","Epoch 46: val_accuracy did not improve from 0.79569\n","36/36 [==============================] - 1s 33ms/step - loss: 0.2516 - accuracy: 0.9294 - val_loss: 0.8498 - val_accuracy: 0.7879\n","Epoch 47/1000\n","36/36 [==============================] - ETA: 0s - loss: 0.2474 - accuracy: 0.9307\n","Epoch 47: val_accuracy did not improve from 0.79569\n","36/36 [==============================] - 1s 33ms/step - loss: 0.2474 - accuracy: 0.9307 - val_loss: 0.8448 - val_accuracy: 0.7896\n","Epoch 48/1000\n","35/36 [============================>.] - ETA: 0s - loss: 0.2431 - accuracy: 0.9320\n","Epoch 48: val_accuracy did not improve from 0.79569\n","36/36 [==============================] - 1s 31ms/step - loss: 0.2429 - accuracy: 0.9320 - val_loss: 0.8609 - val_accuracy: 0.7880\n","Epoch 49/1000\n","35/36 [============================>.] - ETA: 0s - loss: 0.2400 - accuracy: 0.9329\n","Epoch 49: val_accuracy did not improve from 0.79569\n","36/36 [==============================] - 1s 30ms/step - loss: 0.2402 - accuracy: 0.9328 - val_loss: 0.8569 - val_accuracy: 0.7880\n","Epoch 50/1000\n","35/36 [============================>.] - ETA: 0s - loss: 0.2356 - accuracy: 0.9342\n","Epoch 50: val_accuracy did not improve from 0.79569\n","36/36 [==============================] - 1s 31ms/step - loss: 0.2354 - accuracy: 0.9342 - val_loss: 0.8584 - val_accuracy: 0.7891\n","Epoch 51/1000\n","35/36 [============================>.] - ETA: 0s - loss: 0.2315 - accuracy: 0.9354\n","Epoch 51: val_accuracy did not improve from 0.79569\n","36/36 [==============================] - 1s 30ms/step - loss: 0.2315 - accuracy: 0.9354 - val_loss: 0.8753 - val_accuracy: 0.7881\n","Epoch 52/1000\n","35/36 [============================>.] - ETA: 0s - loss: 0.2280 - accuracy: 0.9362\n","Epoch 52: val_accuracy did not improve from 0.79569\n","36/36 [==============================] - 1s 31ms/step - loss: 0.2280 - accuracy: 0.9363 - val_loss: 0.8780 - val_accuracy: 0.7874\n","Epoch 53/1000\n","36/36 [==============================] - ETA: 0s - loss: 0.2239 - accuracy: 0.9378\n","Epoch 53: val_accuracy did not improve from 0.79569\n","36/36 [==============================] - 1s 30ms/step - loss: 0.2239 - accuracy: 0.9378 - val_loss: 0.8773 - val_accuracy: 0.7872\n","Epoch 54/1000\n","36/36 [==============================] - ETA: 0s - loss: 0.2205 - accuracy: 0.9388\n","Epoch 54: val_accuracy did not improve from 0.79569\n","36/36 [==============================] - 1s 33ms/step - loss: 0.2205 - accuracy: 0.9388 - val_loss: 0.8872 - val_accuracy: 0.7873\n","Epoch 55/1000\n","34/36 [===========================>..] - ETA: 0s - loss: 0.2166 - accuracy: 0.9396\n","Epoch 55: val_accuracy did not improve from 0.79569\n","36/36 [==============================] - 1s 33ms/step - loss: 0.2171 - accuracy: 0.9394 - val_loss: 0.8790 - val_accuracy: 0.7874\n","Epoch 56/1000\n","34/36 [===========================>..] - ETA: 0s - loss: 0.2127 - accuracy: 0.9408\n","Epoch 56: val_accuracy did not improve from 0.79569\n","36/36 [==============================] - 1s 34ms/step - loss: 0.2133 - accuracy: 0.9408 - val_loss: 0.9052 - val_accuracy: 0.7852\n","Epoch 57/1000\n","36/36 [==============================] - ETA: 0s - loss: 0.2098 - accuracy: 0.9420\n","Epoch 57: val_accuracy did not improve from 0.79569\n","36/36 [==============================] - 1s 31ms/step - loss: 0.2098 - accuracy: 0.9420 - val_loss: 0.9080 - val_accuracy: 0.7848\n","Epoch 58/1000\n","35/36 [============================>.] - ETA: 0s - loss: 0.2071 - accuracy: 0.9428\n","Epoch 58: val_accuracy did not improve from 0.79569\n","36/36 [==============================] - 1s 30ms/step - loss: 0.2072 - accuracy: 0.9428 - val_loss: 0.9167 - val_accuracy: 0.7858\n","Epoch 59/1000\n","36/36 [==============================] - ETA: 0s - loss: 0.2028 - accuracy: 0.9437\n","Epoch 59: val_accuracy did not improve from 0.79569\n","36/36 [==============================] - 1s 31ms/step - loss: 0.2028 - accuracy: 0.9437 - val_loss: 0.9260 - val_accuracy: 0.7852\n","Epoch 60/1000\n","35/36 [============================>.] - ETA: 0s - loss: 0.2010 - accuracy: 0.9442\n","Epoch 60: val_accuracy did not improve from 0.79569\n","36/36 [==============================] - 1s 30ms/step - loss: 0.2012 - accuracy: 0.9441 - val_loss: 0.9396 - val_accuracy: 0.7830\n","Epoch 61/1000\n","35/36 [============================>.] - ETA: 0s - loss: 0.2822 - accuracy: 0.9230\n","Epoch 61: val_accuracy did not improve from 0.79569\n","36/36 [==============================] - 1s 31ms/step - loss: 0.2819 - accuracy: 0.9230 - val_loss: 0.8824 - val_accuracy: 0.7876\n","Epoch 62/1000\n","35/36 [============================>.] - ETA: 0s - loss: 0.2539 - accuracy: 0.9270\n","Epoch 62: val_accuracy did not improve from 0.79569\n","36/36 [==============================] - 1s 30ms/step - loss: 0.2535 - accuracy: 0.9271 - val_loss: 0.8617 - val_accuracy: 0.7913\n","Epoch 63/1000\n","36/36 [==============================] - ETA: 0s - loss: 0.2300 - accuracy: 0.9344\n","Epoch 63: val_accuracy did not improve from 0.79569\n","36/36 [==============================] - 1s 30ms/step - loss: 0.2300 - accuracy: 0.9344 - val_loss: 0.8812 - val_accuracy: 0.7900\n","Epoch 64/1000\n","35/36 [============================>.] - ETA: 0s - loss: 0.2196 - accuracy: 0.9378\n","Epoch 64: val_accuracy did not improve from 0.79569\n","36/36 [==============================] - 1s 30ms/step - loss: 0.2196 - accuracy: 0.9377 - val_loss: 0.8898 - val_accuracy: 0.7899\n","Epoch 65/1000\n","35/36 [============================>.] - ETA: 0s - loss: 0.2101 - accuracy: 0.9412\n","Epoch 65: val_accuracy did not improve from 0.79569\n","36/36 [==============================] - 1s 31ms/step - loss: 0.2104 - accuracy: 0.9411 - val_loss: 0.8947 - val_accuracy: 0.7884\n","Epoch 66/1000\n","35/36 [============================>.] - ETA: 0s - loss: 0.2059 - accuracy: 0.9421\n","Epoch 66: val_accuracy did not improve from 0.79569\n","36/36 [==============================] - 1s 30ms/step - loss: 0.2061 - accuracy: 0.9420 - val_loss: 0.8877 - val_accuracy: 0.7915\n","Epoch 67/1000\n","36/36 [==============================] - ETA: 0s - loss: 0.2008 - accuracy: 0.9441\n","Epoch 67: val_accuracy did not improve from 0.79569\n","36/36 [==============================] - 1s 30ms/step - loss: 0.2008 - accuracy: 0.9441 - val_loss: 0.8951 - val_accuracy: 0.7886\n","Epoch 68/1000\n","35/36 [============================>.] - ETA: 0s - loss: 0.1978 - accuracy: 0.9448\n","Epoch 68: val_accuracy did not improve from 0.79569\n","36/36 [==============================] - 1s 30ms/step - loss: 0.1979 - accuracy: 0.9448 - val_loss: 0.9116 - val_accuracy: 0.7876\n","Epoch 69/1000\n","36/36 [==============================] - ETA: 0s - loss: 0.1939 - accuracy: 0.9462\n","Epoch 69: val_accuracy did not improve from 0.79569\n","36/36 [==============================] - 1s 31ms/step - loss: 0.1939 - accuracy: 0.9462 - val_loss: 0.9291 - val_accuracy: 0.7871\n","Epoch 70/1000\n","36/36 [==============================] - ETA: 0s - loss: 0.1938 - accuracy: 0.9458\n","Epoch 70: val_accuracy did not improve from 0.79569\n","36/36 [==============================] - 1s 30ms/step - loss: 0.1938 - accuracy: 0.9458 - val_loss: 0.9652 - val_accuracy: 0.7832\n","Epoch 71/1000\n","36/36 [==============================] - ETA: 0s - loss: 0.1883 - accuracy: 0.9476\n","Epoch 71: val_accuracy did not improve from 0.79569\n","36/36 [==============================] - 1s 31ms/step - loss: 0.1883 - accuracy: 0.9476 - val_loss: 0.9325 - val_accuracy: 0.7870\n","Epoch 72/1000\n","35/36 [============================>.] - ETA: 0s - loss: 0.1864 - accuracy: 0.9480\n","Epoch 72: val_accuracy did not improve from 0.79569\n","36/36 [==============================] - 1s 30ms/step - loss: 0.1863 - accuracy: 0.9481 - val_loss: 0.9637 - val_accuracy: 0.7834\n","Epoch 73/1000\n","34/36 [===========================>..] - ETA: 0s - loss: 0.1808 - accuracy: 0.9503\n","Epoch 73: val_accuracy did not improve from 0.79569\n","36/36 [==============================] - 1s 30ms/step - loss: 0.1815 - accuracy: 0.9500 - val_loss: 0.9522 - val_accuracy: 0.7869\n","Epoch 74/1000\n","34/36 [===========================>..] - ETA: 0s - loss: 0.1784 - accuracy: 0.9507\n","Epoch 74: val_accuracy did not improve from 0.79569\n","36/36 [==============================] - 1s 30ms/step - loss: 0.1788 - accuracy: 0.9506 - val_loss: 0.9753 - val_accuracy: 0.7841\n","Epoch 75/1000\n","36/36 [==============================] - ETA: 0s - loss: 0.1780 - accuracy: 0.9510\n","Epoch 75: val_accuracy did not improve from 0.79569\n","36/36 [==============================] - 1s 30ms/step - loss: 0.1780 - accuracy: 0.9510 - val_loss: 0.9725 - val_accuracy: 0.7853\n","Epoch 76/1000\n","35/36 [============================>.] - ETA: 0s - loss: 0.1730 - accuracy: 0.9529\n","Epoch 76: val_accuracy did not improve from 0.79569\n","36/36 [==============================] - 1s 30ms/step - loss: 0.1731 - accuracy: 0.9529 - val_loss: 0.9872 - val_accuracy: 0.7854\n","Epoch 77/1000\n","36/36 [==============================] - ETA: 0s - loss: 0.1700 - accuracy: 0.9538\n","Epoch 77: val_accuracy did not improve from 0.79569\n","36/36 [==============================] - 1s 30ms/step - loss: 0.1700 - accuracy: 0.9538 - val_loss: 0.9983 - val_accuracy: 0.7837\n","Epoch 78/1000\n","35/36 [============================>.] - ETA: 0s - loss: 0.1666 - accuracy: 0.9545\n","Epoch 78: val_accuracy did not improve from 0.79569\n","36/36 [==============================] - 1s 30ms/step - loss: 0.1669 - accuracy: 0.9544 - val_loss: 1.0028 - val_accuracy: 0.7842\n","Epoch 79/1000\n","35/36 [============================>.] - ETA: 0s - loss: 0.1642 - accuracy: 0.9554\n","Epoch 79: val_accuracy did not improve from 0.79569\n","36/36 [==============================] - 1s 31ms/step - loss: 0.1644 - accuracy: 0.9554 - val_loss: 1.0122 - val_accuracy: 0.7840\n","Epoch 80/1000\n","35/36 [============================>.] - ETA: 0s - loss: 0.1603 - accuracy: 0.9565\n","Epoch 80: val_accuracy did not improve from 0.79569\n","36/36 [==============================] - 1s 31ms/step - loss: 0.1603 - accuracy: 0.9564 - val_loss: 1.0213 - val_accuracy: 0.7835\n","Epoch 81/1000\n","36/36 [==============================] - ETA: 0s - loss: 0.1576 - accuracy: 0.9573\n","Epoch 81: val_accuracy did not improve from 0.79569\n","36/36 [==============================] - 1s 30ms/step - loss: 0.1576 - accuracy: 0.9573 - val_loss: 1.0070 - val_accuracy: 0.7851\n","Epoch 82/1000\n","35/36 [============================>.] - ETA: 0s - loss: 0.1548 - accuracy: 0.9582\n","Epoch 82: val_accuracy did not improve from 0.79569\n","36/36 [==============================] - 1s 30ms/step - loss: 0.1550 - accuracy: 0.9581 - val_loss: 1.0231 - val_accuracy: 0.7830\n","Epoch 83/1000\n","36/36 [==============================] - ETA: 0s - loss: 0.1529 - accuracy: 0.9584\n","Epoch 83: val_accuracy did not improve from 0.79569\n","36/36 [==============================] - 1s 31ms/step - loss: 0.1529 - accuracy: 0.9584 - val_loss: 1.0344 - val_accuracy: 0.7833\n","Epoch 84/1000\n","36/36 [==============================] - ETA: 0s - loss: 0.1490 - accuracy: 0.9602\n","Epoch 84: val_accuracy did not improve from 0.79569\n","36/36 [==============================] - 1s 30ms/step - loss: 0.1490 - accuracy: 0.9602 - val_loss: 1.0515 - val_accuracy: 0.7813\n","Epoch 85/1000\n","35/36 [============================>.] - ETA: 0s - loss: 0.1474 - accuracy: 0.9604\n","Epoch 85: val_accuracy did not improve from 0.79569\n","36/36 [==============================] - 1s 30ms/step - loss: 0.1475 - accuracy: 0.9604 - val_loss: 1.0484 - val_accuracy: 0.7843\n","Epoch 86/1000\n","35/36 [============================>.] - ETA: 0s - loss: 0.1453 - accuracy: 0.9608\n","Epoch 86: val_accuracy did not improve from 0.79569\n","36/36 [==============================] - 1s 31ms/step - loss: 0.1454 - accuracy: 0.9608 - val_loss: 1.0606 - val_accuracy: 0.7832\n","Epoch 87/1000\n","36/36 [==============================] - ETA: 0s - loss: 0.1434 - accuracy: 0.9613\n","Epoch 87: val_accuracy did not improve from 0.79569\n","36/36 [==============================] - 1s 30ms/step - loss: 0.1434 - accuracy: 0.9613 - val_loss: 1.0563 - val_accuracy: 0.7834\n","Epoch 88/1000\n","36/36 [==============================] - ETA: 0s - loss: 0.1423 - accuracy: 0.9616\n","Epoch 88: val_accuracy did not improve from 0.79569\n","36/36 [==============================] - 1s 30ms/step - loss: 0.1423 - accuracy: 0.9616 - val_loss: 1.0712 - val_accuracy: 0.7828\n","Epoch 89/1000\n","35/36 [============================>.] - ETA: 0s - loss: 0.1395 - accuracy: 0.9626\n","Epoch 89: val_accuracy did not improve from 0.79569\n","36/36 [==============================] - 1s 31ms/step - loss: 0.1396 - accuracy: 0.9625 - val_loss: 1.0740 - val_accuracy: 0.7825\n","Epoch 90/1000\n","36/36 [==============================] - ETA: 0s - loss: 0.1375 - accuracy: 0.9630\n","Epoch 90: val_accuracy did not improve from 0.79569\n","36/36 [==============================] - 1s 31ms/step - loss: 0.1375 - accuracy: 0.9630 - val_loss: 1.0980 - val_accuracy: 0.7832\n","Epoch 91/1000\n","34/36 [===========================>..] - ETA: 0s - loss: 0.1356 - accuracy: 0.9637\n","Epoch 91: val_accuracy did not improve from 0.79569\n","36/36 [==============================] - 1s 30ms/step - loss: 0.1357 - accuracy: 0.9636 - val_loss: 1.0795 - val_accuracy: 0.7818\n","Epoch 92/1000\n","35/36 [============================>.] - ETA: 0s - loss: 0.1340 - accuracy: 0.9640\n","Epoch 92: val_accuracy did not improve from 0.79569\n","36/36 [==============================] - 1s 31ms/step - loss: 0.1342 - accuracy: 0.9638 - val_loss: 1.1063 - val_accuracy: 0.7819\n","Epoch 93/1000\n","35/36 [============================>.] - ETA: 0s - loss: 0.1315 - accuracy: 0.9646\n","Epoch 93: val_accuracy did not improve from 0.79569\n","36/36 [==============================] - 1s 31ms/step - loss: 0.1316 - accuracy: 0.9645 - val_loss: 1.1200 - val_accuracy: 0.7809\n","Epoch 94/1000\n","35/36 [============================>.] - ETA: 0s - loss: 0.1300 - accuracy: 0.9652\n","Epoch 94: val_accuracy did not improve from 0.79569\n","36/36 [==============================] - 1s 31ms/step - loss: 0.1298 - accuracy: 0.9653 - val_loss: 1.1351 - val_accuracy: 0.7802\n","Epoch 95/1000\n","35/36 [============================>.] - ETA: 0s - loss: 0.1284 - accuracy: 0.9654\n","Epoch 95: val_accuracy did not improve from 0.79569\n","36/36 [==============================] - 1s 31ms/step - loss: 0.1284 - accuracy: 0.9654 - val_loss: 1.1216 - val_accuracy: 0.7821\n","Epoch 96/1000\n","36/36 [==============================] - ETA: 0s - loss: 0.1264 - accuracy: 0.9663\n","Epoch 96: val_accuracy did not improve from 0.79569\n","36/36 [==============================] - 1s 30ms/step - loss: 0.1264 - accuracy: 0.9663 - val_loss: 1.1260 - val_accuracy: 0.7814\n","Epoch 97/1000\n","35/36 [============================>.] - ETA: 0s - loss: 0.1252 - accuracy: 0.9664\n","Epoch 97: val_accuracy did not improve from 0.79569\n","36/36 [==============================] - 1s 31ms/step - loss: 0.1251 - accuracy: 0.9664 - val_loss: 1.1353 - val_accuracy: 0.7810\n","Epoch 98/1000\n","35/36 [============================>.] - ETA: 0s - loss: 0.1232 - accuracy: 0.9671\n","Epoch 98: val_accuracy did not improve from 0.79569\n","36/36 [==============================] - 1s 30ms/step - loss: 0.1234 - accuracy: 0.9671 - val_loss: 1.1397 - val_accuracy: 0.7814\n","Epoch 99/1000\n","35/36 [============================>.] - ETA: 0s - loss: 0.1214 - accuracy: 0.9677\n","Epoch 99: val_accuracy did not improve from 0.79569\n","36/36 [==============================] - 1s 31ms/step - loss: 0.1216 - accuracy: 0.9677 - val_loss: 1.1742 - val_accuracy: 0.7798\n","Epoch 100/1000\n","35/36 [============================>.] - ETA: 0s - loss: 0.1197 - accuracy: 0.9681\n","Epoch 100: val_accuracy did not improve from 0.79569\n","36/36 [==============================] - 1s 32ms/step - loss: 0.1198 - accuracy: 0.9680 - val_loss: 1.1648 - val_accuracy: 0.7809\n","Epoch 101/1000\n","35/36 [============================>.] - ETA: 0s - loss: 0.1185 - accuracy: 0.9683\n","Epoch 101: val_accuracy did not improve from 0.79569\n","36/36 [==============================] - 1s 31ms/step - loss: 0.1186 - accuracy: 0.9682 - val_loss: 1.1723 - val_accuracy: 0.7793\n","Epoch 102/1000\n","36/36 [==============================] - ETA: 0s - loss: 0.1175 - accuracy: 0.9686\n","Epoch 102: val_accuracy did not improve from 0.79569\n","36/36 [==============================] - 1s 31ms/step - loss: 0.1175 - accuracy: 0.9686 - val_loss: 1.1904 - val_accuracy: 0.7797\n","Epoch 103/1000\n","35/36 [============================>.] - ETA: 0s - loss: 0.1152 - accuracy: 0.9694\n","Epoch 103: val_accuracy did not improve from 0.79569\n","36/36 [==============================] - 1s 31ms/step - loss: 0.1153 - accuracy: 0.9693 - val_loss: 1.1805 - val_accuracy: 0.7799\n","Epoch 104/1000\n","35/36 [============================>.] - ETA: 0s - loss: 0.1146 - accuracy: 0.9692\n","Epoch 104: val_accuracy did not improve from 0.79569\n","36/36 [==============================] - 1s 31ms/step - loss: 0.1144 - accuracy: 0.9692 - val_loss: 1.1891 - val_accuracy: 0.7802\n","Epoch 105/1000\n","35/36 [============================>.] - ETA: 0s - loss: 0.1129 - accuracy: 0.9699\n","Epoch 105: val_accuracy did not improve from 0.79569\n","36/36 [==============================] - 1s 31ms/step - loss: 0.1130 - accuracy: 0.9699 - val_loss: 1.2092 - val_accuracy: 0.7791\n","Epoch 106/1000\n","35/36 [============================>.] - ETA: 0s - loss: 0.1115 - accuracy: 0.9702\n","Epoch 106: val_accuracy did not improve from 0.79569\n","36/36 [==============================] - 1s 31ms/step - loss: 0.1114 - accuracy: 0.9703 - val_loss: 1.2093 - val_accuracy: 0.7792\n","Epoch 107/1000\n","35/36 [============================>.] - ETA: 0s - loss: 0.1110 - accuracy: 0.9704\n","Epoch 107: val_accuracy did not improve from 0.79569\n","36/36 [==============================] - 1s 31ms/step - loss: 0.1110 - accuracy: 0.9704 - val_loss: 1.1977 - val_accuracy: 0.7816\n","Epoch 108/1000\n","35/36 [============================>.] - ETA: 0s - loss: 0.1105 - accuracy: 0.9704\n","Epoch 108: val_accuracy did not improve from 0.79569\n","36/36 [==============================] - 1s 31ms/step - loss: 0.1105 - accuracy: 0.9705 - val_loss: 1.2143 - val_accuracy: 0.7803\n","Epoch 109/1000\n","35/36 [============================>.] - ETA: 0s - loss: 0.1076 - accuracy: 0.9713\n","Epoch 109: val_accuracy did not improve from 0.79569\n","36/36 [==============================] - 1s 31ms/step - loss: 0.1076 - accuracy: 0.9713 - val_loss: 1.2035 - val_accuracy: 0.7809\n","Epoch 110/1000\n","35/36 [============================>.] - ETA: 0s - loss: 0.1065 - accuracy: 0.9717\n","Epoch 110: val_accuracy did not improve from 0.79569\n","36/36 [==============================] - 1s 31ms/step - loss: 0.1065 - accuracy: 0.9716 - val_loss: 1.2347 - val_accuracy: 0.7795\n","Epoch 111/1000\n","35/36 [============================>.] - ETA: 0s - loss: 0.1074 - accuracy: 0.9712\n","Epoch 111: val_accuracy did not improve from 0.79569\n","36/36 [==============================] - 1s 31ms/step - loss: 0.1074 - accuracy: 0.9712 - val_loss: 1.2444 - val_accuracy: 0.7789\n","Epoch 112/1000\n","35/36 [============================>.] - ETA: 0s - loss: 0.1057 - accuracy: 0.9716\n","Epoch 112: val_accuracy did not improve from 0.79569\n","36/36 [==============================] - 1s 31ms/step - loss: 0.1059 - accuracy: 0.9715 - val_loss: 1.2344 - val_accuracy: 0.7791\n","Epoch 113/1000\n","35/36 [============================>.] - ETA: 0s - loss: 0.1064 - accuracy: 0.9712\n","Epoch 113: val_accuracy did not improve from 0.79569\n","36/36 [==============================] - 1s 31ms/step - loss: 0.1068 - accuracy: 0.9711 - val_loss: 1.2448 - val_accuracy: 0.7791\n","Epoch 114/1000\n","35/36 [============================>.] - ETA: 0s - loss: 0.1094 - accuracy: 0.9698\n","Epoch 114: val_accuracy did not improve from 0.79569\n","36/36 [==============================] - 1s 31ms/step - loss: 0.1094 - accuracy: 0.9699 - val_loss: 1.2450 - val_accuracy: 0.7791\n","Epoch 115/1000\n","35/36 [============================>.] - ETA: 0s - loss: 0.1074 - accuracy: 0.9709\n","Epoch 115: val_accuracy did not improve from 0.79569\n","36/36 [==============================] - 1s 31ms/step - loss: 0.1077 - accuracy: 0.9708 - val_loss: 1.2604 - val_accuracy: 0.7789\n","Epoch 116/1000\n","35/36 [============================>.] - ETA: 0s - loss: 0.1086 - accuracy: 0.9703\n","Epoch 116: val_accuracy did not improve from 0.79569\n","36/36 [==============================] - 1s 35ms/step - loss: 0.1088 - accuracy: 0.9702 - val_loss: 1.2597 - val_accuracy: 0.7786\n","Epoch 117/1000\n","35/36 [============================>.] - ETA: 0s - loss: 0.1061 - accuracy: 0.9709\n","Epoch 117: val_accuracy did not improve from 0.79569\n","36/36 [==============================] - 1s 38ms/step - loss: 0.1061 - accuracy: 0.9710 - val_loss: 1.2616 - val_accuracy: 0.7794\n","Epoch 118/1000\n","35/36 [============================>.] - ETA: 0s - loss: 0.1096 - accuracy: 0.9698\n","Epoch 118: val_accuracy did not improve from 0.79569\n","36/36 [==============================] - 1s 37ms/step - loss: 0.1097 - accuracy: 0.9698 - val_loss: 1.2612 - val_accuracy: 0.7797\n","Epoch 119/1000\n","35/36 [============================>.] - ETA: 0s - loss: 0.0984 - accuracy: 0.9733\n","Epoch 119: val_accuracy did not improve from 0.79569\n","36/36 [==============================] - 1s 34ms/step - loss: 0.0984 - accuracy: 0.9734 - val_loss: 1.2709 - val_accuracy: 0.7802\n","Epoch 120/1000\n","36/36 [==============================] - ETA: 0s - loss: 0.0959 - accuracy: 0.9739\n","Epoch 120: val_accuracy did not improve from 0.79569\n","36/36 [==============================] - 1s 31ms/step - loss: 0.0959 - accuracy: 0.9739 - val_loss: 1.2872 - val_accuracy: 0.7803\n","Epoch 121/1000\n","35/36 [============================>.] - ETA: 0s - loss: 0.0986 - accuracy: 0.9732\n","Epoch 121: val_accuracy did not improve from 0.79569\n","36/36 [==============================] - 1s 31ms/step - loss: 0.0986 - accuracy: 0.9732 - val_loss: 1.2788 - val_accuracy: 0.7795\n","Epoch 122/1000\n","35/36 [============================>.] - ETA: 0s - loss: 0.0990 - accuracy: 0.9725\n","Epoch 122: val_accuracy did not improve from 0.79569\n","36/36 [==============================] - 1s 31ms/step - loss: 0.0990 - accuracy: 0.9726 - val_loss: 1.3047 - val_accuracy: 0.7782\n","Epoch 123/1000\n","35/36 [============================>.] - ETA: 0s - loss: 0.0980 - accuracy: 0.9731\n","Epoch 123: val_accuracy did not improve from 0.79569\n","36/36 [==============================] - 1s 31ms/step - loss: 0.0979 - accuracy: 0.9731 - val_loss: 1.3027 - val_accuracy: 0.7787\n","Epoch 124/1000\n","35/36 [============================>.] - ETA: 0s - loss: 0.0968 - accuracy: 0.9734\n","Epoch 124: val_accuracy did not improve from 0.79569\n","36/36 [==============================] - 1s 31ms/step - loss: 0.0970 - accuracy: 0.9734 - val_loss: 1.2991 - val_accuracy: 0.7781\n","Epoch 125/1000\n","35/36 [============================>.] - ETA: 0s - loss: 0.0958 - accuracy: 0.9734\n","Epoch 125: val_accuracy did not improve from 0.79569\n","36/36 [==============================] - 1s 32ms/step - loss: 0.0959 - accuracy: 0.9734 - val_loss: 1.3212 - val_accuracy: 0.7778\n","Epoch 126/1000\n","35/36 [============================>.] - ETA: 0s - loss: 0.0952 - accuracy: 0.9736\n","Epoch 126: val_accuracy did not improve from 0.79569\n","36/36 [==============================] - 1s 31ms/step - loss: 0.0950 - accuracy: 0.9737 - val_loss: 1.3045 - val_accuracy: 0.7786\n","Epoch 127/1000\n","35/36 [============================>.] - ETA: 0s - loss: 0.0945 - accuracy: 0.9738\n","Epoch 127: val_accuracy did not improve from 0.79569\n","36/36 [==============================] - 1s 31ms/step - loss: 0.0946 - accuracy: 0.9737 - val_loss: 1.3249 - val_accuracy: 0.7778\n","Epoch 128/1000\n","35/36 [============================>.] - ETA: 0s - loss: 0.0937 - accuracy: 0.9742\n","Epoch 128: val_accuracy did not improve from 0.79569\n","36/36 [==============================] - 1s 32ms/step - loss: 0.0937 - accuracy: 0.9742 - val_loss: 1.3301 - val_accuracy: 0.7786\n","Epoch 129/1000\n","36/36 [==============================] - ETA: 0s - loss: 0.0926 - accuracy: 0.9742\n","Epoch 129: val_accuracy did not improve from 0.79569\n","36/36 [==============================] - 1s 31ms/step - loss: 0.0926 - accuracy: 0.9742 - val_loss: 1.3246 - val_accuracy: 0.7796\n","Epoch 130/1000\n","35/36 [============================>.] - ETA: 0s - loss: 0.0914 - accuracy: 0.9747\n","Epoch 130: val_accuracy did not improve from 0.79569\n","36/36 [==============================] - 1s 31ms/step - loss: 0.0914 - accuracy: 0.9746 - val_loss: 1.3471 - val_accuracy: 0.7793\n","Epoch 131/1000\n","35/36 [============================>.] - ETA: 0s - loss: 0.0909 - accuracy: 0.9747\n","Epoch 131: val_accuracy did not improve from 0.79569\n","36/36 [==============================] - 1s 32ms/step - loss: 0.0910 - accuracy: 0.9747 - val_loss: 1.3419 - val_accuracy: 0.7799\n","Epoch 132/1000\n","35/36 [============================>.] - ETA: 0s - loss: 0.0897 - accuracy: 0.9752\n","Epoch 132: val_accuracy did not improve from 0.79569\n","36/36 [==============================] - 1s 32ms/step - loss: 0.0897 - accuracy: 0.9752 - val_loss: 1.3522 - val_accuracy: 0.7783\n","Epoch 133/1000\n","35/36 [============================>.] - ETA: 0s - loss: 0.0909 - accuracy: 0.9746\n","Epoch 133: val_accuracy did not improve from 0.79569\n","36/36 [==============================] - 1s 32ms/step - loss: 0.0909 - accuracy: 0.9746 - val_loss: 1.3524 - val_accuracy: 0.7782\n","Epoch 134/1000\n","35/36 [============================>.] - ETA: 0s - loss: 0.0896 - accuracy: 0.9747\n","Epoch 134: val_accuracy did not improve from 0.79569\n","36/36 [==============================] - 1s 32ms/step - loss: 0.0896 - accuracy: 0.9747 - val_loss: 1.3581 - val_accuracy: 0.7780\n","Epoch 135/1000\n","35/36 [============================>.] - ETA: 0s - loss: 0.0901 - accuracy: 0.9748\n","Epoch 135: val_accuracy did not improve from 0.79569\n","36/36 [==============================] - 1s 32ms/step - loss: 0.0901 - accuracy: 0.9748 - val_loss: 1.3416 - val_accuracy: 0.7795\n","Epoch 136/1000\n","35/36 [============================>.] - ETA: 0s - loss: 0.0890 - accuracy: 0.9752\n","Epoch 136: val_accuracy did not improve from 0.79569\n","36/36 [==============================] - 1s 32ms/step - loss: 0.0890 - accuracy: 0.9752 - val_loss: 1.3562 - val_accuracy: 0.7794\n","Epoch 137/1000\n","35/36 [============================>.] - ETA: 0s - loss: 0.0882 - accuracy: 0.9753\n","Epoch 137: val_accuracy did not improve from 0.79569\n","36/36 [==============================] - 1s 31ms/step - loss: 0.0882 - accuracy: 0.9753 - val_loss: 1.3640 - val_accuracy: 0.7773\n","Epoch 138/1000\n","35/36 [============================>.] - ETA: 0s - loss: 0.0867 - accuracy: 0.9756\n","Epoch 138: val_accuracy did not improve from 0.79569\n","36/36 [==============================] - 1s 32ms/step - loss: 0.0867 - accuracy: 0.9755 - val_loss: 1.3857 - val_accuracy: 0.7776\n","Epoch 139/1000\n","35/36 [============================>.] - ETA: 0s - loss: 0.0871 - accuracy: 0.9755\n","Epoch 139: val_accuracy did not improve from 0.79569\n","36/36 [==============================] - 1s 32ms/step - loss: 0.0872 - accuracy: 0.9755 - val_loss: 1.3744 - val_accuracy: 0.7785\n","Epoch 140/1000\n","35/36 [============================>.] - ETA: 0s - loss: 0.0891 - accuracy: 0.9747\n","Epoch 140: val_accuracy did not improve from 0.79569\n","36/36 [==============================] - 1s 32ms/step - loss: 0.0893 - accuracy: 0.9747 - val_loss: 1.3783 - val_accuracy: 0.7785\n","Epoch 141/1000\n","35/36 [============================>.] - ETA: 0s - loss: 0.0891 - accuracy: 0.9748\n","Epoch 141: val_accuracy did not improve from 0.79569\n","36/36 [==============================] - 1s 31ms/step - loss: 0.0892 - accuracy: 0.9747 - val_loss: 1.3792 - val_accuracy: 0.7791\n","Epoch 142/1000\n","35/36 [============================>.] - ETA: 0s - loss: 0.0876 - accuracy: 0.9748\n","Epoch 142: val_accuracy did not improve from 0.79569\n","36/36 [==============================] - 1s 32ms/step - loss: 0.0876 - accuracy: 0.9748 - val_loss: 1.3986 - val_accuracy: 0.7786\n","Epoch 143/1000\n","35/36 [============================>.] - ETA: 0s - loss: 0.0871 - accuracy: 0.9752\n","Epoch 143: val_accuracy did not improve from 0.79569\n","36/36 [==============================] - 1s 31ms/step - loss: 0.0872 - accuracy: 0.9752 - val_loss: 1.3869 - val_accuracy: 0.7792\n","Epoch 144/1000\n","35/36 [============================>.] - ETA: 0s - loss: 0.0862 - accuracy: 0.9752\n","Epoch 144: val_accuracy did not improve from 0.79569\n","36/36 [==============================] - 1s 32ms/step - loss: 0.0861 - accuracy: 0.9752 - val_loss: 1.4027 - val_accuracy: 0.7791\n","Epoch 145/1000\n","35/36 [============================>.] - ETA: 0s - loss: 0.0850 - accuracy: 0.9758\n","Epoch 145: val_accuracy did not improve from 0.79569\n","36/36 [==============================] - 1s 32ms/step - loss: 0.0851 - accuracy: 0.9757 - val_loss: 1.4030 - val_accuracy: 0.7770\n","Epoch 146/1000\n","35/36 [============================>.] - ETA: 0s - loss: 0.0845 - accuracy: 0.9757\n","Epoch 146: val_accuracy did not improve from 0.79569\n","36/36 [==============================] - 1s 31ms/step - loss: 0.0845 - accuracy: 0.9757 - val_loss: 1.4201 - val_accuracy: 0.7785\n","Epoch 147/1000\n","35/36 [============================>.] - ETA: 0s - loss: 0.0861 - accuracy: 0.9752\n","Epoch 147: val_accuracy did not improve from 0.79569\n","36/36 [==============================] - 1s 32ms/step - loss: 0.0861 - accuracy: 0.9752 - val_loss: 1.4279 - val_accuracy: 0.7781\n","Epoch 148/1000\n","35/36 [============================>.] - ETA: 0s - loss: 0.0859 - accuracy: 0.9750\n","Epoch 148: val_accuracy did not improve from 0.79569\n","36/36 [==============================] - 1s 32ms/step - loss: 0.0861 - accuracy: 0.9749 - val_loss: 1.4043 - val_accuracy: 0.7789\n","Epoch 149/1000\n","35/36 [============================>.] - ETA: 0s - loss: 0.0848 - accuracy: 0.9752\n","Epoch 149: val_accuracy did not improve from 0.79569\n","36/36 [==============================] - 1s 32ms/step - loss: 0.0848 - accuracy: 0.9752 - val_loss: 1.4368 - val_accuracy: 0.7777\n","Epoch 150/1000\n","35/36 [============================>.] - ETA: 0s - loss: 0.0838 - accuracy: 0.9756\n","Epoch 150: val_accuracy did not improve from 0.79569\n","36/36 [==============================] - 1s 33ms/step - loss: 0.0839 - accuracy: 0.9756 - val_loss: 1.4190 - val_accuracy: 0.7792\n","Epoch 151/1000\n","35/36 [============================>.] - ETA: 0s - loss: 0.0834 - accuracy: 0.9758\n","Epoch 151: val_accuracy did not improve from 0.79569\n","36/36 [==============================] - 1s 32ms/step - loss: 0.0834 - accuracy: 0.9757 - val_loss: 1.4049 - val_accuracy: 0.7792\n","Epoch 152/1000\n","35/36 [============================>.] - ETA: 0s - loss: 0.0823 - accuracy: 0.9762\n","Epoch 152: val_accuracy did not improve from 0.79569\n","36/36 [==============================] - 1s 33ms/step - loss: 0.0824 - accuracy: 0.9761 - val_loss: 1.4223 - val_accuracy: 0.7782\n","Epoch 153/1000\n","35/36 [============================>.] - ETA: 0s - loss: 0.0826 - accuracy: 0.9759\n","Epoch 153: val_accuracy did not improve from 0.79569\n","36/36 [==============================] - 1s 32ms/step - loss: 0.0825 - accuracy: 0.9759 - val_loss: 1.4192 - val_accuracy: 0.7790\n","Epoch 154/1000\n","35/36 [============================>.] - ETA: 0s - loss: 0.0817 - accuracy: 0.9761\n","Epoch 154: val_accuracy did not improve from 0.79569\n","36/36 [==============================] - 1s 32ms/step - loss: 0.0817 - accuracy: 0.9760 - val_loss: 1.4479 - val_accuracy: 0.7775\n","Epoch 155/1000\n","35/36 [============================>.] - ETA: 0s - loss: 0.0809 - accuracy: 0.9764\n","Epoch 155: val_accuracy did not improve from 0.79569\n","36/36 [==============================] - 1s 32ms/step - loss: 0.0809 - accuracy: 0.9764 - val_loss: 1.4395 - val_accuracy: 0.7796\n","Epoch 156/1000\n","35/36 [============================>.] - ETA: 0s - loss: 0.0807 - accuracy: 0.9762\n","Epoch 156: val_accuracy did not improve from 0.79569\n","36/36 [==============================] - 1s 32ms/step - loss: 0.0809 - accuracy: 0.9761 - val_loss: 1.4612 - val_accuracy: 0.7768\n","Epoch 157/1000\n","35/36 [============================>.] - ETA: 0s - loss: 0.0800 - accuracy: 0.9769\n","Epoch 157: val_accuracy did not improve from 0.79569\n","36/36 [==============================] - 1s 32ms/step - loss: 0.0800 - accuracy: 0.9768 - val_loss: 1.4348 - val_accuracy: 0.7798\n","Epoch 158/1000\n","35/36 [============================>.] - ETA: 0s - loss: 0.0793 - accuracy: 0.9767\n","Epoch 158: val_accuracy did not improve from 0.79569\n","36/36 [==============================] - 1s 32ms/step - loss: 0.0793 - accuracy: 0.9767 - val_loss: 1.4164 - val_accuracy: 0.7807\n","Epoch 159/1000\n","35/36 [============================>.] - ETA: 0s - loss: 0.0792 - accuracy: 0.9766\n","Epoch 159: val_accuracy did not improve from 0.79569\n","36/36 [==============================] - 1s 32ms/step - loss: 0.0793 - accuracy: 0.9766 - val_loss: 1.4510 - val_accuracy: 0.7780\n","Epoch 160/1000\n","35/36 [============================>.] - ETA: 0s - loss: 0.0785 - accuracy: 0.9771\n","Epoch 160: val_accuracy did not improve from 0.79569\n","36/36 [==============================] - 1s 33ms/step - loss: 0.0786 - accuracy: 0.9770 - val_loss: 1.4415 - val_accuracy: 0.7779\n","Epoch 161/1000\n","35/36 [============================>.] - ETA: 0s - loss: 0.0781 - accuracy: 0.9770\n","Epoch 161: val_accuracy did not improve from 0.79569\n","36/36 [==============================] - 1s 32ms/step - loss: 0.0781 - accuracy: 0.9770 - val_loss: 1.4529 - val_accuracy: 0.7797\n","Epoch 162/1000\n","35/36 [============================>.] - ETA: 0s - loss: 0.0776 - accuracy: 0.9770\n","Epoch 162: val_accuracy did not improve from 0.79569\n","36/36 [==============================] - 1s 32ms/step - loss: 0.0777 - accuracy: 0.9770 - val_loss: 1.4702 - val_accuracy: 0.7781\n","Epoch 163/1000\n","35/36 [============================>.] - ETA: 0s - loss: 0.0771 - accuracy: 0.9772\n","Epoch 163: val_accuracy did not improve from 0.79569\n","36/36 [==============================] - 1s 32ms/step - loss: 0.0772 - accuracy: 0.9772 - val_loss: 1.4595 - val_accuracy: 0.7788\n","Epoch 164/1000\n","35/36 [============================>.] - ETA: 0s - loss: 0.0768 - accuracy: 0.9774\n","Epoch 164: val_accuracy did not improve from 0.79569\n","36/36 [==============================] - 1s 33ms/step - loss: 0.0770 - accuracy: 0.9773 - val_loss: 1.4934 - val_accuracy: 0.7777\n","Epoch 165/1000\n","35/36 [============================>.] - ETA: 0s - loss: 0.0765 - accuracy: 0.9774\n","Epoch 165: val_accuracy did not improve from 0.79569\n","36/36 [==============================] - 1s 33ms/step - loss: 0.0766 - accuracy: 0.9773 - val_loss: 1.4667 - val_accuracy: 0.7794\n","Epoch 166/1000\n","35/36 [============================>.] - ETA: 0s - loss: 0.0758 - accuracy: 0.9776\n","Epoch 166: val_accuracy did not improve from 0.79569\n","36/36 [==============================] - 1s 32ms/step - loss: 0.0759 - accuracy: 0.9776 - val_loss: 1.4749 - val_accuracy: 0.7781\n","Epoch 167/1000\n","35/36 [============================>.] - ETA: 0s - loss: 0.0755 - accuracy: 0.9776\n","Epoch 167: val_accuracy did not improve from 0.79569\n","36/36 [==============================] - 1s 32ms/step - loss: 0.0756 - accuracy: 0.9775 - val_loss: 1.4778 - val_accuracy: 0.7802\n","Epoch 168/1000\n","35/36 [============================>.] - ETA: 0s - loss: 0.0754 - accuracy: 0.9776\n","Epoch 168: val_accuracy did not improve from 0.79569\n","36/36 [==============================] - 1s 32ms/step - loss: 0.0754 - accuracy: 0.9775 - val_loss: 1.4903 - val_accuracy: 0.7803\n","Epoch 169/1000\n","35/36 [============================>.] - ETA: 0s - loss: 0.0752 - accuracy: 0.9775\n","Epoch 169: val_accuracy did not improve from 0.79569\n","36/36 [==============================] - 1s 32ms/step - loss: 0.0752 - accuracy: 0.9774 - val_loss: 1.4943 - val_accuracy: 0.7778\n","Epoch 170/1000\n","35/36 [============================>.] - ETA: 0s - loss: 0.0743 - accuracy: 0.9779\n","Epoch 170: val_accuracy did not improve from 0.79569\n","36/36 [==============================] - 1s 32ms/step - loss: 0.0743 - accuracy: 0.9779 - val_loss: 1.5024 - val_accuracy: 0.7789\n","Epoch 171/1000\n","35/36 [============================>.] - ETA: 0s - loss: 0.0743 - accuracy: 0.9776\n","Epoch 171: val_accuracy did not improve from 0.79569\n","36/36 [==============================] - 1s 32ms/step - loss: 0.0743 - accuracy: 0.9776 - val_loss: 1.4933 - val_accuracy: 0.7788\n","Epoch 172/1000\n","35/36 [============================>.] - ETA: 0s - loss: 0.0741 - accuracy: 0.9779\n","Epoch 172: val_accuracy did not improve from 0.79569\n","36/36 [==============================] - 1s 32ms/step - loss: 0.0741 - accuracy: 0.9778 - val_loss: 1.4990 - val_accuracy: 0.7798\n","Epoch 173/1000\n","35/36 [============================>.] - ETA: 0s - loss: 0.0732 - accuracy: 0.9780\n","Epoch 173: val_accuracy did not improve from 0.79569\n","36/36 [==============================] - 1s 33ms/step - loss: 0.0732 - accuracy: 0.9780 - val_loss: 1.4911 - val_accuracy: 0.7792\n","Epoch 174/1000\n","35/36 [============================>.] - ETA: 0s - loss: 0.0732 - accuracy: 0.9782\n","Epoch 174: val_accuracy did not improve from 0.79569\n","36/36 [==============================] - 1s 33ms/step - loss: 0.0732 - accuracy: 0.9782 - val_loss: 1.5046 - val_accuracy: 0.7802\n","Epoch 175/1000\n","35/36 [============================>.] - ETA: 0s - loss: 0.0725 - accuracy: 0.9782\n","Epoch 175: val_accuracy did not improve from 0.79569\n","36/36 [==============================] - 1s 32ms/step - loss: 0.0725 - accuracy: 0.9782 - val_loss: 1.5000 - val_accuracy: 0.7799\n","Epoch 176/1000\n","35/36 [============================>.] - ETA: 0s - loss: 0.0725 - accuracy: 0.9783\n","Epoch 176: val_accuracy did not improve from 0.79569\n","36/36 [==============================] - 1s 33ms/step - loss: 0.0725 - accuracy: 0.9783 - val_loss: 1.5089 - val_accuracy: 0.7787\n","Epoch 177/1000\n","35/36 [============================>.] - ETA: 0s - loss: 0.0722 - accuracy: 0.9784\n","Epoch 177: val_accuracy did not improve from 0.79569\n","36/36 [==============================] - 1s 32ms/step - loss: 0.0722 - accuracy: 0.9784 - val_loss: 1.5024 - val_accuracy: 0.7793\n","Epoch 178/1000\n","35/36 [============================>.] - ETA: 0s - loss: 0.0711 - accuracy: 0.9788\n","Epoch 178: val_accuracy did not improve from 0.79569\n","36/36 [==============================] - 1s 32ms/step - loss: 0.0712 - accuracy: 0.9787 - val_loss: 1.5136 - val_accuracy: 0.7802\n","Epoch 179/1000\n","35/36 [============================>.] - ETA: 0s - loss: 0.0714 - accuracy: 0.9786\n","Epoch 179: val_accuracy did not improve from 0.79569\n","36/36 [==============================] - 1s 32ms/step - loss: 0.0714 - accuracy: 0.9786 - val_loss: 1.5250 - val_accuracy: 0.7793\n","Epoch 180/1000\n","35/36 [============================>.] - ETA: 0s - loss: 0.0718 - accuracy: 0.9785\n","Epoch 180: val_accuracy did not improve from 0.79569\n","36/36 [==============================] - 1s 32ms/step - loss: 0.0718 - accuracy: 0.9784 - val_loss: 1.5384 - val_accuracy: 0.7786\n","Epoch 181/1000\n","35/36 [============================>.] - ETA: 0s - loss: 0.0713 - accuracy: 0.9786\n","Epoch 181: val_accuracy did not improve from 0.79569\n","36/36 [==============================] - 1s 33ms/step - loss: 0.0713 - accuracy: 0.9786 - val_loss: 1.5199 - val_accuracy: 0.7791\n","Epoch 182/1000\n","35/36 [============================>.] - ETA: 0s - loss: 0.0701 - accuracy: 0.9788\n","Epoch 182: val_accuracy did not improve from 0.79569\n","36/36 [==============================] - 1s 33ms/step - loss: 0.0702 - accuracy: 0.9788 - val_loss: 1.5142 - val_accuracy: 0.7806\n","Epoch 183/1000\n","35/36 [============================>.] - ETA: 0s - loss: 0.0734 - accuracy: 0.9786\n","Epoch 183: val_accuracy did not improve from 0.79569\n","36/36 [==============================] - 1s 33ms/step - loss: 0.0734 - accuracy: 0.9786 - val_loss: 1.5095 - val_accuracy: 0.7795\n","Epoch 184/1000\n","35/36 [============================>.] - ETA: 0s - loss: 0.0699 - accuracy: 0.9790\n","Epoch 184: val_accuracy did not improve from 0.79569\n","36/36 [==============================] - 1s 32ms/step - loss: 0.0699 - accuracy: 0.9790 - val_loss: 1.5289 - val_accuracy: 0.7791\n","Epoch 185/1000\n","35/36 [============================>.] - ETA: 0s - loss: 0.0693 - accuracy: 0.9790\n","Epoch 185: val_accuracy did not improve from 0.79569\n","36/36 [==============================] - 1s 32ms/step - loss: 0.0693 - accuracy: 0.9790 - val_loss: 1.5241 - val_accuracy: 0.7802\n","Epoch 186/1000\n","35/36 [============================>.] - ETA: 0s - loss: 0.0697 - accuracy: 0.9790\n","Epoch 186: val_accuracy did not improve from 0.79569\n","36/36 [==============================] - 1s 32ms/step - loss: 0.0697 - accuracy: 0.9789 - val_loss: 1.5389 - val_accuracy: 0.7791\n","Epoch 187/1000\n","35/36 [============================>.] - ETA: 0s - loss: 0.0699 - accuracy: 0.9789\n","Epoch 187: val_accuracy did not improve from 0.79569\n","36/36 [==============================] - 1s 32ms/step - loss: 0.0700 - accuracy: 0.9788 - val_loss: 1.5449 - val_accuracy: 0.7792\n","Epoch 188/1000\n","35/36 [============================>.] - ETA: 0s - loss: 0.0694 - accuracy: 0.9790\n","Epoch 188: val_accuracy did not improve from 0.79569\n","36/36 [==============================] - 1s 33ms/step - loss: 0.0694 - accuracy: 0.9790 - val_loss: 1.5487 - val_accuracy: 0.7794\n","Epoch 189/1000\n","35/36 [============================>.] - ETA: 0s - loss: 0.0687 - accuracy: 0.9791\n","Epoch 189: val_accuracy did not improve from 0.79569\n","36/36 [==============================] - 1s 33ms/step - loss: 0.0687 - accuracy: 0.9790 - val_loss: 1.5349 - val_accuracy: 0.7811\n","Epoch 190/1000\n","35/36 [============================>.] - ETA: 0s - loss: 0.0684 - accuracy: 0.9791\n","Epoch 190: val_accuracy did not improve from 0.79569\n","36/36 [==============================] - 1s 32ms/step - loss: 0.0683 - accuracy: 0.9791 - val_loss: 1.5512 - val_accuracy: 0.7806\n","Epoch 191/1000\n","35/36 [============================>.] - ETA: 0s - loss: 0.0678 - accuracy: 0.9793\n","Epoch 191: val_accuracy did not improve from 0.79569\n","36/36 [==============================] - 1s 32ms/step - loss: 0.0678 - accuracy: 0.9793 - val_loss: 1.5698 - val_accuracy: 0.7778\n","Epoch 192/1000\n","35/36 [============================>.] - ETA: 0s - loss: 0.0682 - accuracy: 0.9792\n","Epoch 192: val_accuracy did not improve from 0.79569\n","36/36 [==============================] - 1s 32ms/step - loss: 0.0682 - accuracy: 0.9793 - val_loss: 1.5569 - val_accuracy: 0.7793\n","Epoch 193/1000\n","35/36 [============================>.] - ETA: 0s - loss: 0.0684 - accuracy: 0.9792\n","Epoch 193: val_accuracy did not improve from 0.79569\n","36/36 [==============================] - 1s 32ms/step - loss: 0.0684 - accuracy: 0.9792 - val_loss: 1.5606 - val_accuracy: 0.7785\n","Epoch 194/1000\n","35/36 [============================>.] - ETA: 0s - loss: 0.0676 - accuracy: 0.9794\n","Epoch 194: val_accuracy did not improve from 0.79569\n","36/36 [==============================] - 1s 33ms/step - loss: 0.0677 - accuracy: 0.9794 - val_loss: 1.5676 - val_accuracy: 0.7790\n","Epoch 195/1000\n","35/36 [============================>.] - ETA: 0s - loss: 0.0677 - accuracy: 0.9794\n","Epoch 195: val_accuracy did not improve from 0.79569\n","36/36 [==============================] - 1s 33ms/step - loss: 0.0677 - accuracy: 0.9794 - val_loss: 1.5697 - val_accuracy: 0.7779\n","Epoch 196/1000\n","35/36 [============================>.] - ETA: 0s - loss: 0.0671 - accuracy: 0.9795\n","Epoch 196: val_accuracy did not improve from 0.79569\n","36/36 [==============================] - 1s 32ms/step - loss: 0.0671 - accuracy: 0.9794 - val_loss: 1.5574 - val_accuracy: 0.7800\n","Epoch 197/1000\n","35/36 [============================>.] - ETA: 0s - loss: 0.0670 - accuracy: 0.9795\n","Epoch 197: val_accuracy did not improve from 0.79569\n","36/36 [==============================] - 1s 33ms/step - loss: 0.0672 - accuracy: 0.9794 - val_loss: 1.5795 - val_accuracy: 0.7785\n","Epoch 198/1000\n","35/36 [============================>.] - ETA: 0s - loss: 0.0665 - accuracy: 0.9797\n","Epoch 198: val_accuracy did not improve from 0.79569\n","36/36 [==============================] - 1s 32ms/step - loss: 0.0665 - accuracy: 0.9797 - val_loss: 1.5870 - val_accuracy: 0.7766\n","Epoch 199/1000\n","35/36 [============================>.] - ETA: 0s - loss: 0.0664 - accuracy: 0.9798\n","Epoch 199: val_accuracy did not improve from 0.79569\n","36/36 [==============================] - 1s 33ms/step - loss: 0.0664 - accuracy: 0.9797 - val_loss: 1.5921 - val_accuracy: 0.7778\n","Epoch 200/1000\n","35/36 [============================>.] - ETA: 0s - loss: 0.0663 - accuracy: 0.9796\n","Epoch 200: val_accuracy did not improve from 0.79569\n","36/36 [==============================] - 1s 33ms/step - loss: 0.0663 - accuracy: 0.9797 - val_loss: 1.5830 - val_accuracy: 0.7793\n","Epoch 201/1000\n","35/36 [============================>.] - ETA: 0s - loss: 0.0656 - accuracy: 0.9799\n","Epoch 201: val_accuracy did not improve from 0.79569\n","36/36 [==============================] - 1s 32ms/step - loss: 0.0657 - accuracy: 0.9799 - val_loss: 1.5849 - val_accuracy: 0.7788\n","Epoch 202/1000\n","35/36 [============================>.] - ETA: 0s - loss: 0.0657 - accuracy: 0.9799\n","Epoch 202: val_accuracy did not improve from 0.79569\n","36/36 [==============================] - 1s 33ms/step - loss: 0.0658 - accuracy: 0.9799 - val_loss: 1.5922 - val_accuracy: 0.7791\n","Epoch 203/1000\n","35/36 [============================>.] - ETA: 0s - loss: 0.0650 - accuracy: 0.9801\n","Epoch 203: val_accuracy did not improve from 0.79569\n","36/36 [==============================] - 1s 33ms/step - loss: 0.0650 - accuracy: 0.9801 - val_loss: 1.5927 - val_accuracy: 0.7798\n","Epoch 204/1000\n","35/36 [============================>.] - ETA: 0s - loss: 0.0651 - accuracy: 0.9801\n","Epoch 204: val_accuracy did not improve from 0.79569\n","36/36 [==============================] - 1s 33ms/step - loss: 0.0651 - accuracy: 0.9801 - val_loss: 1.5766 - val_accuracy: 0.7790\n","Epoch 205/1000\n","35/36 [============================>.] - ETA: 0s - loss: 0.0650 - accuracy: 0.9801\n","Epoch 205: val_accuracy did not improve from 0.79569\n","36/36 [==============================] - 1s 32ms/step - loss: 0.0650 - accuracy: 0.9801 - val_loss: 1.6062 - val_accuracy: 0.7796\n","Epoch 206/1000\n","35/36 [============================>.] - ETA: 0s - loss: 0.0646 - accuracy: 0.9801\n","Epoch 206: val_accuracy did not improve from 0.79569\n","36/36 [==============================] - 1s 33ms/step - loss: 0.0646 - accuracy: 0.9801 - val_loss: 1.5831 - val_accuracy: 0.7798\n","Epoch 207/1000\n","35/36 [============================>.] - ETA: 0s - loss: 0.0650 - accuracy: 0.9799\n","Epoch 207: val_accuracy did not improve from 0.79569\n","36/36 [==============================] - 1s 32ms/step - loss: 0.0650 - accuracy: 0.9799 - val_loss: 1.5899 - val_accuracy: 0.7792\n","Epoch 208/1000\n","35/36 [============================>.] - ETA: 0s - loss: 0.0639 - accuracy: 0.9803\n","Epoch 208: val_accuracy did not improve from 0.79569\n","36/36 [==============================] - 1s 32ms/step - loss: 0.0639 - accuracy: 0.9802 - val_loss: 1.5970 - val_accuracy: 0.7791\n","Epoch 209/1000\n","35/36 [============================>.] - ETA: 0s - loss: 0.0638 - accuracy: 0.9801\n","Epoch 209: val_accuracy did not improve from 0.79569\n","36/36 [==============================] - 1s 33ms/step - loss: 0.0639 - accuracy: 0.9801 - val_loss: 1.6030 - val_accuracy: 0.7791\n","Epoch 210/1000\n","35/36 [============================>.] - ETA: 0s - loss: 0.0643 - accuracy: 0.9801\n","Epoch 210: val_accuracy did not improve from 0.79569\n","36/36 [==============================] - 1s 32ms/step - loss: 0.0644 - accuracy: 0.9801 - val_loss: 1.6015 - val_accuracy: 0.7796\n","Epoch 211/1000\n","35/36 [============================>.] - ETA: 0s - loss: 0.0632 - accuracy: 0.9804\n","Epoch 211: val_accuracy did not improve from 0.79569\n","36/36 [==============================] - 1s 32ms/step - loss: 0.0632 - accuracy: 0.9803 - val_loss: 1.5979 - val_accuracy: 0.7789\n","Epoch 212/1000\n","35/36 [============================>.] - ETA: 0s - loss: 0.0630 - accuracy: 0.9806\n","Epoch 212: val_accuracy did not improve from 0.79569\n","36/36 [==============================] - 1s 33ms/step - loss: 0.0631 - accuracy: 0.9806 - val_loss: 1.6233 - val_accuracy: 0.7794\n","Epoch 213/1000\n","35/36 [============================>.] - ETA: 0s - loss: 0.0634 - accuracy: 0.9803\n","Epoch 213: val_accuracy did not improve from 0.79569\n","36/36 [==============================] - 1s 33ms/step - loss: 0.0634 - accuracy: 0.9803 - val_loss: 1.6153 - val_accuracy: 0.7789\n","Epoch 214/1000\n","35/36 [============================>.] - ETA: 0s - loss: 0.0624 - accuracy: 0.9807\n","Epoch 214: val_accuracy did not improve from 0.79569\n","36/36 [==============================] - 1s 32ms/step - loss: 0.0625 - accuracy: 0.9806 - val_loss: 1.6173 - val_accuracy: 0.7790\n","Epoch 215/1000\n","35/36 [============================>.] - ETA: 0s - loss: 0.0624 - accuracy: 0.9806\n","Epoch 215: val_accuracy did not improve from 0.79569\n","36/36 [==============================] - 1s 32ms/step - loss: 0.0625 - accuracy: 0.9807 - val_loss: 1.6296 - val_accuracy: 0.7779\n","Epoch 216/1000\n","35/36 [============================>.] - ETA: 0s - loss: 0.0627 - accuracy: 0.9806\n","Epoch 216: val_accuracy did not improve from 0.79569\n","36/36 [==============================] - 1s 33ms/step - loss: 0.0627 - accuracy: 0.9806 - val_loss: 1.6194 - val_accuracy: 0.7789\n","Epoch 217/1000\n","35/36 [============================>.] - ETA: 0s - loss: 0.0619 - accuracy: 0.9808\n","Epoch 217: val_accuracy did not improve from 0.79569\n","36/36 [==============================] - 1s 32ms/step - loss: 0.0620 - accuracy: 0.9808 - val_loss: 1.6167 - val_accuracy: 0.7802\n","Epoch 218/1000\n","35/36 [============================>.] - ETA: 0s - loss: 0.0620 - accuracy: 0.9807\n","Epoch 218: val_accuracy did not improve from 0.79569\n","36/36 [==============================] - 1s 33ms/step - loss: 0.0621 - accuracy: 0.9807 - val_loss: 1.6205 - val_accuracy: 0.7797\n","Epoch 219/1000\n","35/36 [============================>.] - ETA: 0s - loss: 0.0613 - accuracy: 0.9808\n","Epoch 219: val_accuracy did not improve from 0.79569\n","36/36 [==============================] - 1s 32ms/step - loss: 0.0613 - accuracy: 0.9809 - val_loss: 1.6226 - val_accuracy: 0.7799\n","Epoch 220/1000\n","35/36 [============================>.] - ETA: 0s - loss: 0.0602 - accuracy: 0.9811\n","Epoch 220: val_accuracy did not improve from 0.79569\n","36/36 [==============================] - 1s 33ms/step - loss: 0.0603 - accuracy: 0.9811 - val_loss: 1.6377 - val_accuracy: 0.7792\n","Epoch 221/1000\n","35/36 [============================>.] - ETA: 0s - loss: 0.0615 - accuracy: 0.9809\n","Epoch 221: val_accuracy did not improve from 0.79569\n","36/36 [==============================] - 1s 32ms/step - loss: 0.0615 - accuracy: 0.9809 - val_loss: 1.6342 - val_accuracy: 0.7784\n","Epoch 222/1000\n","35/36 [============================>.] - ETA: 0s - loss: 0.0613 - accuracy: 0.9808\n","Epoch 222: val_accuracy did not improve from 0.79569\n","36/36 [==============================] - 1s 33ms/step - loss: 0.0613 - accuracy: 0.9808 - val_loss: 1.6465 - val_accuracy: 0.7788\n","Epoch 223/1000\n","35/36 [============================>.] - ETA: 0s - loss: 0.0609 - accuracy: 0.9808\n","Epoch 223: val_accuracy did not improve from 0.79569\n","36/36 [==============================] - 1s 33ms/step - loss: 0.0609 - accuracy: 0.9808 - val_loss: 1.6670 - val_accuracy: 0.7766\n","Epoch 224/1000\n","35/36 [============================>.] - ETA: 0s - loss: 0.0614 - accuracy: 0.9808\n","Epoch 224: val_accuracy did not improve from 0.79569\n","36/36 [==============================] - 1s 32ms/step - loss: 0.0615 - accuracy: 0.9808 - val_loss: 1.6444 - val_accuracy: 0.7783\n","Epoch 225/1000\n","35/36 [============================>.] - ETA: 0s - loss: 0.0608 - accuracy: 0.9810\n","Epoch 225: val_accuracy did not improve from 0.79569\n","36/36 [==============================] - 1s 33ms/step - loss: 0.0608 - accuracy: 0.9810 - val_loss: 1.6285 - val_accuracy: 0.7799\n","Epoch 226/1000\n","35/36 [============================>.] - ETA: 0s - loss: 0.0604 - accuracy: 0.9810\n","Epoch 226: val_accuracy did not improve from 0.79569\n","36/36 [==============================] - 1s 32ms/step - loss: 0.0605 - accuracy: 0.9810 - val_loss: 1.6332 - val_accuracy: 0.7797\n","Epoch 226: early stopping\n"]}],"source":["## Train the model\n","model.compile(optimizer=\"rmsprop\", loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])\n","history =model.fit(\n","    [encoder_input_data, decoder_input_data],\n","    decoder_target_data,\n","    batch_size=batch_size,\n","    epochs=1000,\n","    validation_split=0.2,\n","    callbacks=[es,mc])\n","# Save model\n"]},{"cell_type":"code","source":["os.getcwd()"],"metadata":{"id":"EsM5JiM0Qv92","executionInfo":{"status":"aborted","timestamp":1666550599462,"user_tz":-330,"elapsed":24,"user":{"displayName":"Gouthaman S","userId":"11145458456231366145"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["model.save(\"englishtohindi\",save_format='h5')"],"metadata":{"id":"v5424FAnQubn","executionInfo":{"status":"ok","timestamp":1666551069259,"user_tz":-330,"elapsed":404,"user":{"displayName":"Gouthaman S","userId":"11145458456231366145"}}},"execution_count":36,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"1onfpeqSL9MB"},"source":["## Run inference (sampling)\n","\n","1. encode input and retrieve initial decoder state\n","2. run one step of decoder with this initial state\n","and a \"start of sequence\" token as target.\n","Output will be the next target token.\n","3. Repeat with the current target token and current states\n"]},{"cell_type":"code","execution_count":32,"metadata":{"id":"M-DmTWHbL9MB","executionInfo":{"status":"ok","timestamp":1666537247085,"user_tz":-330,"elapsed":2584,"user":{"displayName":"Gouthaman S","userId":"11145458456231366145"}}},"outputs":[],"source":["# Define sampling models\n","# Restore the model and construct the encoder and decoder.\n","model = keras.models.load_model(\"englishtohindi\")"]},{"cell_type":"code","source":["model.summary()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"avWUcIO2XD6k","executionInfo":{"status":"ok","timestamp":1666537255877,"user_tz":-330,"elapsed":600,"user":{"displayName":"Gouthaman S","userId":"11145458456231366145"}},"outputId":"a87de1c7-7977-46e0-98e3-2b3b2b5053ce"},"execution_count":33,"outputs":[{"output_type":"stream","name":"stdout","text":["Model: \"model\"\n","__________________________________________________________________________________________________\n"," Layer (type)                   Output Shape         Param #     Connected to                     \n","==================================================================================================\n"," input_1 (InputLayer)           [(None, None, 70)]   0           []                               \n","                                                                                                  \n"," input_2 (InputLayer)           [(None, None, 92)]   0           []                               \n","                                                                                                  \n"," lstm (LSTM)                    [(None, 256),        334848      ['input_1[0][0]']                \n","                                 (None, 256),                                                     \n","                                 (None, 256)]                                                     \n","                                                                                                  \n"," lstm_1 (LSTM)                  [(None, None, 256),  357376      ['input_2[0][0]',                \n","                                 (None, 256),                     'lstm[0][1]',                   \n","                                 (None, 256)]                     'lstm[0][2]']                   \n","                                                                                                  \n"," dense (Dense)                  (None, None, 92)     23644       ['lstm_1[0][0]']                 \n","                                                                                                  \n","==================================================================================================\n","Total params: 715,868\n","Trainable params: 715,868\n","Non-trainable params: 0\n","__________________________________________________________________________________________________\n"]}]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ezY3Acl2L9MD"},"outputs":[],"source":["model.input?"]},{"cell_type":"code","execution_count":37,"metadata":{"id":"gaf0wvL3L9ME","executionInfo":{"status":"ok","timestamp":1666551077905,"user_tz":-330,"elapsed":621,"user":{"displayName":"Gouthaman S","userId":"11145458456231366145"}}},"outputs":[],"source":["encoder_inputs = model.input[0]  # input_1\n","encoder_outputs, state_h_enc, state_c_enc = model.layers[2].output  # lstm_1\n","encoder_states = [state_h_enc, state_c_enc]\n","encoder_model = keras.Model(encoder_inputs, encoder_states)\n","\n","decoder_inputs = model.input[1]  # input_2\n","decoder_state_input_h = keras.Input(shape=(latent_dim,), name=\"inpu t_3\")\n","decoder_state_input_c = keras.Input(shape=(latent_dim,), name=\"input_4\")\n","decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n","decoder_lstm = model.layers[3]\n","decoder_outputs, state_h_dec, state_c_dec = decoder_lstm(\n","    decoder_inputs, initial_state=decoder_states_inputs\n",")\n","decoder_states = [state_h_dec, state_c_dec]\n","decoder_dense = model.layers[4]\n","decoder_outputs = decoder_dense(decoder_outputs)\n","decoder_model = keras.Model(\n","    [decoder_inputs] + decoder_states_inputs, [decoder_outputs] + decoder_states\n",")"]},{"cell_type":"code","execution_count":38,"metadata":{"id":"Nj-WdO6WL9MG","executionInfo":{"status":"ok","timestamp":1666551081733,"user_tz":-330,"elapsed":471,"user":{"displayName":"Gouthaman S","userId":"11145458456231366145"}}},"outputs":[],"source":["# Reverse-lookup token index to decode sequences back to\n","# something readable.\n","reverse_input_char_index = dict((i, char) for char, i in input_token_index.items())\n","reverse_target_char_index = dict((i, char) for char, i in target_token_index.items())"]},{"cell_type":"code","execution_count":39,"metadata":{"id":"r4mTz_bGL9MG","executionInfo":{"status":"ok","timestamp":1666551083992,"user_tz":-330,"elapsed":9,"user":{"displayName":"Gouthaman S","userId":"11145458456231366145"}}},"outputs":[],"source":["def decode_sequence(input_seq):\n","    # Encode the input as state vectors.\n","    states_value = encoder_model.predict(input_seq)\n","\n","    # Generate empty target sequence of length 1.\n","    target_seq = np.zeros((1, 1, num_decoder_tokens))\n","    # Populate the first character of target sequence with the start character.\n","    target_seq[0, 0, target_token_index[\"\\t\"]] = 1.0\n","\n","    # Sampling loop for a batch of sequences\n","    # (to simplify, here we assume a batch of size 1).\n","    stop_condition = False\n","    decoded_sentence = \"\"\n","    while not stop_condition:\n","        output_tokens, h, c = decoder_model.predict([target_seq] + states_value)\n","\n","        # Sample a token\n","        sampled_token_index = np.argmax(output_tokens[0, -1, :])\n","        sampled_char = reverse_target_char_index[sampled_token_index]\n","        decoded_sentence += sampled_char\n","\n","        # Exit condition: either hit max length\n","        # or find stop character.\n","        if sampled_char == \"\\n\" or len(decoded_sentence) > max_decoder_seq_length:\n","            stop_condition = True\n","\n","        # Update the target sequence (of length 1).\n","        target_seq = np.zeros((1, 1, num_decoder_tokens))\n","        target_seq[0, 0, sampled_token_index] = 1.0\n","\n","        # Update states\n","        states_value = [h, c]\n","    return decoded_sentence"]},{"cell_type":"code","source":["seq_index=54\n","\n","input_seq = encoder_input_data[seq_index : seq_index + 1]\n","decoded_sentence = decode_sequence(input_seq)\n","print(\"-\")\n","print(\"Input sentence:\", input_texts[seq_index])\n","print(\"Decoded sentence:\", decoded_sentence)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"KSreCv7jXOg5","executionInfo":{"status":"ok","timestamp":1666551092396,"user_tz":-330,"elapsed":2880,"user":{"displayName":"Gouthaman S","userId":"11145458456231366145"}},"outputId":"dfd2f645-5ade-474a-d88a-3419bc42c96d"},"execution_count":40,"outputs":[{"output_type":"stream","name":"stdout","text":["1/1 [==============================] - 0s 391ms/step\n","1/1 [==============================] - 0s 366ms/step\n","1/1 [==============================] - 0s 17ms/step\n","1/1 [==============================] - 0s 16ms/step\n","1/1 [==============================] - 0s 22ms/step\n","1/1 [==============================] - 0s 16ms/step\n","1/1 [==============================] - 0s 17ms/step\n","1/1 [==============================] - 0s 16ms/step\n","1/1 [==============================] - 0s 20ms/step\n","1/1 [==============================] - 0s 27ms/step\n","1/1 [==============================] - 0s 18ms/step\n","1/1 [==============================] - 0s 18ms/step\n","1/1 [==============================] - 0s 16ms/step\n","1/1 [==============================] - 0s 18ms/step\n","1/1 [==============================] - 0s 19ms/step\n","1/1 [==============================] - 0s 18ms/step\n","1/1 [==============================] - 0s 19ms/step\n","1/1 [==============================] - 0s 17ms/step\n","1/1 [==============================] - 0s 22ms/step\n","1/1 [==============================] - 0s 17ms/step\n","1/1 [==============================] - 0s 17ms/step\n","1/1 [==============================] - 0s 20ms/step\n","1/1 [==============================] - 0s 21ms/step\n","1/1 [==============================] - 0s 17ms/step\n","1/1 [==============================] - 0s 17ms/step\n","1/1 [==============================] - 0s 20ms/step\n","1/1 [==============================] - 0s 17ms/step\n","1/1 [==============================] - 0s 17ms/step\n","1/1 [==============================] - 0s 17ms/step\n","1/1 [==============================] - 0s 16ms/step\n","1/1 [==============================] - 0s 20ms/step\n","-\n","Input sentence: I love you.\n","Decoded sentence: मैं तुम्हारे जितना लम्बा हूँ।\n","\n"]}]},{"cell_type":"code","execution_count":41,"metadata":{"id":"9lqtRL-wL9MI","outputId":"4bcd1f2d-b603-42a8-e50e-25dbdccbf8dc","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1666551125401,"user_tz":-330,"elapsed":6597,"user":{"displayName":"Gouthaman S","userId":"11145458456231366145"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["1/1 [==============================] - 0s 18ms/step\n","1/1 [==============================] - 0s 17ms/step\n","1/1 [==============================] - 0s 19ms/step\n","1/1 [==============================] - 0s 19ms/step\n","1/1 [==============================] - 0s 19ms/step\n","1/1 [==============================] - 0s 18ms/step\n","1/1 [==============================] - 0s 19ms/step\n","1/1 [==============================] - 0s 17ms/step\n","1/1 [==============================] - 0s 16ms/step\n","1/1 [==============================] - 0s 20ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 16ms/step\n","1/1 [==============================] - 0s 16ms/step\n","1/1 [==============================] - 0s 19ms/step\n","1/1 [==============================] - 0s 16ms/step\n","1/1 [==============================] - 0s 21ms/step\n","1/1 [==============================] - 0s 16ms/step\n","1/1 [==============================] - 0s 18ms/step\n","1/1 [==============================] - 0s 18ms/step\n","1/1 [==============================] - 0s 19ms/step\n","1/1 [==============================] - 0s 18ms/step\n","1/1 [==============================] - 0s 17ms/step\n","1/1 [==============================] - 0s 16ms/step\n","1/1 [==============================] - 0s 18ms/step\n","-\n","Input sentence: Wow!\n","Decoded sentence: मुझे अपना पता बतादेना।\n","\n","1/1 [==============================] - 0s 21ms/step\n","1/1 [==============================] - 0s 19ms/step\n","1/1 [==============================] - 0s 20ms/step\n","1/1 [==============================] - 0s 17ms/step\n","1/1 [==============================] - 0s 19ms/step\n","1/1 [==============================] - 0s 19ms/step\n","1/1 [==============================] - 0s 18ms/step\n","1/1 [==============================] - 0s 19ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 21ms/step\n","1/1 [==============================] - 0s 18ms/step\n","1/1 [==============================] - 0s 20ms/step\n","1/1 [==============================] - 0s 26ms/step\n","1/1 [==============================] - 0s 20ms/step\n","1/1 [==============================] - 0s 22ms/step\n","1/1 [==============================] - 0s 18ms/step\n","1/1 [==============================] - 0s 19ms/step\n","1/1 [==============================] - 0s 18ms/step\n","-\n","Input sentence: Help!\n","Decoded sentence: यह मुफ़्त का है।\n","\n","1/1 [==============================] - 0s 19ms/step\n","1/1 [==============================] - 0s 20ms/step\n","1/1 [==============================] - 0s 18ms/step\n","1/1 [==============================] - 0s 17ms/step\n","1/1 [==============================] - 0s 18ms/step\n","1/1 [==============================] - 0s 21ms/step\n","1/1 [==============================] - 0s 16ms/step\n","1/1 [==============================] - 0s 17ms/step\n","1/1 [==============================] - 0s 21ms/step\n","1/1 [==============================] - 0s 16ms/step\n","1/1 [==============================] - 0s 16ms/step\n","1/1 [==============================] - 0s 17ms/step\n","1/1 [==============================] - 0s 18ms/step\n","1/1 [==============================] - 0s 17ms/step\n","1/1 [==============================] - 0s 18ms/step\n","1/1 [==============================] - 0s 19ms/step\n","1/1 [==============================] - 0s 22ms/step\n","1/1 [==============================] - 0s 17ms/step\n","1/1 [==============================] - 0s 20ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 22ms/step\n","1/1 [==============================] - 0s 19ms/step\n","1/1 [==============================] - 0s 22ms/step\n","1/1 [==============================] - 0s 17ms/step\n","-\n","Input sentence: Jump.\n","Decoded sentence: मुझे अपना पता बतादेना।\n","\n","1/1 [==============================] - 0s 19ms/step\n","1/1 [==============================] - 0s 19ms/step\n","1/1 [==============================] - 0s 18ms/step\n","1/1 [==============================] - 0s 18ms/step\n","1/1 [==============================] - 0s 19ms/step\n","1/1 [==============================] - 0s 19ms/step\n","1/1 [==============================] - 0s 21ms/step\n","1/1 [==============================] - 0s 17ms/step\n","1/1 [==============================] - 0s 20ms/step\n","1/1 [==============================] - 0s 20ms/step\n","1/1 [==============================] - 0s 18ms/step\n","1/1 [==============================] - 0s 17ms/step\n","1/1 [==============================] - 0s 20ms/step\n","1/1 [==============================] - 0s 19ms/step\n","1/1 [==============================] - 0s 17ms/step\n","1/1 [==============================] - 0s 17ms/step\n","1/1 [==============================] - 0s 18ms/step\n","1/1 [==============================] - 0s 16ms/step\n","1/1 [==============================] - 0s 19ms/step\n","1/1 [==============================] - 0s 18ms/step\n","1/1 [==============================] - 0s 18ms/step\n","1/1 [==============================] - 0s 28ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 17ms/step\n","-\n","Input sentence: Jump.\n","Decoded sentence: मुझे अपना पता बतादेना।\n","\n","1/1 [==============================] - 0s 19ms/step\n","1/1 [==============================] - 0s 21ms/step\n","1/1 [==============================] - 0s 20ms/step\n","1/1 [==============================] - 0s 18ms/step\n","1/1 [==============================] - 0s 19ms/step\n","1/1 [==============================] - 0s 20ms/step\n","1/1 [==============================] - 0s 19ms/step\n","1/1 [==============================] - 0s 17ms/step\n","1/1 [==============================] - 0s 17ms/step\n","1/1 [==============================] - 0s 21ms/step\n","1/1 [==============================] - 0s 20ms/step\n","1/1 [==============================] - 0s 19ms/step\n","1/1 [==============================] - 0s 19ms/step\n","1/1 [==============================] - 0s 20ms/step\n","1/1 [==============================] - 0s 18ms/step\n","1/1 [==============================] - 0s 16ms/step\n","1/1 [==============================] - 0s 20ms/step\n","1/1 [==============================] - 0s 17ms/step\n","1/1 [==============================] - 0s 18ms/step\n","1/1 [==============================] - 0s 18ms/step\n","1/1 [==============================] - 0s 18ms/step\n","1/1 [==============================] - 0s 16ms/step\n","1/1 [==============================] - 0s 21ms/step\n","1/1 [==============================] - 0s 20ms/step\n","-\n","Input sentence: Jump.\n","Decoded sentence: मुझे अपना पता बतादेना।\n","\n"]}],"source":["#You can now generate decoded sentences as such:\n","for seq_index in range(5):\n","    # Take one sequence (part of the training set)\n","    # for trying out decoding.\n","    input_seq = encoder_input_data[seq_index : seq_index + 1]\n","    decoded_sentence = decode_sequence(input_seq)\n","    print(\"-\")\n","    print(\"Input sentence:\", input_texts[seq_index])\n","    print(\"Decoded sentence:\", decoded_sentence)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"z0OZy8msL9MK"},"outputs":[],"source":[]}],"metadata":{"kernelspec":{"display_name":"Python 3.9.12 ('base')","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.12"},"varInspector":{"cols":{"lenName":16,"lenType":16,"lenVar":40},"kernels_config":{"python":{"delete_cmd_postfix":"","delete_cmd_prefix":"del ","library":"var_list.py","varRefreshCmd":"print(var_dic_list())"},"r":{"delete_cmd_postfix":") ","delete_cmd_prefix":"rm(","library":"var_list.r","varRefreshCmd":"cat(var_dic_list()) "}},"types_to_exclude":["module","function","builtin_function_or_method","instance","_Feature"],"window_display":false},"vscode":{"interpreter":{"hash":"ad2bdc8ecc057115af97d19610ffacc2b4e99fae6737bb82f5d7fb13d2f2c186"}},"colab":{"provenance":[{"file_id":"13MX3goznfoqJ_QMzpXD1j7-_nK2tXIG4","timestamp":1666534780134}],"collapsed_sections":[]},"accelerator":"GPU"},"nbformat":4,"nbformat_minor":0}